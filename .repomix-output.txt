This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-18T21:25:39.811Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
public/
  sounds/
  placeholder.svg
scripts/
  dump-codebase.sh
src/
  accessibility/
    haptics/
    screenreader/
      jaws-adapter.ts
    voice/
      voice-control.ts
  adapters/
    base.adapter.ts
  ai/
    adapters/
      base.adapter.ts
    models/
    services/
      typing-tutor.service.ts
  api/
    perplexity-search.ts
  components/
    achievements/
      AchievementGrid.tsx
      AchievementNotification.tsx
    auth/
      AuthForm.tsx
      ProtectedRoute.tsx
    layout/
      Navigation.tsx
    leaderboard/
      LeaderboardTable.tsx
    ui/
      accordion.tsx
      alert-dialog.tsx
      alert.tsx
      aspect-ratio.tsx
      avatar.tsx
      badge.tsx
      breadcrumb.tsx
      button.tsx
      calendar.tsx
      card.tsx
      carousel.tsx
      chart.tsx
      checkbox.tsx
      collapsible.tsx
      command.tsx
      context-menu.tsx
      context-window.tsx
      dialog.tsx
      drawer.tsx
      dropdown-menu.tsx
      form.tsx
      hover-card.tsx
      input-otp.tsx
      input.tsx
      label.tsx
      menubar.tsx
      navigation-menu.tsx
      pagination.tsx
      popover.tsx
      progress.tsx
      radio-group.tsx
      resizable.tsx
      scroll-area.tsx
      select.tsx
      separator.tsx
      sheet.tsx
      sidebar.tsx
      skeleton.tsx
      slider.tsx
      sonner.tsx
      spinner.tsx
      switch.tsx
      table.tsx
      tabs.tsx
      textarea.tsx
      toast.tsx
      toaster.tsx
      toggle-group.tsx
      toggle.tsx
      tooltip.tsx
      use-toast.ts
    AccessibleTypingTutor.tsx
    ChartTooltip.tsx
    ErrorBoundary.tsx
    LoadingState.tsx
    PerformanceChart.tsx
    VisualKeyboard.tsx
  config/
    ai.config.ts
  contexts/
    theme-context.tsx
  hooks/
    use-focus-management.ts
    use-haptic-feedback.ts
    use-keyboard-layout.ts
    use-mobile.tsx
    use-screen-reader.ts
    use-toast.ts
    useAchievements.ts
    useAI.ts
    useLeaderboard.ts
    useSupabase.ts
  integrations/
    supabase/
      client.ts
      types.ts
  lib/
    __tests__/
      ai-core.test.ts
    typing/
      error-analysis.worker.ts
    accessibility-core.ts
    achievements.ts
    ai-core.ts
    ai-utils.ts
    config.ts
    gemini-tutor.ts
    utils.ts
  providers/
  routes/
    Auth.tsx
    Index.tsx
    Leaderboard.tsx
    NotFound.tsx
    Profile.tsx
    TypingTutor.tsx
  services/
    ai-tutor.service.ts
  styles/
    base.css
    components.css
    globals.css
    utilities.css
  test/
    setup.ts
  types/
    theme.d.ts
    typing.ts
  utils/
    typingAnalytics.ts
  App.css
  App.tsx
  index.css
  main.tsx
  PRD.md
  vite-env.d.ts
supabase/
  functions/
    gemini-tutor/
      index.ts
  migrations/
    20240218_user_profiles.sql
    20240219_leaderboard_functions.sql
    20240229000000_create_typing_history.sql
  apply-migrations.sh
  config.toml
.cursorrules
.gitignore
components.json
eslint.config.js
index.html
package.json
postcss.config.cjs
README.md
tailwind.config.ts
tsconfig.app.json
tsconfig.json
tsconfig.node.json
vite.config.ts
vitest.config.ts
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="public/placeholder.svg">
<svg xmlns="http://www.w3.org/2000/svg" width="1200" height="1200" fill="none"><rect width="1200" height="1200" fill="#EAEAEA" rx="3"/><g opacity=".5"><g opacity=".5"><path fill="#FAFAFA" d="M600.709 736.5c-75.454 0-136.621-61.167-136.621-136.62 0-75.454 61.167-136.621 136.621-136.621 75.453 0 136.62 61.167 136.62 136.621 0 75.453-61.167 136.62-136.62 136.62Z"/><path stroke="#C9C9C9" stroke-width="2.418" d="M600.709 736.5c-75.454 0-136.621-61.167-136.621-136.62 0-75.454 61.167-136.621 136.621-136.621 75.453 0 136.62 61.167 136.62 136.621 0 75.453-61.167 136.62-136.62 136.62Z"/></g><path stroke="url(#a)" stroke-width="2.418" d="M0-1.209h553.581" transform="scale(1 -1) rotate(45 1163.11 91.165)"/><path stroke="url(#b)" stroke-width="2.418" d="M404.846 598.671h391.726"/><path stroke="url(#c)" stroke-width="2.418" d="M599.5 795.742V404.017"/><path stroke="url(#d)" stroke-width="2.418" d="m795.717 796.597-391.441-391.44"/><path fill="#fff" d="M600.709 656.704c-31.384 0-56.825-25.441-56.825-56.824 0-31.384 25.441-56.825 56.825-56.825 31.383 0 56.824 25.441 56.824 56.825 0 31.383-25.441 56.824-56.824 56.824Z"/><g clip-path="url(#e)"><path fill="#666" fill-rule="evenodd" d="M616.426 586.58h-31.434v16.176l3.553-3.554.531-.531h9.068l.074-.074 8.463-8.463h2.565l7.18 7.181V586.58Zm-15.715 14.654 3.698 3.699 1.283 1.282-2.565 2.565-1.282-1.283-5.2-5.199h-6.066l-5.514 5.514-.073.073v2.876a2.418 2.418 0 0 0 2.418 2.418h26.598a2.418 2.418 0 0 0 2.418-2.418v-8.317l-8.463-8.463-7.181 7.181-.071.072Zm-19.347 5.442v4.085a6.045 6.045 0 0 0 6.046 6.045h26.598a6.044 6.044 0 0 0 6.045-6.045v-7.108l1.356-1.355-1.282-1.283-.074-.073v-17.989h-38.689v23.43l-.146.146.146.147Z" clip-rule="evenodd"/></g><path stroke="#C9C9C9" stroke-width="2.418" d="M600.709 656.704c-31.384 0-56.825-25.441-56.825-56.824 0-31.384 25.441-56.825 56.825-56.825 31.383 0 56.824 25.441 56.824 56.825 0 31.383-25.441 56.824-56.824 56.824Z"/></g><defs><linearGradient id="a" x1="554.061" x2="-.48" y1=".083" y2=".087" gradientUnits="userSpaceOnUse"><stop stop-color="#C9C9C9" stop-opacity="0"/><stop offset=".208" stop-color="#C9C9C9"/><stop offset=".792" stop-color="#C9C9C9"/><stop offset="1" stop-color="#C9C9C9" stop-opacity="0"/></linearGradient><linearGradient id="b" x1="796.912" x2="404.507" y1="599.963" y2="599.965" gradientUnits="userSpaceOnUse"><stop stop-color="#C9C9C9" stop-opacity="0"/><stop offset=".208" stop-color="#C9C9C9"/><stop offset=".792" stop-color="#C9C9C9"/><stop offset="1" stop-color="#C9C9C9" stop-opacity="0"/></linearGradient><linearGradient id="c" x1="600.792" x2="600.794" y1="403.677" y2="796.082" gradientUnits="userSpaceOnUse"><stop stop-color="#C9C9C9" stop-opacity="0"/><stop offset=".208" stop-color="#C9C9C9"/><stop offset=".792" stop-color="#C9C9C9"/><stop offset="1" stop-color="#C9C9C9" stop-opacity="0"/></linearGradient><linearGradient id="d" x1="404.85" x2="796.972" y1="403.903" y2="796.02" gradientUnits="userSpaceOnUse"><stop stop-color="#C9C9C9" stop-opacity="0"/><stop offset=".208" stop-color="#C9C9C9"/><stop offset=".792" stop-color="#C9C9C9"/><stop offset="1" stop-color="#C9C9C9" stop-opacity="0"/></linearGradient><clipPath id="e"><path fill="#fff" d="M581.364 580.535h38.689v38.689h-38.689z"/></clipPath></defs></svg>
</file>

<file path="scripts/dump-codebase.sh">
#!/bin/bash
# Create dumps directory if it doesn't exist
DUMP_DIR=".codebase-dumps"
mkdir -p $DUMP_DIR
# Generate timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DUMP_FILE="$DUMP_DIR/codebase_dump_$TIMESTAMP.txt"
# Initialize word count
TOTAL_WORDS=0
TARGET_WORDS=20000  # This should give us roughly 25-30k tokens
# Function to check if a file should be included
should_include() {
    local file=$1
    # Skip node_modules, .git, dump files, and other non-essential files
    if [[ $file == *"node_modules"* ]] ||
       [[ $file == *".git"* ]] ||
       [[ $file == *"codebase_dump_"* ]] ||
       [[ $file == *".next"* ]] ||
       [[ $file == *"dist"* ]] ||
       [[ $file == *".DS_Store"* ]] ||
       [[ $file == *".env"* ]] ||
       [[ $file == *"package-lock.json"* ]] ||
       [[ $file == *"pnpm-lock.yaml"* ]] ||
       [[ $file == *".ico" ]] ||
       [[ $file == *".png" ]] ||
       [[ $file == *".jpg" ]] ||
       [[ $file == *".svg" ]]; then
        return 1
    fi
    return 0
}
# Function to get file type emoji
get_file_emoji() {
    local file=$1
    case $file in
        *.ts|*.tsx) echo "🟦" ;; # TypeScript
        *.js|*.jsx) echo "🟨" ;; # JavaScript
        *.css) echo "🎨" ;; # CSS
        *.html) echo "🌐" ;; # HTML
        *.json) echo "📝" ;; # JSON
        *.md) echo "📚" ;; # Markdown
        *.sh) echo "⚡️" ;; # Shell
        *.sql) echo "💾" ;; # SQL
        *.test.*|*.spec.*) echo "🧪" ;; # Test files
        *worker*) echo "⚙️" ;; # Web Workers
        *) echo "📄" ;; # Other
    esac
}
# Function to estimate tokens from words
estimate_tokens() {
    local words=$1
    echo "scale=0; $words * 1.3" | bc
}
# Write header
echo "🗂 DeepType Codebase Dump" > "$DUMP_FILE"
echo "📅 Generated on: $(date)" >> "$DUMP_FILE"
echo "═══════════════════════════════════════════════════" >> "$DUMP_FILE"
echo "" >> "$DUMP_FILE"
# Priority files to always include
PRIORITY_FILES=(
    # Core Services
    "src/services/ai-tutor.service.ts"
    "src/adapters/base.adapter.ts"
    # Types and Config
    "src/types/typing.ts"
    "src/config/ai.config.ts"
    # Components
    "src/components/typing/accessible-keyboard.tsx"
    # Hooks
    "src/hooks/use-keyboard-layout.ts"
    "src/hooks/use-sound.ts"
    "src/hooks/use-haptic-feedback.ts"
    "src/hooks/use-screen-reader.ts"
    # Workers
    "src/lib/typing/error-analysis.worker.ts"
)
# Write priority files first
echo "📌 Core System Files" >> "$DUMP_FILE"
echo "───────────────────────────────────────────────────" >> "$DUMP_FILE"
for file in "${PRIORITY_FILES[@]}"; do
    if [ -f "$file" ]; then
        emoji=$(get_file_emoji "$file")
        echo "" >> "$DUMP_FILE"
        echo "$emoji File: $file" >> "$DUMP_FILE"
        echo "───────────────────────────────────────────────────" >> "$DUMP_FILE"
        echo "\`\`\`${file##*.}" >> "$DUMP_FILE"
        cat "$file" >> "$DUMP_FILE"
        echo "\`\`\`" >> "$DUMP_FILE"
        echo "" >> "$DUMP_FILE"
        # Update word count
        WORDS=$(cat "$file" | wc -w)
        TOTAL_WORDS=$((TOTAL_WORDS + WORDS))
    fi
done
# Write important directories content
echo "📂 Component Files" >> "$DUMP_FILE"
echo "───────────────────────────────────────────────────" >> "$DUMP_FILE"
# Find and sort files by importance (using a scoring system)
find . -type f \( -name "*.ts" -o -name "*.tsx" -o -name "*.css" -o -name "*.sql" -o -name "*.json" -o -name "*.md" \) | while read -r file; do
    if should_include "$file"; then
        # Skip if it's already included in priority files
        if [[ " ${PRIORITY_FILES[@]} " =~ " ${file#./} " ]]; then
            continue
        fi
        # Score the file based on its content and path
        SCORE=0
        if [[ $file == *"/ai/"* ]]; then SCORE=$((SCORE + 10)); fi
        if [[ $file == *"/accessibility/"* ]]; then SCORE=$((SCORE + 10)); fi
        if [[ $file == *"/components/typing/"* ]]; then SCORE=$((SCORE + 9)); fi
        if [[ $file == *"/components/"* ]]; then SCORE=$((SCORE + 8)); fi
        if [[ $file == *"/hooks/"* ]]; then SCORE=$((SCORE + 7)); fi
        if [[ $file == *"/providers/"* ]]; then SCORE=$((SCORE + 7)); fi
        if [[ $file == *"/types/"* ]]; then SCORE=$((SCORE + 6)); fi
        if [[ $file == *"/config/"* ]]; then SCORE=$((SCORE + 6)); fi
        if [[ $file == *"/lib/"* ]]; then SCORE=$((SCORE + 5)); fi
        if [[ $file == *"worker"* ]]; then SCORE=$((SCORE + 5)); fi
        if [[ $file == *".test."* ]] || [[ $file == *".spec."* ]]; then SCORE=$((SCORE + 4)); fi
        if [[ $file == *"README"* ]] || [[ $file == *".md" ]]; then SCORE=$((SCORE + 3)); fi
        # Boost score for files with important keywords
        if grep -q -i "accessibility\|performance\|optimization\|security\|worker\|async\|concurrent" "$file" 2>/dev/null; then
            SCORE=$((SCORE + 4))
        fi
        # Check file size (prefer medium-sized files)
        SIZE=$(wc -l < "$file")
        if [ $SIZE -gt 500 ]; then SCORE=$((SCORE - 2)); fi
        if [ $SIZE -lt 10 ]; then SCORE=$((SCORE - 1)); fi
        echo "$SCORE $file"
    fi
done | sort -rn | while read -r score_and_file; do
    file=$(echo "$score_and_file" | cut -d' ' -f2-)
    # Check if we've reached our target
    if [ $TOTAL_WORDS -ge $TARGET_WORDS ]; then
        break
    fi
    # Add file content
    emoji=$(get_file_emoji "$file")
    echo "" >> "$DUMP_FILE"
    echo "$emoji File: ${file#./}" >> "$DUMP_FILE"
    echo "───────────────────────────────────────────────────" >> "$DUMP_FILE"
    echo "\`\`\`${file##*.}" >> "$DUMP_FILE"
    cat "$file" >> "$DUMP_FILE"
    echo "\`\`\`" >> "$DUMP_FILE"
    echo "" >> "$DUMP_FILE"
    # Update word count
    WORDS=$(cat "$file" | wc -w)
    TOTAL_WORDS=$((TOTAL_WORDS + WORDS))
done
# Add summary
echo "📊 Dump Statistics" >> "$DUMP_FILE"
echo "───────────────────────────────────────────────────" >> "$DUMP_FILE"
echo "Total words: $TOTAL_WORDS" >> "$DUMP_FILE"
echo "Estimated tokens: $(estimate_tokens $TOTAL_WORDS)" >> "$DUMP_FILE"
echo "Generated on: $(date)" >> "$DUMP_FILE"
# Make the script executable
chmod +x "$DUMP_FILE"
echo "✅ Codebase dump created at: $DUMP_FILE"
echo "📈 Total words: $TOTAL_WORDS (≈$(estimate_tokens $TOTAL_WORDS) tokens)"
</file>

<file path="src/accessibility/screenreader/jaws-adapter.ts">
/*
 *     ██╗ █████╗ ██╗    ██╗███████╗
 *     ██║██╔══██╗██║    ██║██╔════╝
 *     ██║███████║██║ █╗ ██║███████╗
 *██   ██║██╔══██║██║███╗██║╚════██║
 *╚█████╔╝██║  ██║╚███╔███╔╝███████║
 * ╚════╝ ╚═╝  ╚═╝ ╚══╝╚══╝ ╚══════╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Adapter: JAWS Screen Reader                                                                    ║
 * ║ Description: Adapter for integrating with JAWS screen reader software                          ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
interface JAWSCommand {
  command: string;
  description: string;
  shortcut?: string;
  category: 'navigation' | 'reading' | 'typing' | 'system';
}
interface JAWSSettings {
  verbosity: 'high' | 'medium' | 'low';
  punctuation: 'all' | 'some' | 'none';
  typingEcho: 'characters' | 'words' | 'none';
  rate: number; // 1-100
  pitch: number; // 1-100
  volume: number; // 1-100
}
/**
 * JAWS keyboard commands for typing tutor
 */
const JAWS_COMMANDS: JAWSCommand[] = [
  {
    command: 'SayCharacter',
    description: 'Speak the current character',
    shortcut: 'NumPad5',
    category: 'reading'
  },
  {
    command: 'SayWord',
    description: 'Speak the current word',
    shortcut: 'Ctrl+NumPad5',
    category: 'reading'
  },
  {
    command: 'SayLine',
    description: 'Speak the current line',
    shortcut: 'Insert+UpArrow',
    category: 'reading'
  },
  {
    command: 'StopSpeech',
    description: 'Stop speech immediately',
    shortcut: 'Ctrl',
    category: 'system'
  },
  {
    command: 'ToggleTypingEcho',
    description: 'Toggle typing echo mode',
    shortcut: 'Insert+2',
    category: 'typing'
  },
  {
    command: 'AdjustRate',
    description: 'Adjust speech rate',
    shortcut: 'Alt+Ctrl+PageUp/Down',
    category: 'system'
  }
];
/**
 * Default JAWS settings for typing tutor
 */
const DEFAULT_SETTINGS: JAWSSettings = {
  verbosity: 'high',
  punctuation: 'all',
  typingEcho: 'characters',
  rate: 60,
  pitch: 50,
  volume: 80
};
/**
 * JAWS screen reader adapter
 */
export class JAWSAdapter {
  private settings: JAWSSettings;
  private isConnected: boolean = false;
  private commandQueue: string[] = [];
  private lastError: Error | null = null;
  constructor(settings: Partial<JAWSSettings> = {}) {
    this.settings = { ...DEFAULT_SETTINGS, ...settings };
    this.initialize();
  }
  /**
   * Initialize JAWS adapter
   */
  private async initialize(): Promise<void> {
    console.log('Initializing JAWS adapter...');
    try {
      // Check if JAWS is running
      if (await this.checkJAWSRunning()) {
        await this.connect();
        await this.configureSettings();
        console.log('JAWS adapter initialized successfully');
      } else {
        throw new Error('JAWS is not running');
      }
    } catch (error) {
      console.error('Failed to initialize JAWS adapter:', error);
      this.lastError = error as Error;
    }
  }
  /**
   * Check if JAWS is running
   */
  private async checkJAWSRunning(): Promise<boolean> {
    try {
      // Attempt to detect JAWS process or service
      // This is platform-specific and requires native integration
      return true; // Placeholder
    } catch (error) {
      console.error('Error checking JAWS status:', error);
      return false;
    }
  }
  /**
   * Connect to JAWS
   */
  private async connect(): Promise<void> {
    try {
      // Establish connection with JAWS
      // This requires native integration with JAWS API
      this.isConnected = true;
      console.log('Connected to JAWS successfully');
    } catch (error) {
      console.error('Failed to connect to JAWS:', error);
      throw error;
    }
  }
  /**
   * Configure JAWS settings
   */
  private async configureSettings(): Promise<void> {
    if (!this.isConnected) {
      throw new Error('Not connected to JAWS');
    }
    try {
      await this.setVerbosity(this.settings.verbosity);
      await this.setPunctuation(this.settings.punctuation);
      await this.setTypingEcho(this.settings.typingEcho);
      await this.setSpeechParameters(
        this.settings.rate,
        this.settings.pitch,
        this.settings.volume
      );
    } catch (error) {
      console.error('Error configuring JAWS settings:', error);
      throw error;
    }
  }
  /**
   * Set JAWS verbosity level
   */
  private async setVerbosity(level: JAWSSettings['verbosity']): Promise<void> {
    const verbosityMap = {
      high: 3,
      medium: 2,
      low: 1
    };
    await this.sendCommand(`SetVerbosity ${verbosityMap[level]}`);
  }
  /**
   * Set punctuation level
   */
  private async setPunctuation(level: JAWSSettings['punctuation']): Promise<void> {
    await this.sendCommand(`SetPunctuation ${level}`);
  }
  /**
   * Set typing echo mode
   */
  private async setTypingEcho(mode: JAWSSettings['typingEcho']): Promise<void> {
    await this.sendCommand(`SetTypingEcho ${mode}`);
  }
  /**
   * Set speech parameters
   */
  private async setSpeechParameters(
    rate: number,
    pitch: number,
    volume: number
  ): Promise<void> {
    await this.sendCommand(`SetRate ${rate}`);
    await this.sendCommand(`SetPitch ${pitch}`);
    await this.sendCommand(`SetVolume ${volume}`);
  }
  /**
   * Send command to JAWS
   */
  private async sendCommand(command: string): Promise<void> {
    if (!this.isConnected) {
      this.commandQueue.push(command);
      return;
    }
    try {
      // Send command to JAWS
      // This requires native integration with JAWS API
      console.log('Sending JAWS command:', command);
    } catch (error) {
      console.error('Error sending JAWS command:', error);
      throw error;
    }
  }
  /**
   * Speak text using JAWS
   */
  public async speak(text: string, interrupt: boolean = true): Promise<void> {
    if (!this.isConnected) {
      throw new Error('Not connected to JAWS');
    }
    try {
      if (interrupt) {
        await this.sendCommand('StopSpeech');
      }
      await this.sendCommand(`SayString "${text}"`);
    } catch (error) {
      console.error('Error speaking text with JAWS:', error);
      throw error;
    }
  }
  /**
   * Stop JAWS speech
   */
  public async stopSpeech(): Promise<void> {
    if (!this.isConnected) return;
    try {
      await this.sendCommand('StopSpeech');
    } catch (error) {
      console.error('Error stopping JAWS speech:', error);
      throw error;
    }
  }
  /**
   * Update JAWS settings
   */
  public async updateSettings(newSettings: Partial<JAWSSettings>): Promise<void> {
    this.settings = { ...this.settings, ...newSettings };
    await this.configureSettings();
  }
  /**
   * Get current JAWS settings
   */
  public getSettings(): JAWSSettings {
    return { ...this.settings };
  }
  /**
   * Get JAWS connection status
   */
  public isActive(): boolean {
    return this.isConnected;
  }
  /**
   * Get last error
   */
  public getLastError(): Error | null {
    return this.lastError;
  }
  /**
   * Get available JAWS commands
   */
  public getCommands(): JAWSCommand[] {
    return [...JAWS_COMMANDS];
  }
  /**
   * Clean up JAWS adapter
   */
  public async dispose(): Promise<void> {
    if (!this.isConnected) return;
    try {
      // Clean up resources and disconnect
      this.isConnected = false;
      this.commandQueue = [];
      console.log('JAWS adapter disposed successfully');
    } catch (error) {
      console.error('Error disposing JAWS adapter:', error);
      throw error;
    }
  }
}
</file>

<file path="src/accessibility/voice/voice-control.ts">
/*
 * ██╗   ██╗ ██████╗ ██╗ ██████╗███████╗
 * ██║   ██║██╔═══██╗██║██╔════╝██╔════╝
 * ██║   ██║██║   ██║██║██║     █████╗
 * ╚██╗ ██╔╝██║   ██║██║██║     ██╔══╝
 *  ╚████╔╝ ╚██████╔╝██║╚██████╗███████╗
 *   ╚═══╝   ╚═════╝ ╚═╝ ╚═════╝╚══════╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Voice Control                                                                          ║
 * ║ Description: Voice command system for hands-free typing practice                               ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
interface VoiceCommand {
  command: string;
  aliases?: string[];
  description: string;
  category: 'navigation' | 'typing' | 'system' | 'practice';
  action: () => Promise<void>;
}
interface VoiceControlConfig {
  language: string;
  confidence: number;
  continuous: boolean;
  interimResults: boolean;
  maxAlternatives: number;
}
type CommandHandler = (command: string, confidence: number) => Promise<void>;
/**
 * Default configuration
 */
const DEFAULT_CONFIG: VoiceControlConfig = {
  language: 'en-US',
  confidence: 0.8,
  continuous: true,
  interimResults: false,
  maxAlternatives: 3
};
/**
 * Voice commands for typing practice
 */
export class VoiceControl {
  private recognition: SpeechRecognition | null = null;
  private synthesis: SpeechSynthesisUtterance | null = null;
  private commands: Map<string, VoiceCommand> = new Map();
  private isListening: boolean = false;
  private commandHandlers: Set<CommandHandler> = new Set();
  private config: VoiceControlConfig;
  constructor(config: Partial<VoiceControlConfig> = {}) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.initialize();
  }
  /**
   * Initialize voice control system
   */
  private initialize(): void {
    console.log('Initializing voice control system...');
    try {
      // Initialize speech recognition
      if ('webkitSpeechRecognition' in window) {
        this.recognition = new (window as any).webkitSpeechRecognition();
        this.configureRecognition();
      } else if ('SpeechRecognition' in window) {
        this.recognition = new (window as any).SpeechRecognition();
        this.configureRecognition();
      } else {
        throw new Error('Speech recognition not supported');
      }
      // Initialize speech synthesis
      this.synthesis = new SpeechSynthesisUtterance();
      this.configureSynthesis();
      // Register default commands
      this.registerDefaultCommands();
      console.log('Voice control system initialized successfully');
    } catch (error) {
      console.error('Failed to initialize voice control:', error);
    }
  }
  /**
   * Configure speech recognition
   */
  private configureRecognition(): void {
    if (!this.recognition) return;
    this.recognition.continuous = this.config.continuous;
    this.recognition.interimResults = this.config.interimResults;
    this.recognition.maxAlternatives = this.config.maxAlternatives;
    this.recognition.lang = this.config.language;
    // Set up event handlers
    this.recognition.onstart = () => {
      console.log('Voice recognition started');
      this.isListening = true;
    };
    this.recognition.onend = () => {
      console.log('Voice recognition ended');
      this.isListening = false;
      // Restart if continuous mode
      if (this.config.continuous) {
        this.recognition?.start();
      }
    };
    this.recognition.onerror = (event) => {
      console.error('Voice recognition error:', event.error);
    };
    this.recognition.onresult = (event) => {
      this.handleRecognitionResult(event);
    };
  }
  /**
   * Configure speech synthesis
   */
  private configureSynthesis(): void {
    if (!this.synthesis) return;
    this.synthesis.lang = this.config.language;
    this.synthesis.rate = 1.0;
    this.synthesis.pitch = 1.0;
    this.synthesis.volume = 1.0;
  }
  /**
   * Register default voice commands
   */
  private registerDefaultCommands(): void {
    // Navigation commands
    this.registerCommand({
      command: 'start practice',
      aliases: ['begin practice', 'start typing'],
      description: 'Start a typing practice session',
      category: 'practice',
      action: async () => {
        await this.speak('Starting typing practice session');
        // Trigger practice start
      }
    });
    this.registerCommand({
      command: 'stop practice',
      aliases: ['end practice', 'finish typing'],
      description: 'End the current practice session',
      category: 'practice',
      action: async () => {
        await this.speak('Ending practice session');
        // Trigger practice end
      }
    });
    // System commands
    this.registerCommand({
      command: 'pause listening',
      aliases: ['stop listening'],
      description: 'Pause voice recognition',
      category: 'system',
      action: async () => {
        await this.stopListening();
        await this.speak('Voice recognition paused');
      }
    });
    this.registerCommand({
      command: 'resume listening',
      aliases: ['start listening'],
      description: 'Resume voice recognition',
      category: 'system',
      action: async () => {
        await this.startListening();
        await this.speak('Voice recognition resumed');
      }
    });
  }
  /**
   * Handle speech recognition results
   */
  private async handleRecognitionResult(event: SpeechRecognitionEvent): Promise<void> {
    const result = event.results[event.results.length - 1];
    const transcript = result[0].transcript.toLowerCase().trim();
    const confidence = result[0].confidence;
    console.log('Recognized:', transcript, `(${confidence})`);
    if (confidence < this.config.confidence) {
      console.log('Recognition confidence too low');
      return;
    }
    // Check for registered commands
    for (const [commandText, command] of this.commands) {
      if (
        transcript === commandText ||
        command.aliases?.some(alias => transcript === alias)
      ) {
        try {
          await command.action();
          // Notify command handlers
          await this.notifyCommandHandlers(commandText, confidence);
          return;
        } catch (error) {
          console.error('Error executing command:', error);
          await this.speak('Error executing command');
        }
      }
    }
    console.log('No matching command found');
  }
  /**
   * Register a new voice command
   */
  public registerCommand(command: VoiceCommand): void {
    this.commands.set(command.command.toLowerCase(), command);
    console.log('Registered command:', command.command);
  }
  /**
   * Unregister a voice command
   */
  public unregisterCommand(commandText: string): void {
    this.commands.delete(commandText.toLowerCase());
    console.log('Unregistered command:', commandText);
  }
  /**
   * Start listening for voice commands
   */
  public async startListening(): Promise<void> {
    if (this.isListening) return;
    try {
      await this.recognition?.start();
      console.log('Started listening for voice commands');
    } catch (error) {
      console.error('Error starting voice recognition:', error);
      throw error;
    }
  }
  /**
   * Stop listening for voice commands
   */
  public async stopListening(): Promise<void> {
    if (!this.isListening) return;
    try {
      await this.recognition?.stop();
      console.log('Stopped listening for voice commands');
    } catch (error) {
      console.error('Error stopping voice recognition:', error);
      throw error;
    }
  }
  /**
   * Speak text using speech synthesis
   */
  public async speak(text: string): Promise<void> {
    return new Promise((resolve, reject) => {
      if (!this.synthesis) {
        reject(new Error('Speech synthesis not initialized'));
        return;
      }
      this.synthesis.text = text;
      this.synthesis.onend = () => resolve();
      this.synthesis.onerror = (error) => reject(error);
      window.speechSynthesis.speak(this.synthesis);
    });
  }
  /**
   * Add command handler
   */
  public addCommandHandler(handler: CommandHandler): void {
    this.commandHandlers.add(handler);
  }
  /**
   * Remove command handler
   */
  public removeCommandHandler(handler: CommandHandler): void {
    this.commandHandlers.delete(handler);
  }
  /**
   * Notify command handlers
   */
  private async notifyCommandHandlers(
    command: string,
    confidence: number
  ): Promise<void> {
    const promises = Array.from(this.commandHandlers).map(handler =>
      handler(command, confidence)
    );
    await Promise.all(promises);
  }
  /**
   * Update configuration
   */
  public updateConfig(newConfig: Partial<VoiceControlConfig>): void {
    this.config = { ...this.config, ...newConfig };
    this.configureRecognition();
    this.configureSynthesis();
  }
  /**
   * Get registered commands
   */
  public getCommands(): VoiceCommand[] {
    return Array.from(this.commands.values());
  }
  /**
   * Get current configuration
   */
  public getConfig(): VoiceControlConfig {
    return { ...this.config };
  }
  /**
   * Check if system is currently listening
   */
  public isActive(): boolean {
    return this.isListening;
  }
  /**
   * Clean up resources
   */
  public dispose(): void {
    this.stopListening();
    this.commands.clear();
    this.commandHandlers.clear();
    this.recognition = null;
    this.synthesis = null;
  }
}
</file>

<file path="src/adapters/base.adapter.ts">
/*
 *  █████╗ ██╗    ████████╗██╗   ██╗████████╗ ██████╗ ██████╗
 * ██╔══██╗██║    ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗
 * ███████║██║       ██║   ██║   ██║   ██║   ██║   ██║██████╔╝
 * ██╔══██║██║       ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗
 * ██║  ██║██║       ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║
 * ╚═╝  ╚═╝╚═╝       ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Adapter: Base AI Adapter Interface                                                             ║
 * ║ Description: Core interface for AI service adapters                                            ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { Lesson, Analysis, TutorFeedback, FeedbackRequest } from '../types/typing';
/**
 * Base interface for AI service adapters
 * Defines the core functionality required for AI-powered typing tutoring
 */
export interface IAIAdapter {
  /**
   * Initialize the AI adapter with configuration
   * @param config Configuration object for the AI service
   */
  initialize(config: AIAdapterConfig): Promise<void>;
  /**
   * Generate a personalized typing lesson
   * @param userLevel Current user level (1-100)
   * @param focusAreas Array of areas to focus on (e.g., ["speed", "accuracy"])
   * @param preferences User preferences for lesson generation
   */
  generateLesson(
    userLevel: number,
    focusAreas: string[],
    preferences: LessonPreferences
  ): Promise<Lesson>;
  /**
   * Analyze typing performance and provide insights
   * @param sessionData Raw typing session data
   * @param userHistory Previous session history
   */
  analyzePerformance(
    sessionData: TypingSessionData,
    userHistory: PerformanceHistory
  ): Promise<Analysis>;
  /**
   * Provide real-time feedback during typing practice
   * @param request Feedback request containing current state
   */
  provideFeedback(request: FeedbackRequest): Promise<TutorFeedback>;
  /**
   * Check adapter health and connectivity
   * @returns Boolean indicating if the adapter is healthy
   */
  healthCheck(): Promise<boolean>;
}
/**
 * Configuration for AI adapter initialization
 */
export interface AIAdapterConfig {
  apiKey?: string;
  endpoint?: string;
  modelName?: string;
  temperature?: number;
  maxTokens?: number;
  requestTimeout?: number;
  retryConfig?: {
    maxRetries: number;
    backoffFactor: number;
    initialDelay: number;
  };
  cacheConfig?: {
    enabled: boolean;
    ttl: number;
    maxSize: number;
  };
}
/**
 * User preferences for lesson generation
 */
export interface LessonPreferences {
  difficulty: 'beginner' | 'intermediate' | 'advanced';
  contentType: 'code' | 'text' | 'mixed';
  duration: number;
  includeExercises: boolean;
  adaptivePacing: boolean;
  thematicContent?: string[];
}
/**
 * Raw typing session data
 */
export interface TypingSessionData {
  keyPresses: Array<{
    key: string;
    timestamp: number;
    duration: number;
    pressure?: number;
  }>;
  errors: Array<{
    expected: string;
    actual: string;
    position: number;
    timestamp: number;
  }>;
  metrics: {
    wpm: number;
    accuracy: number;
    duration: number;
    pauseCount: number;
  };
  context: {
    lessonId: string;
    difficulty: string;
    targetText: string;
    completedText: string;
  };
}
/**
 * Historical performance data
 */
export interface PerformanceHistory {
  recentSessions: Array<{
    timestamp: number;
    wpm: number;
    accuracy: number;
    lessonId: string;
  }>;
  weaknesses: Array<{
    pattern: string;
    frequency: number;
    lastSeen: number;
  }>;
  strengths: Array<{
    pattern: string;
    accuracy: number;
    speed: number;
  }>;
  progressMetrics: {
    averageWPM: number;
    averageAccuracy: number;
    totalPracticeTime: number;
    lessonsCompleted: number;
  };
}
</file>

<file path="src/ai/adapters/base.adapter.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Interface: AI Adapter Base                                                                     ║
 * ║ Description: Base interface for AI model adapters (Gemini, GPT-4, etc.)                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
export interface AIConfig {
  model: string;
  temperature: number;
  maxTokens: number;
  streamCallback?: (text: string) => void;
}
export interface AIResponse {
  text: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata?: Record<string, unknown>;
}
export interface AIAdapter {
  /**
   * Initialize the AI adapter with configuration
   * @param config The AI model configuration
   */
  initialize(config: AIConfig): Promise<void>;
  /**
   * Generate content based on a prompt
   * @param prompt The input prompt
   * @returns The generated content
   */
  generateContent(prompt: string): Promise<string>;
  /**
   * Stream content generation
   * @param prompt The input prompt
   * @param onToken Callback for each token
   */
  streamContent(prompt: string, onToken: (token: string) => void): Promise<void>;
  /**
   * Get the model's configuration
   */
  getConfig(): AIConfig;
  /**
   * Get usage statistics
   */
  getUsage(): Promise<{
    tokensUsed: number;
    requestsMade: number;
    lastRequestTime: Date;
  }>;
}
</file>

<file path="src/ai/services/typing-tutor.service.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Service: AI Typing Tutor                                                                       ║
 * ║ Description: Core AI service for generating lessons, analyzing performance, and giving feedback ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { AIAdapter } from '../adapters/base.adapter';
import { UserProfile, Lesson, TypingSession, Analysis, TutorFeedback, FeedbackRequest } from '../../types/typing';
import { ErrorAnalysisWorker } from '../../lib/typing/error-analysis.worker';
import { getConfig } from '../../config/ai.config';
interface IAITutorService {
  generateLesson(user: UserProfile): Promise<Lesson>;
  analyzePerformance(session: TypingSession): Promise<Analysis>;
  provideFeedback(input: FeedbackRequest): Promise<TutorFeedback>;
}
export class AITutorService implements IAITutorService {
  private readonly worker: ErrorAnalysisWorker;
  private readonly config = getConfig();
  constructor(private adapter: AIAdapter) {
    this.worker = new ErrorAnalysisWorker();
    this.initializeService();
  }
  private async initializeService(): Promise<void> {
    console.info('Initializing AI Tutor Service with config:', {
      model: this.config.model,
      temperature: this.config.temperature,
      maxTokens: this.config.maxTokens,
    });
  }
  async generateLesson(user: UserProfile): Promise<Lesson> {
    try {
      const prompt = this.buildLessonPrompt(user);
      const response = await this.adapter.generateContent(prompt);
      return this.parseLessonResponse(response, user.level);
    } catch (error) {
      console.error('Failed to generate lesson:', error);
      return this.getFallbackLesson(user.level);
    }
  }
  async analyzePerformance(session: TypingSession): Promise<Analysis> {
    try {
      // Offload heavy computation to web worker
      const analysis = await this.worker.analyze(session);
      // Enhance analysis with AI insights
      const aiInsights = await this.adapter.generateContent(
        this.buildAnalysisPrompt(session, analysis)
      );
      return {
        ...analysis,
        aiRecommendations: this.parseAIInsights(aiInsights),
      };
    } catch (error) {
      console.error('Failed to analyze performance:', error);
      return this.getFallbackAnalysis(session);
    }
  }
  async provideFeedback(input: FeedbackRequest): Promise<TutorFeedback> {
    try {
      const prompt = this.buildFeedbackPrompt(input);
      const response = await this.adapter.generateContent(prompt);
      return this.parseFeedbackResponse(response);
    } catch (error) {
      console.error('Failed to provide feedback:', error);
      return this.getFallbackFeedback(input);
    }
  }
  private buildLessonPrompt(user: UserProfile): string {
    return `Generate a typing lesson for:
Level: ${user.level}
Progress: ${user.accuracy}% accuracy, ${user.wpm} WPM
Weak keys: ${user.weakKeys.join(', ')}
Learning style: ${user.learningStyle}
Previous mistakes: ${JSON.stringify(user.recentMistakes)}
Focus areas: ${user.focusAreas.join(', ')}`;
  }
  private buildAnalysisPrompt(session: TypingSession, baseAnalysis: Analysis): string {
    return `Analyze typing performance:
Session data: ${JSON.stringify(session)}
Base analysis: ${JSON.stringify(baseAnalysis)}
Provide specific recommendations for improvement.`;
  }
  private buildFeedbackPrompt(input: FeedbackRequest): string {
    return `Provide typing feedback:
Current text: ${input.currentText}
Target text: ${input.targetText}
Recent mistakes: ${JSON.stringify(input.recentMistakes)}
User level: ${input.userLevel}
Learning style: ${input.learningStyle}`;
  }
  private parseLessonResponse(response: string, level: number): Lesson {
    try {
      return JSON.parse(response);
    } catch (error) {
      console.error('Failed to parse lesson response:', error);
      return this.getFallbackLesson(level);
    }
  }
  private parseAIInsights(insights: string): string[] {
    try {
      return JSON.parse(insights);
    } catch (error) {
      console.error('Failed to parse AI insights:', error);
      return ['Focus on accuracy and consistent speed'];
    }
  }
  private parseFeedbackResponse(response: string): TutorFeedback {
    try {
      return JSON.parse(response);
    } catch (error) {
      console.error('Failed to parse feedback response:', error);
      return {
        message: 'Keep practicing to improve your speed and accuracy.',
        corrections: [],
        suggestions: ['Take your time to type accurately'],
      };
    }
  }
  private getFallbackLesson(level: number): Lesson {
    const lessons = {
      1: 'The quick brown fox jumps over the lazy dog.',
      2: 'Pack my box with five dozen liquor jugs.',
      3: 'How vexingly quick daft zebras jump!',
      4: 'The five boxing wizards jump quickly.',
      5: 'Sphinx of black quartz, judge my vow.',
    };
    return {
      id: `fallback-${Date.now()}`,
      content: lessons[level as keyof typeof lessons] || lessons[1],
      level,
      focusKeys: [],
      estimatedDuration: 300,
    };
  }
  private getFallbackAnalysis(session: TypingSession): Analysis {
    return {
      wpm: session.wpm,
      accuracy: session.accuracy,
      errorPatterns: [],
      speedTrends: [],
      recommendedFocus: ['Practice for consistent speed and accuracy'],
      aiRecommendations: ['Continue practicing to build muscle memory'],
    };
  }
  private getFallbackFeedback(input: FeedbackRequest): TutorFeedback {
    return {
      message: 'Keep practicing to improve your typing skills.',
      corrections: input.recentMistakes.map(mistake => ({
        actual: mistake.actual,
        expected: mistake.expected,
        suggestion: 'Type carefully and accurately',
      })),
      suggestions: ['Focus on accuracy first, then speed will follow'],
    };
  }
}
</file>

<file path="src/api/perplexity-search.ts">
import { PerplexityAI } from "@/lib/perplexity";
export async function perplexitySearch(query: string) {
  try {
    const perplexity = new PerplexityAI({
      apiKey: process.env.PERPLEXITY_API_KEY,
    });
    const response = await perplexity.query({
      query,
      context: {
        // Add relevant context about your codebase
        project: "deeptype-transition",
        framework: "React + TypeScript + Vite",
        libraries: ["shadcn/ui", "tailwindcss", "radix-ui"],
      },
    });
    return {
      results: response.results.map((result) => ({
        title: result.title,
        description: result.snippet,
        url: result.url,
        type: result.type || "documentation",
      })),
    };
  } catch (error) {
    console.error("Perplexity search error:", error);
    return {
      results: [],
    };
  }
}
</file>

<file path="src/components/achievements/AchievementGrid.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Achievement Grid                                                                    ║
 * ║ Description: Grid display of achievements with progress and unlock status                      ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { ACHIEVEMENTS, ACHIEVEMENT_ICONS, type AchievementProgress } from '@/lib/achievements';
import { cn } from '@/lib/utils';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from '@/components/ui/tooltip';
interface AchievementGridProps {
  progress: AchievementProgress[];
  className?: string;
}
const rarityColors = {
  common: {
    bg: 'bg-slate-500/10',
    border: 'border-slate-500/20',
    text: 'text-slate-400',
    badge: 'bg-slate-500',
  },
  rare: {
    bg: 'bg-blue-500/10',
    border: 'border-blue-500/20',
    text: 'text-blue-400',
    badge: 'bg-blue-500',
  },
  epic: {
    bg: 'bg-purple-500/10',
    border: 'border-purple-500/20',
    text: 'text-purple-400',
    badge: 'bg-purple-500',
  },
  legendary: {
    bg: 'bg-yellow-500/10',
    border: 'border-yellow-500/20',
    text: 'text-yellow-400',
    badge: 'bg-yellow-500',
  },
};
const AchievementGrid: React.FC<AchievementGridProps> = ({ progress, className }) => {
  // Create a map of achievement progress for easy lookup
  const progressMap = new Map(progress.map(p => [p.achievementId, p]));
  return (
    <div className={cn("grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4", className)}>
      {ACHIEVEMENTS.map(achievement => {
        const achievementProgress = progressMap.get(achievement.id);
        const isUnlocked = achievementProgress?.isUnlocked || false;
        const progressValue = achievementProgress?.progress || 0;
        const IconComponent = ACHIEVEMENT_ICONS[achievement.icon];
        const colors = rarityColors[achievement.rarity];
        return (
          <TooltipProvider key={achievement.id}>
            <Tooltip>
              <TooltipTrigger asChild>
                <div
                  className={cn(
                    "relative p-4 rounded-lg border-2 transition-all duration-300",
                    colors.border,
                    colors.bg,
                    isUnlocked ? "opacity-100" : "opacity-60 hover:opacity-80",
                    "group cursor-help"
                  )}
                >
                  {/* Icon */}
                  <div className="flex justify-center mb-3">
                    <div className={cn(
                      "w-12 h-12 rounded-full flex items-center justify-center",
                      "bg-gradient-to-br from-background/50 to-background",
                      "transition-transform duration-300 group-hover:scale-110"
                    )}>
                      <IconComponent className={cn(
                        "w-6 h-6 transition-all duration-300",
                        colors.text,
                        isUnlocked ? "opacity-100" : "opacity-50 group-hover:opacity-75"
                      )} />
                    </div>
                  </div>
                  {/* Title and Rarity */}
                  <div className="text-center mb-2">
                    <h3 className="font-semibold text-sm mb-1 truncate">
                      {achievement.name}
                    </h3>
                    <Badge variant="secondary" className={cn(
                      "text-[10px] px-1.5 py-0.5",
                      colors.badge
                    )}>
                      {achievement.rarity.toUpperCase()}
                    </Badge>
                  </div>
                  {/* Progress Bar */}
                  <Progress
                    value={progressValue}
                    className="h-1 mt-2"
                  />
                  {/* Unlock Status */}
                  {isUnlocked && (
                    <div className="absolute -top-2 -right-2">
                      <Badge className="bg-green-500">
                        ✓
                      </Badge>
                    </div>
                  )}
                </div>
              </TooltipTrigger>
              <TooltipContent>
                <div className="text-sm">
                  <p className="font-medium mb-1">{achievement.name}</p>
                  <p className="text-muted-foreground">{achievement.description}</p>
                  {achievementProgress?.unlockedAt && (
                    <p className="text-xs text-muted-foreground mt-2">
                      Unlocked on {new Date(achievementProgress.unlockedAt).toLocaleDateString()}
                    </p>
                  )}
                  {!isUnlocked && (
                    <p className="text-xs text-muted-foreground mt-2">
                      Progress: {progressValue}%
                    </p>
                  )}
                </div>
              </TooltipContent>
            </Tooltip>
          </TooltipProvider>
        );
      })}
    </div>
  );
};
export default AchievementGrid;
</file>

<file path="src/components/achievements/AchievementNotification.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: AchievementNotification                                                             ║
 * ║ Description: Toast notification component for unlocked achievements                            ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { LucideIcon } from 'lucide-react';
import { cn } from '@/lib/utils';
import { Badge } from '@/components/ui/badge';
interface AchievementNotificationProps {
  name: string;
  description: string;
  icon: LucideIcon;
  rarity: 'common' | 'rare' | 'epic' | 'legendary';
}
const rarityColors = {
  common: 'bg-slate-500',
  rare: 'bg-blue-500',
  epic: 'bg-purple-500',
  legendary: 'bg-yellow-500'
};
const AchievementNotification: React.FC<AchievementNotificationProps> = ({
  name,
  description,
  icon: Icon,
  rarity
}) => {
  return (
    <div className="flex items-center space-x-3 p-2">
      <div className={cn(
        "w-12 h-12 rounded-full flex items-center justify-center",
        "bg-gradient-to-br from-background/50 to-background",
        "border-2",
        {
          'border-slate-500': rarity === 'common',
          'border-blue-500': rarity === 'rare',
          'border-purple-500': rarity === 'epic',
          'border-yellow-500': rarity === 'legendary',
        }
      )}>
        <Icon className={cn(
          "w-6 h-6",
          {
            'text-slate-400': rarity === 'common',
            'text-blue-400': rarity === 'rare',
            'text-purple-400': rarity === 'epic',
            'text-yellow-400': rarity === 'legendary',
          }
        )} />
      </div>
      <div className="flex-1 min-w-0">
        <div className="flex items-center gap-2">
          <h4 className="font-semibold text-sm truncate">{name}</h4>
          <Badge variant="secondary" className={cn(
            "text-[10px] px-1.5 py-0.5",
            rarityColors[rarity]
          )}>
            {rarity.toUpperCase()}
          </Badge>
        </div>
        <p className="text-sm text-muted-foreground truncate">{description}</p>
      </div>
    </div>
  );
};
export default AchievementNotification;
</file>

<file path="src/components/auth/AuthForm.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: AuthForm                                                                            ║
 * ║ Description: Authentication form with login, signup, and password reset functionality          ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useState } from 'react';
import { useSupabase } from '@/hooks/useSupabase';
import { useToast } from '@/components/ui/use-toast';
import {
  Card,
  CardContent,
  CardDescription,
  CardFooter,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Label } from '@/components/ui/label';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Spinner } from '@/components/ui/spinner';
type AuthMode = 'login' | 'signup' | 'reset';
const AuthForm: React.FC = () => {
  const [email, setEmail] = useState('');
  const [password, setPassword] = useState('');
  const [confirmPassword, setConfirmPassword] = useState('');
  const [mode, setMode] = useState<AuthMode>('login');
  const [isLoading, setIsLoading] = useState(false);
  const { supabase } = useSupabase();
  const { toast } = useToast();
  const handleAuth = async (e: React.FormEvent) => {
    e.preventDefault();
    setIsLoading(true);
    try {
      if (mode === 'signup') {
        if (password !== confirmPassword) {
          throw new Error('Passwords do not match');
        }
        const { error } = await supabase.auth.signUp({
          email,
          password,
          options: {
            emailRedirectTo: `${window.location.origin}/auth/callback`,
          },
        });
        if (error) throw error;
        toast({
          title: 'Check your email',
          description: 'We sent you a confirmation link to complete your signup.',
        });
      } else if (mode === 'login') {
        const { error } = await supabase.auth.signInWithPassword({
          email,
          password,
        });
        if (error) throw error;
        toast({
          title: 'Welcome back!',
          description: 'Successfully logged in.',
        });
      } else if (mode === 'reset') {
        const { error } = await supabase.auth.resetPasswordForEmail(email, {
          redirectTo: `${window.location.origin}/auth/reset-password`,
        });
        if (error) throw error;
        toast({
          title: 'Check your email',
          description: 'We sent you a password reset link.',
        });
      }
    } catch (error) {
      console.error('Authentication error:', error);
      toast({
        title: 'Error',
        description: error instanceof Error ? error.message : 'An error occurred',
        variant: 'destructive',
      });
    } finally {
      setIsLoading(false);
    }
  };
  return (
    <Card className="w-full max-w-md mx-auto">
      <CardHeader>
        <CardTitle>Welcome to DeepType</CardTitle>
        <CardDescription>
          {mode === 'login'
            ? 'Sign in to continue your typing journey'
            : mode === 'signup'
            ? 'Create an account to start learning'
            : 'Reset your password'}
        </CardDescription>
      </CardHeader>
      <CardContent>
        <Tabs value={mode} onValueChange={(value) => setMode(value as AuthMode)}>
          <TabsList className="grid w-full grid-cols-3">
            <TabsTrigger value="login">Login</TabsTrigger>
            <TabsTrigger value="signup">Sign Up</TabsTrigger>
            <TabsTrigger value="reset">Reset</TabsTrigger>
          </TabsList>
          <form onSubmit={handleAuth} className="space-y-4 mt-4">
            <div className="space-y-2">
              <Label htmlFor="email">Email</Label>
              <Input
                id="email"
                type="email"
                placeholder="you@example.com"
                value={email}
                onChange={(e) => setEmail(e.target.value)}
                required
              />
            </div>
            {mode !== 'reset' && (
              <div className="space-y-2">
                <Label htmlFor="password">Password</Label>
                <Input
                  id="password"
                  type="password"
                  value={password}
                  onChange={(e) => setPassword(e.target.value)}
                  required
                />
              </div>
            )}
            {mode === 'signup' && (
              <div className="space-y-2">
                <Label htmlFor="confirmPassword">Confirm Password</Label>
                <Input
                  id="confirmPassword"
                  type="password"
                  value={confirmPassword}
                  onChange={(e) => setConfirmPassword(e.target.value)}
                  required
                />
              </div>
            )}
            <Button
              type="submit"
              className="w-full"
              disabled={isLoading}
            >
              {isLoading ? (
                <Spinner size="sm" className="mr-2" />
              ) : mode === 'login' ? (
                'Sign In'
              ) : mode === 'signup' ? (
                'Create Account'
              ) : (
                'Send Reset Link'
              )}
            </Button>
          </form>
        </Tabs>
      </CardContent>
      <CardFooter className="flex flex-col space-y-2">
        <div className="text-sm text-muted-foreground text-center">
          {mode === 'login' ? (
            <>
              Don't have an account?{' '}
              <Button
                variant="link"
                className="p-0 h-auto"
                onClick={() => setMode('signup')}
              >
                Sign up
              </Button>
            </>
          ) : mode === 'signup' ? (
            <>
              Already have an account?{' '}
              <Button
                variant="link"
                className="p-0 h-auto"
                onClick={() => setMode('login')}
              >
                Sign in
              </Button>
            </>
          ) : (
            <Button
              variant="link"
              className="p-0 h-auto"
              onClick={() => setMode('login')}
            >
              Back to login
            </Button>
          )}
        </div>
      </CardFooter>
    </Card>
  );
};
export default AuthForm;
</file>

<file path="src/components/auth/ProtectedRoute.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: ProtectedRoute                                                                      ║
 * ║ Description: Route wrapper that checks for authentication before rendering protected content   ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useEffect, useState } from 'react';
import { Navigate, useLocation } from 'react-router-dom';
import { useSupabase } from '@/hooks/useSupabase';
import LoadingState from '@/components/LoadingState';
interface ProtectedRouteProps {
  children: React.ReactNode;
}
const ProtectedRoute: React.FC<ProtectedRouteProps> = ({ children }) => {
  const [isLoading, setIsLoading] = useState(true);
  const [isAuthenticated, setIsAuthenticated] = useState(false);
  const { supabase } = useSupabase();
  const location = useLocation();
  useEffect(() => {
    const checkAuth = async () => {
      try {
        const { data: { session } } = await supabase.auth.getSession();
        setIsAuthenticated(!!session);
      } catch (error) {
        console.error('Auth check failed:', error);
        setIsAuthenticated(false);
      } finally {
        setIsLoading(false);
      }
    };
    checkAuth();
    const { data: { subscription } } = supabase.auth.onAuthStateChange((_event, session) => {
      setIsAuthenticated(!!session);
      setIsLoading(false);
    });
    return () => {
      subscription.unsubscribe();
    };
  }, [supabase.auth]);
  if (isLoading) {
    return (
      <LoadingState
        fullScreen
        message="Checking authentication..."
      />
    );
  }
  if (!isAuthenticated) {
    // Redirect to login page with return path
    return <Navigate to="/auth" state={{ from: location }} replace />;
  }
  return <>{children}</>;
};
export default ProtectedRoute;
</file>

<file path="src/components/layout/Navigation.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Navigation                                                                          ║
 * ║ Description: Main navigation bar with user menu and page links                                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { Link, useNavigate, useLocation } from 'react-router-dom';
import { useSupabase } from '@/hooks/useSupabase';
import { useQuery } from '@tanstack/react-query';
import {
  DropdownMenu,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuTrigger,
} from '@/components/ui/dropdown-menu';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import { Button } from '@/components/ui/button';
import { useToast } from '@/components/ui/use-toast';
const Navigation: React.FC = () => {
  const navigate = useNavigate();
  const location = useLocation();
  const { supabase } = useSupabase();
  const { toast } = useToast();
  // Fetch user profile
  const { data: profile } = useQuery({
    queryKey: ['profile'],
    queryFn: async () => {
      const { data: { user } } = await supabase.auth.getUser();
      if (!user) return null;
      const { data, error } = await supabase
        .from('profiles')
        .select('username, avatar_url, full_name')
        .eq('id', user.id)
        .single();
      if (error) throw error;
      return data;
    },
  });
  const handleSignOut = async () => {
    try {
      const { error } = await supabase.auth.signOut();
      if (error) throw error;
      toast({
        title: 'Signed out successfully',
        description: 'See you next time!',
      });
      navigate('/auth');
    } catch (error) {
      console.error('Error signing out:', error);
      toast({
        title: 'Error',
        description: 'Failed to sign out. Please try again.',
        variant: 'destructive',
      });
    }
  };
  return (
    <nav className="border-b bg-background/95 backdrop-blur supports-[backdrop-filter]:bg-background/60">
      <div className="container flex h-14 items-center">
        <div className="mr-4 hidden md:flex">
          <Link to="/" className="mr-6 flex items-center space-x-2">
            <span className="hidden font-bold sm:inline-block">
              DeepType
            </span>
          </Link>
          <div className="flex items-center space-x-6 text-sm font-medium">
            <Link
              to="/"
              className={location.pathname === '/' ? 'text-foreground' : 'text-muted-foreground'}
            >
              Practice
            </Link>
            <Link
              to="/profile"
              className={location.pathname === '/profile' ? 'text-foreground' : 'text-muted-foreground'}
            >
              Profile
            </Link>
            <Link
              to="/leaderboard"
              className={location.pathname === '/leaderboard' ? 'text-foreground' : 'text-muted-foreground'}
            >
              Leaderboard
            </Link>
          </div>
        </div>
        <div className="flex flex-1 items-center justify-between space-x-2 md:justify-end">
          <div className="w-full flex-1 md:w-auto md:flex-none">
            {/* Add search or other controls here */}
          </div>
          <DropdownMenu>
            <DropdownMenuTrigger asChild>
              <Button
                variant="ghost"
                className="relative h-8 w-8 rounded-full"
              >
                <Avatar className="h-8 w-8">
                  <AvatarImage src={profile?.avatar_url || undefined} />
                  <AvatarFallback>
                    {profile?.username?.slice(0, 2).toUpperCase() || 'U'}
                  </AvatarFallback>
                </Avatar>
              </Button>
            </DropdownMenuTrigger>
            <DropdownMenuContent align="end">
              <DropdownMenuLabel>
                {profile?.full_name || profile?.username || 'User'}
              </DropdownMenuLabel>
              <DropdownMenuSeparator />
              <DropdownMenuItem asChild>
                <Link to="/profile">Profile</Link>
              </DropdownMenuItem>
              <DropdownMenuItem
                className="text-red-600 focus:text-red-600"
                onClick={handleSignOut}
              >
                Sign Out
              </DropdownMenuItem>
            </DropdownMenuContent>
          </DropdownMenu>
        </div>
      </div>
    </nav>
  );
};
export default Navigation;
</file>

<file path="src/components/leaderboard/LeaderboardTable.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: LeaderboardTable                                                                    ║
 * ║ Description: Displays user rankings and stats with beautiful animations                        ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { useQuery } from '@tanstack/react-query';
import { useSupabase } from '@/hooks/useSupabase';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import { Trophy, Medal, Award } from 'lucide-react';
import { cn } from '@/lib/utils';
import {
  Table,
  TableBody,
  TableCell,
  TableHead,
  TableHeader,
  TableRow,
} from '@/components/ui/table';
import { useLeaderboard, type TimePeriod, type SortBy } from '@/hooks/useLeaderboard';
import LoadingState from '@/components/LoadingState';
interface LeaderboardEntry {
  id: string;
  username: string;
  full_name: string | null;
  avatar_url: string | null;
  average_wpm: number;
  average_accuracy: number;
  total_lessons_completed: number;
  level: number;
  achievements_count: number;
  rank: number;
}
interface LeaderboardTableProps {
  className?: string;
  limit?: number;
  timePeriod?: TimePeriod;
  sortBy?: SortBy;
}
const rankColors = {
  1: 'text-yellow-500',
  2: 'text-slate-400',
  3: 'text-amber-600',
};
const rankIcons = {
  1: Trophy,
  2: Medal,
  3: Award,
};
const LeaderboardTable: React.FC<LeaderboardTableProps> = ({
  className,
  limit = 100,
  timePeriod = 'all',
  sortBy = 'wpm'
}) => {
  const {
    leaderboard,
    isLoading,
    error,
    currentUserRank,
  } = useLeaderboard({
    limit,
    timePeriod,
    sortBy,
    onError: (error) => {
      console.error('Leaderboard error:', error);
    },
  });
  if (error) {
    return (
      <div className="w-full p-8 text-center">
        <p className="text-destructive">Failed to load leaderboard</p>
        <p className="text-sm text-muted-foreground mt-2">{error.message}</p>
      </div>
    );
  }
  if (isLoading) {
    return (
      <div className="w-full h-[400px]">
        <LoadingState
          message="Loading leaderboard..."
          variant="fancy"
          size="lg"
        />
      </div>
    );
  }
  if (!leaderboard?.length) {
    return (
      <div className="w-full p-8 text-center">
        <p className="text-muted-foreground">No data available</p>
      </div>
    );
  }
  return (
    <div className={cn("rounded-lg border bg-card", className)}>
      <Table>
        <TableHeader>
          <TableRow>
            <TableHead className="w-[100px]">Rank</TableHead>
            <TableHead>User</TableHead>
            <TableHead className="text-right">WPM</TableHead>
            <TableHead className="text-right">Accuracy</TableHead>
            <TableHead className="text-right">Lessons</TableHead>
            <TableHead className="text-right">Level</TableHead>
            <TableHead className="text-right">Achievements</TableHead>
          </TableRow>
        </TableHeader>
        <TableBody>
          {leaderboard.map((entry) => {
            const RankIcon = rankIcons[entry.rank as keyof typeof rankIcons];
            const isCurrentUser = entry.rank === currentUserRank;
            return (
              <TableRow
                key={entry.id}
                className={cn(
                  "transition-colors hover:bg-muted/50",
                  isCurrentUser && "bg-primary/5 hover:bg-primary/10",
                  "animate-in fade-in-50 slide-in-from-left-1"
                )}
              >
                <TableCell className="font-medium">
                  <div className="flex items-center gap-2">
                    {RankIcon ? (
                      <RankIcon
                        className={cn(
                          "w-5 h-5",
                          rankColors[entry.rank as keyof typeof rankColors]
                        )}
                      />
                    ) : (
                      <span className="text-muted-foreground">#{entry.rank}</span>
                    )}
                  </div>
                </TableCell>
                <TableCell>
                  <div className="flex items-center gap-3">
                    <Avatar className="w-8 h-8">
                      <AvatarImage src={entry.avatar_url || undefined} />
                      <AvatarFallback>
                        {entry.username?.slice(0, 2).toUpperCase() || 'U'}
                      </AvatarFallback>
                    </Avatar>
                    <div className="flex flex-col">
                      <span className="font-medium">
                        {entry.full_name || entry.username}
                      </span>
                      {entry.full_name && (
                        <span className="text-xs text-muted-foreground">
                          @{entry.username}
                        </span>
                      )}
                    </div>
                  </div>
                </TableCell>
                <TableCell className="text-right">
                  <div className="flex items-center justify-end gap-2">
                    <span className="font-medium">{entry.average_wpm}</span>
                    <Progress
                      value={entry.average_wpm}
                      max={150}
                      className="w-16 h-2"
                    />
                  </div>
                </TableCell>
                <TableCell className="text-right">
                  <div className="flex items-center justify-end gap-2">
                    <span className="font-medium">{entry.average_accuracy}%</span>
                    <Progress
                      value={entry.average_accuracy}
                      max={100}
                      className="w-16 h-2"
                    />
                  </div>
                </TableCell>
                <TableCell className="text-right font-medium">
                  {entry.total_lessons_completed}
                </TableCell>
                <TableCell className="text-right">
                  <Badge variant="secondary">
                    Level {entry.level}
                  </Badge>
                </TableCell>
                <TableCell className="text-right font-medium">
                  {entry.achievements_count}
                </TableCell>
              </TableRow>
            );
          })}
        </TableBody>
      </Table>
    </div>
  );
};
export default LeaderboardTable;
</file>

<file path="src/components/ui/accordion.tsx">
import * as React from "react"
import * as AccordionPrimitive from "@radix-ui/react-accordion"
import { ChevronDown } from "lucide-react"
import { cn } from "@/lib/utils"
const Accordion = AccordionPrimitive.Root
const AccordionItem = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Item>
>(({ className, ...props }, ref) => (
  <AccordionPrimitive.Item
    ref={ref}
    className={cn("border-b", className)}
    {...props}
  />
))
AccordionItem.displayName = "AccordionItem"
const AccordionTrigger = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <AccordionPrimitive.Header className="flex">
    <AccordionPrimitive.Trigger
      ref={ref}
      className={cn(
        "flex flex-1 items-center justify-between py-4 font-medium transition-all hover:underline [&[data-state=open]>svg]:rotate-180",
        className
      )}
      {...props}
    >
      {children}
      <ChevronDown className="h-4 w-4 shrink-0 transition-transform duration-200" />
    </AccordionPrimitive.Trigger>
  </AccordionPrimitive.Header>
))
AccordionTrigger.displayName = AccordionPrimitive.Trigger.displayName
const AccordionContent = React.forwardRef<
  React.ElementRef<typeof AccordionPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof AccordionPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <AccordionPrimitive.Content
    ref={ref}
    className="overflow-hidden text-sm transition-all data-[state=closed]:animate-accordion-up data-[state=open]:animate-accordion-down"
    {...props}
  >
    <div className={cn("pb-4 pt-0", className)}>{children}</div>
  </AccordionPrimitive.Content>
))
AccordionContent.displayName = AccordionPrimitive.Content.displayName
export { Accordion, AccordionItem, AccordionTrigger, AccordionContent }
</file>

<file path="src/components/ui/alert-dialog.tsx">
import * as React from "react"
import * as AlertDialogPrimitive from "@radix-ui/react-alert-dialog"
import { cn } from "@/lib/utils"
import { buttonVariants } from "@/components/ui/button"
const AlertDialog = AlertDialogPrimitive.Root
const AlertDialogTrigger = AlertDialogPrimitive.Trigger
const AlertDialogPortal = AlertDialogPrimitive.Portal
const AlertDialogOverlay = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <AlertDialogPrimitive.Overlay
    className={cn(
      "fixed inset-0 z-50 bg-black/80  data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className
    )}
    {...props}
    ref={ref}
  />
))
AlertDialogOverlay.displayName = AlertDialogPrimitive.Overlay.displayName
const AlertDialogContent = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Content>
>(({ className, ...props }, ref) => (
  <AlertDialogPortal>
    <AlertDialogOverlay />
    <AlertDialogPrimitive.Content
      ref={ref}
      className={cn(
        "fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",
        className
      )}
      {...props}
    />
  </AlertDialogPortal>
))
AlertDialogContent.displayName = AlertDialogPrimitive.Content.displayName
const AlertDialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-2 text-center sm:text-left",
      className
    )}
    {...props}
  />
)
AlertDialogHeader.displayName = "AlertDialogHeader"
const AlertDialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className
    )}
    {...props}
  />
)
AlertDialogFooter.displayName = "AlertDialogFooter"
const AlertDialogTitle = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Title>
>(({ className, ...props }, ref) => (
  <AlertDialogPrimitive.Title
    ref={ref}
    className={cn("text-lg font-semibold", className)}
    {...props}
  />
))
AlertDialogTitle.displayName = AlertDialogPrimitive.Title.displayName
const AlertDialogDescription = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Description>
>(({ className, ...props }, ref) => (
  <AlertDialogPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
AlertDialogDescription.displayName =
  AlertDialogPrimitive.Description.displayName
const AlertDialogAction = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Action>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Action>
>(({ className, ...props }, ref) => (
  <AlertDialogPrimitive.Action
    ref={ref}
    className={cn(buttonVariants(), className)}
    {...props}
  />
))
AlertDialogAction.displayName = AlertDialogPrimitive.Action.displayName
const AlertDialogCancel = React.forwardRef<
  React.ElementRef<typeof AlertDialogPrimitive.Cancel>,
  React.ComponentPropsWithoutRef<typeof AlertDialogPrimitive.Cancel>
>(({ className, ...props }, ref) => (
  <AlertDialogPrimitive.Cancel
    ref={ref}
    className={cn(
      buttonVariants({ variant: "outline" }),
      "mt-2 sm:mt-0",
      className
    )}
    {...props}
  />
))
AlertDialogCancel.displayName = AlertDialogPrimitive.Cancel.displayName
export {
  AlertDialog,
  AlertDialogPortal,
  AlertDialogOverlay,
  AlertDialogTrigger,
  AlertDialogContent,
  AlertDialogHeader,
  AlertDialogFooter,
  AlertDialogTitle,
  AlertDialogDescription,
  AlertDialogAction,
  AlertDialogCancel,
}
</file>

<file path="src/components/ui/alert.tsx">
import * as React from "react"
import { cva, type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
const alertVariants = cva(
  "relative w-full rounded-lg border p-4 [&>svg~*]:pl-7 [&>svg+div]:translate-y-[-3px] [&>svg]:absolute [&>svg]:left-4 [&>svg]:top-4 [&>svg]:text-foreground",
  {
    variants: {
      variant: {
        default: "bg-background text-foreground",
        destructive:
          "border-destructive/50 text-destructive dark:border-destructive [&>svg]:text-destructive",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)
const Alert = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & VariantProps<typeof alertVariants>
>(({ className, variant, ...props }, ref) => (
  <div
    ref={ref}
    role="alert"
    className={cn(alertVariants({ variant }), className)}
    {...props}
  />
))
Alert.displayName = "Alert"
const AlertTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h5
    ref={ref}
    className={cn("mb-1 font-medium leading-none tracking-tight", className)}
    {...props}
  />
))
AlertTitle.displayName = "AlertTitle"
const AlertDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("text-sm [&_p]:leading-relaxed", className)}
    {...props}
  />
))
AlertDescription.displayName = "AlertDescription"
export { Alert, AlertTitle, AlertDescription }
</file>

<file path="src/components/ui/aspect-ratio.tsx">
import * as AspectRatioPrimitive from "@radix-ui/react-aspect-ratio"
const AspectRatio = AspectRatioPrimitive.Root
export { AspectRatio }
</file>

<file path="src/components/ui/avatar.tsx">
import * as React from "react"
import * as AvatarPrimitive from "@radix-ui/react-avatar"
import { cn } from "@/lib/utils"
const Avatar = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Root>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Root
    ref={ref}
    className={cn(
      "relative flex h-10 w-10 shrink-0 overflow-hidden rounded-full",
      className
    )}
    {...props}
  />
))
Avatar.displayName = AvatarPrimitive.Root.displayName
const AvatarImage = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Image>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Image>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Image
    ref={ref}
    className={cn("aspect-square h-full w-full", className)}
    {...props}
  />
))
AvatarImage.displayName = AvatarPrimitive.Image.displayName
const AvatarFallback = React.forwardRef<
  React.ElementRef<typeof AvatarPrimitive.Fallback>,
  React.ComponentPropsWithoutRef<typeof AvatarPrimitive.Fallback>
>(({ className, ...props }, ref) => (
  <AvatarPrimitive.Fallback
    ref={ref}
    className={cn(
      "flex h-full w-full items-center justify-center rounded-full bg-muted",
      className
    )}
    {...props}
  />
))
AvatarFallback.displayName = AvatarPrimitive.Fallback.displayName
export { Avatar, AvatarImage, AvatarFallback }
</file>

<file path="src/components/ui/badge.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Badge                                                                               ║
 * ║ Description: Badge component with multiple variants for different states                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import { cva, type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
const badgeVariants = cva(
  "inline-flex items-center rounded-full border px-2.5 py-0.5 text-xs font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2",
  {
    variants: {
      variant: {
        default:
          "border-transparent bg-primary text-primary-foreground hover:bg-primary/80",
        secondary:
          "border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80",
        destructive:
          "border-transparent bg-destructive text-destructive-foreground hover:bg-destructive/80",
        outline: "text-foreground",
        success:
          "border-transparent bg-green-500 text-white hover:bg-green-500/80",
        warning:
          "border-transparent bg-yellow-500 text-white hover:bg-yellow-500/80",
        info: "border-transparent bg-blue-500 text-white hover:bg-blue-500/80",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)
export interface BadgeProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof badgeVariants> {}
function Badge({ className, variant, ...props }: BadgeProps) {
  return (
    <div className={cn(badgeVariants({ variant }), className)} {...props} />
  )
}
export { Badge, badgeVariants }
</file>

<file path="src/components/ui/breadcrumb.tsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { ChevronRight, MoreHorizontal } from "lucide-react"
import { cn } from "@/lib/utils"
const Breadcrumb = React.forwardRef<
  HTMLElement,
  React.ComponentPropsWithoutRef<"nav"> & {
    separator?: React.ReactNode
  }
>(({ ...props }, ref) => <nav ref={ref} aria-label="breadcrumb" {...props} />)
Breadcrumb.displayName = "Breadcrumb"
const BreadcrumbList = React.forwardRef<
  HTMLOListElement,
  React.ComponentPropsWithoutRef<"ol">
>(({ className, ...props }, ref) => (
  <ol
    ref={ref}
    className={cn(
      "flex flex-wrap items-center gap-1.5 break-words text-sm text-muted-foreground sm:gap-2.5",
      className
    )}
    {...props}
  />
))
BreadcrumbList.displayName = "BreadcrumbList"
const BreadcrumbItem = React.forwardRef<
  HTMLLIElement,
  React.ComponentPropsWithoutRef<"li">
>(({ className, ...props }, ref) => (
  <li
    ref={ref}
    className={cn("inline-flex items-center gap-1.5", className)}
    {...props}
  />
))
BreadcrumbItem.displayName = "BreadcrumbItem"
const BreadcrumbLink = React.forwardRef<
  HTMLAnchorElement,
  React.ComponentPropsWithoutRef<"a"> & {
    asChild?: boolean
  }
>(({ asChild, className, ...props }, ref) => {
  const Comp = asChild ? Slot : "a"
  return (
    <Comp
      ref={ref}
      className={cn("transition-colors hover:text-foreground", className)}
      {...props}
    />
  )
})
BreadcrumbLink.displayName = "BreadcrumbLink"
const BreadcrumbPage = React.forwardRef<
  HTMLSpanElement,
  React.ComponentPropsWithoutRef<"span">
>(({ className, ...props }, ref) => (
  <span
    ref={ref}
    role="link"
    aria-disabled="true"
    aria-current="page"
    className={cn("font-normal text-foreground", className)}
    {...props}
  />
))
BreadcrumbPage.displayName = "BreadcrumbPage"
const BreadcrumbSeparator = ({
  children,
  className,
  ...props
}: React.ComponentProps<"li">) => (
  <li
    role="presentation"
    aria-hidden="true"
    className={cn("[&>svg]:size-3.5", className)}
    {...props}
  >
    {children ?? <ChevronRight />}
  </li>
)
BreadcrumbSeparator.displayName = "BreadcrumbSeparator"
const BreadcrumbEllipsis = ({
  className,
  ...props
}: React.ComponentProps<"span">) => (
  <span
    role="presentation"
    aria-hidden="true"
    className={cn("flex h-9 w-9 items-center justify-center", className)}
    {...props}
  >
    <MoreHorizontal className="h-4 w-4" />
    <span className="sr-only">More</span>
  </span>
)
BreadcrumbEllipsis.displayName = "BreadcrumbElipssis"
export {
  Breadcrumb,
  BreadcrumbList,
  BreadcrumbItem,
  BreadcrumbLink,
  BreadcrumbPage,
  BreadcrumbSeparator,
  BreadcrumbEllipsis,
}
</file>

<file path="src/components/ui/button.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Button                                                                              ║
 * ║ Description: Button component with multiple variants and sizes                                 ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { cva, type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50",
  {
    variants: {
      variant: {
        default:
          "bg-primary text-primary-foreground shadow hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground shadow-sm hover:bg-destructive/90",
        outline:
          "border border-input bg-background shadow-sm hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground shadow-sm hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-9 px-4 py-2",
        sm: "h-8 rounded-md px-3 text-xs",
        lg: "h-10 rounded-md px-8",
        icon: "h-9 w-9",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)
export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean
}
const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button"
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    )
  }
)
Button.displayName = "Button"
export { Button, buttonVariants }
</file>

<file path="src/components/ui/calendar.tsx">
import * as React from "react";
import { ChevronLeft, ChevronRight } from "lucide-react";
import { DayPicker } from "react-day-picker";
import { cn } from "@/lib/utils";
import { buttonVariants } from "@/components/ui/button";
export type CalendarProps = React.ComponentProps<typeof DayPicker>;
function Calendar({
  className,
  classNames,
  showOutsideDays = true,
  ...props
}: CalendarProps) {
  return (
    <DayPicker
      showOutsideDays={showOutsideDays}
      className={cn("p-3", className)}
      classNames={{
        months: "flex flex-col sm:flex-row space-y-4 sm:space-x-4 sm:space-y-0",
        month: "space-y-4",
        caption: "flex justify-center pt-1 relative items-center",
        caption_label: "text-sm font-medium",
        nav: "space-x-1 flex items-center",
        nav_button: cn(
          buttonVariants({ variant: "outline" }),
          "h-7 w-7 bg-transparent p-0 opacity-50 hover:opacity-100"
        ),
        nav_button_previous: "absolute left-1",
        nav_button_next: "absolute right-1",
        table: "w-full border-collapse space-y-1",
        head_row: "flex",
        head_cell:
          "text-muted-foreground rounded-md w-9 font-normal text-[0.8rem]",
        row: "flex w-full mt-2",
        cell: "h-9 w-9 text-center text-sm p-0 relative [&:has([aria-selected].day-range-end)]:rounded-r-md [&:has([aria-selected].day-outside)]:bg-accent/50 [&:has([aria-selected])]:bg-accent first:[&:has([aria-selected])]:rounded-l-md last:[&:has([aria-selected])]:rounded-r-md focus-within:relative focus-within:z-20",
        day: cn(
          buttonVariants({ variant: "ghost" }),
          "h-9 w-9 p-0 font-normal aria-selected:opacity-100"
        ),
        day_range_end: "day-range-end",
        day_selected:
          "bg-primary text-primary-foreground hover:bg-primary hover:text-primary-foreground focus:bg-primary focus:text-primary-foreground",
        day_today: "bg-accent text-accent-foreground",
        day_outside:
          "day-outside text-muted-foreground opacity-50 aria-selected:bg-accent/50 aria-selected:text-muted-foreground aria-selected:opacity-30",
        day_disabled: "text-muted-foreground opacity-50",
        day_range_middle:
          "aria-selected:bg-accent aria-selected:text-accent-foreground",
        day_hidden: "invisible",
        ...classNames,
      }}
      components={{
        IconLeft: ({ ..._props }) => <ChevronLeft className="h-4 w-4" />,
        IconRight: ({ ..._props }) => <ChevronRight className="h-4 w-4" />,
      }}
      {...props}
    />
  );
}
Calendar.displayName = "Calendar";
export { Calendar };
</file>

<file path="src/components/ui/card.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Card                                                                                ║
 * ║ Description: Card component with header, content, and footer sections                         ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import { cn } from "@/lib/utils"
const Card = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn(
      "rounded-lg border bg-card text-card-foreground shadow-sm transition-all hover:shadow-md",
      className
    )}
    {...props}
  />
))
Card.displayName = "Card"
const CardHeader = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex flex-col space-y-1.5 p-6", className)}
    {...props}
  />
))
CardHeader.displayName = "CardHeader"
const CardTitle = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLHeadingElement>
>(({ className, ...props }, ref) => (
  <h3
    ref={ref}
    className={cn(
      "text-2xl font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
CardTitle.displayName = "CardTitle"
const CardDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => (
  <p
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
CardDescription.displayName = "CardDescription"
const CardContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn("p-6 pt-0", className)} {...props} />
))
CardContent.displayName = "CardContent"
const CardFooter = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    className={cn("flex items-center p-6 pt-0", className)}
    {...props}
  />
))
CardFooter.displayName = "CardFooter"
export { Card, CardHeader, CardFooter, CardTitle, CardDescription, CardContent }
</file>

<file path="src/components/ui/carousel.tsx">
import * as React from "react"
import useEmblaCarousel, {
  type UseEmblaCarouselType,
} from "embla-carousel-react"
import { ArrowLeft, ArrowRight } from "lucide-react"
import { cn } from "@/lib/utils"
import { Button } from "@/components/ui/button"
type CarouselApi = UseEmblaCarouselType[1]
type UseCarouselParameters = Parameters<typeof useEmblaCarousel>
type CarouselOptions = UseCarouselParameters[0]
type CarouselPlugin = UseCarouselParameters[1]
type CarouselProps = {
  opts?: CarouselOptions
  plugins?: CarouselPlugin
  orientation?: "horizontal" | "vertical"
  setApi?: (api: CarouselApi) => void
}
type CarouselContextProps = {
  carouselRef: ReturnType<typeof useEmblaCarousel>[0]
  api: ReturnType<typeof useEmblaCarousel>[1]
  scrollPrev: () => void
  scrollNext: () => void
  canScrollPrev: boolean
  canScrollNext: boolean
} & CarouselProps
const CarouselContext = React.createContext<CarouselContextProps | null>(null)
function useCarousel() {
  const context = React.useContext(CarouselContext)
  if (!context) {
    throw new Error("useCarousel must be used within a <Carousel />")
  }
  return context
}
const Carousel = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement> & CarouselProps
>(
  (
    {
      orientation = "horizontal",
      opts,
      setApi,
      plugins,
      className,
      children,
      ...props
    },
    ref
  ) => {
    const [carouselRef, api] = useEmblaCarousel(
      {
        ...opts,
        axis: orientation === "horizontal" ? "x" : "y",
      },
      plugins
    )
    const [canScrollPrev, setCanScrollPrev] = React.useState(false)
    const [canScrollNext, setCanScrollNext] = React.useState(false)
    const onSelect = React.useCallback((api: CarouselApi) => {
      if (!api) {
        return
      }
      setCanScrollPrev(api.canScrollPrev())
      setCanScrollNext(api.canScrollNext())
    }, [])
    const scrollPrev = React.useCallback(() => {
      api?.scrollPrev()
    }, [api])
    const scrollNext = React.useCallback(() => {
      api?.scrollNext()
    }, [api])
    const handleKeyDown = React.useCallback(
      (event: React.KeyboardEvent<HTMLDivElement>) => {
        if (event.key === "ArrowLeft") {
          event.preventDefault()
          scrollPrev()
        } else if (event.key === "ArrowRight") {
          event.preventDefault()
          scrollNext()
        }
      },
      [scrollPrev, scrollNext]
    )
    React.useEffect(() => {
      if (!api || !setApi) {
        return
      }
      setApi(api)
    }, [api, setApi])
    React.useEffect(() => {
      if (!api) {
        return
      }
      onSelect(api)
      api.on("reInit", onSelect)
      api.on("select", onSelect)
      return () => {
        api?.off("select", onSelect)
      }
    }, [api, onSelect])
    return (
      <CarouselContext.Provider
        value={{
          carouselRef,
          api: api,
          opts,
          orientation:
            orientation || (opts?.axis === "y" ? "vertical" : "horizontal"),
          scrollPrev,
          scrollNext,
          canScrollPrev,
          canScrollNext,
        }}
      >
        <div
          ref={ref}
          onKeyDownCapture={handleKeyDown}
          className={cn("relative", className)}
          role="region"
          aria-roledescription="carousel"
          {...props}
        >
          {children}
        </div>
      </CarouselContext.Provider>
    )
  }
)
Carousel.displayName = "Carousel"
const CarouselContent = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { carouselRef, orientation } = useCarousel()
  return (
    <div ref={carouselRef} className="overflow-hidden">
      <div
        ref={ref}
        className={cn(
          "flex",
          orientation === "horizontal" ? "-ml-4" : "-mt-4 flex-col",
          className
        )}
        {...props}
      />
    </div>
  )
})
CarouselContent.displayName = "CarouselContent"
const CarouselItem = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const { orientation } = useCarousel()
  return (
    <div
      ref={ref}
      role="group"
      aria-roledescription="slide"
      className={cn(
        "min-w-0 shrink-0 grow-0 basis-full",
        orientation === "horizontal" ? "pl-4" : "pt-4",
        className
      )}
      {...props}
    />
  )
})
CarouselItem.displayName = "CarouselItem"
const CarouselPrevious = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = "outline", size = "icon", ...props }, ref) => {
  const { orientation, scrollPrev, canScrollPrev } = useCarousel()
  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        "absolute  h-8 w-8 rounded-full",
        orientation === "horizontal"
          ? "-left-12 top-1/2 -translate-y-1/2"
          : "-top-12 left-1/2 -translate-x-1/2 rotate-90",
        className
      )}
      disabled={!canScrollPrev}
      onClick={scrollPrev}
      {...props}
    >
      <ArrowLeft className="h-4 w-4" />
      <span className="sr-only">Previous slide</span>
    </Button>
  )
})
CarouselPrevious.displayName = "CarouselPrevious"
const CarouselNext = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<typeof Button>
>(({ className, variant = "outline", size = "icon", ...props }, ref) => {
  const { orientation, scrollNext, canScrollNext } = useCarousel()
  return (
    <Button
      ref={ref}
      variant={variant}
      size={size}
      className={cn(
        "absolute h-8 w-8 rounded-full",
        orientation === "horizontal"
          ? "-right-12 top-1/2 -translate-y-1/2"
          : "-bottom-12 left-1/2 -translate-x-1/2 rotate-90",
        className
      )}
      disabled={!canScrollNext}
      onClick={scrollNext}
      {...props}
    >
      <ArrowRight className="h-4 w-4" />
      <span className="sr-only">Next slide</span>
    </Button>
  )
})
CarouselNext.displayName = "CarouselNext"
export {
  type CarouselApi,
  Carousel,
  CarouselContent,
  CarouselItem,
  CarouselPrevious,
  CarouselNext,
}
</file>

<file path="src/components/ui/chart.tsx">
import * as React from "react"
import * as RechartsPrimitive from "recharts"
import { cn } from "@/lib/utils"
// Format: { THEME_NAME: CSS_SELECTOR }
const THEMES = { light: "", dark: ".dark" } as const
export type ChartConfig = {
  [k in string]: {
    label?: React.ReactNode
    icon?: React.ComponentType
  } & (
    | { color?: string; theme?: never }
    | { color?: never; theme: Record<keyof typeof THEMES, string> }
  )
}
type ChartContextProps = {
  config: ChartConfig
}
const ChartContext = React.createContext<ChartContextProps | null>(null)
function useChart() {
  const context = React.useContext(ChartContext)
  if (!context) {
    throw new Error("useChart must be used within a <ChartContainer />")
  }
  return context
}
const ChartContainer = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> & {
    config: ChartConfig
    children: React.ComponentProps<
      typeof RechartsPrimitive.ResponsiveContainer
    >["children"]
  }
>(({ id, className, children, config, ...props }, ref) => {
  const uniqueId = React.useId()
  const chartId = `chart-${id || uniqueId.replace(/:/g, "")}`
  return (
    <ChartContext.Provider value={{ config }}>
      <div
        data-chart={chartId}
        ref={ref}
        className={cn(
          "flex aspect-video justify-center text-xs [&_.recharts-cartesian-axis-tick_text]:fill-muted-foreground [&_.recharts-cartesian-grid_line[stroke='#ccc']]:stroke-border/50 [&_.recharts-curve.recharts-tooltip-cursor]:stroke-border [&_.recharts-dot[stroke='#fff']]:stroke-transparent [&_.recharts-layer]:outline-none [&_.recharts-polar-grid_[stroke='#ccc']]:stroke-border [&_.recharts-radial-bar-background-sector]:fill-muted [&_.recharts-rectangle.recharts-tooltip-cursor]:fill-muted [&_.recharts-reference-line_[stroke='#ccc']]:stroke-border [&_.recharts-sector[stroke='#fff']]:stroke-transparent [&_.recharts-sector]:outline-none [&_.recharts-surface]:outline-none",
          className
        )}
        {...props}
      >
        <ChartStyle id={chartId} config={config} />
        <RechartsPrimitive.ResponsiveContainer>
          {children}
        </RechartsPrimitive.ResponsiveContainer>
      </div>
    </ChartContext.Provider>
  )
})
ChartContainer.displayName = "Chart"
const ChartStyle = ({ id, config }: { id: string; config: ChartConfig }) => {
  const colorConfig = Object.entries(config).filter(
    ([_, config]) => config.theme || config.color
  )
  if (!colorConfig.length) {
    return null
  }
  return (
    <style
      dangerouslySetInnerHTML={{
        __html: Object.entries(THEMES)
          .map(
            ([theme, prefix]) => `
${prefix} [data-chart=${id}] {
${colorConfig
  .map(([key, itemConfig]) => {
    const color =
      itemConfig.theme?.[theme as keyof typeof itemConfig.theme] ||
      itemConfig.color
    return color ? `  --color-${key}: ${color};` : null
  })
  .join("\n")}
}
`
          )
          .join("\n"),
      }}
    />
  )
}
const ChartTooltip = RechartsPrimitive.Tooltip
const ChartTooltipContent = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<typeof RechartsPrimitive.Tooltip> &
    React.ComponentProps<"div"> & {
      hideLabel?: boolean
      hideIndicator?: boolean
      indicator?: "line" | "dot" | "dashed"
      nameKey?: string
      labelKey?: string
    }
>(
  (
    {
      active,
      payload,
      className,
      indicator = "dot",
      hideLabel = false,
      hideIndicator = false,
      label,
      labelFormatter,
      labelClassName,
      formatter,
      color,
      nameKey,
      labelKey,
    },
    ref
  ) => {
    const { config } = useChart()
    const tooltipLabel = React.useMemo(() => {
      if (hideLabel || !payload?.length) {
        return null
      }
      const [item] = payload
      const key = `${labelKey || item.dataKey || item.name || "value"}`
      const itemConfig = getPayloadConfigFromPayload(config, item, key)
      const value =
        !labelKey && typeof label === "string"
          ? config[label as keyof typeof config]?.label || label
          : itemConfig?.label
      if (labelFormatter) {
        return (
          <div className={cn("font-medium", labelClassName)}>
            {labelFormatter(value, payload)}
          </div>
        )
      }
      if (!value) {
        return null
      }
      return <div className={cn("font-medium", labelClassName)}>{value}</div>
    }, [
      label,
      labelFormatter,
      payload,
      hideLabel,
      labelClassName,
      config,
      labelKey,
    ])
    if (!active || !payload?.length) {
      return null
    }
    const nestLabel = payload.length === 1 && indicator !== "dot"
    return (
      <div
        ref={ref}
        className={cn(
          "grid min-w-[8rem] items-start gap-1.5 rounded-lg border border-border/50 bg-background px-2.5 py-1.5 text-xs shadow-xl",
          className
        )}
      >
        {!nestLabel ? tooltipLabel : null}
        <div className="grid gap-1.5">
          {payload.map((item, index) => {
            const key = `${nameKey || item.name || item.dataKey || "value"}`
            const itemConfig = getPayloadConfigFromPayload(config, item, key)
            const indicatorColor = color || item.payload.fill || item.color
            return (
              <div
                key={item.dataKey}
                className={cn(
                  "flex w-full flex-wrap items-stretch gap-2 [&>svg]:h-2.5 [&>svg]:w-2.5 [&>svg]:text-muted-foreground",
                  indicator === "dot" && "items-center"
                )}
              >
                {formatter && item?.value !== undefined && item.name ? (
                  formatter(item.value, item.name, item, index, item.payload)
                ) : (
                  <>
                    {itemConfig?.icon ? (
                      <itemConfig.icon />
                    ) : (
                      !hideIndicator && (
                        <div
                          className={cn(
                            "shrink-0 rounded-[2px] border-[--color-border] bg-[--color-bg]",
                            {
                              "h-2.5 w-2.5": indicator === "dot",
                              "w-1": indicator === "line",
                              "w-0 border-[1.5px] border-dashed bg-transparent":
                                indicator === "dashed",
                              "my-0.5": nestLabel && indicator === "dashed",
                            }
                          )}
                          style={
                            {
                              "--color-bg": indicatorColor,
                              "--color-border": indicatorColor,
                            } as React.CSSProperties
                          }
                        />
                      )
                    )}
                    <div
                      className={cn(
                        "flex flex-1 justify-between leading-none",
                        nestLabel ? "items-end" : "items-center"
                      )}
                    >
                      <div className="grid gap-1.5">
                        {nestLabel ? tooltipLabel : null}
                        <span className="text-muted-foreground">
                          {itemConfig?.label || item.name}
                        </span>
                      </div>
                      {item.value && (
                        <span className="font-mono font-medium tabular-nums text-foreground">
                          {item.value.toLocaleString()}
                        </span>
                      )}
                    </div>
                  </>
                )}
              </div>
            )
          })}
        </div>
      </div>
    )
  }
)
ChartTooltipContent.displayName = "ChartTooltip"
const ChartLegend = RechartsPrimitive.Legend
const ChartLegendContent = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> &
    Pick<RechartsPrimitive.LegendProps, "payload" | "verticalAlign"> & {
      hideIcon?: boolean
      nameKey?: string
    }
>(
  (
    { className, hideIcon = false, payload, verticalAlign = "bottom", nameKey },
    ref
  ) => {
    const { config } = useChart()
    if (!payload?.length) {
      return null
    }
    return (
      <div
        ref={ref}
        className={cn(
          "flex items-center justify-center gap-4",
          verticalAlign === "top" ? "pb-3" : "pt-3",
          className
        )}
      >
        {payload.map((item) => {
          const key = `${nameKey || item.dataKey || "value"}`
          const itemConfig = getPayloadConfigFromPayload(config, item, key)
          return (
            <div
              key={item.value}
              className={cn(
                "flex items-center gap-1.5 [&>svg]:h-3 [&>svg]:w-3 [&>svg]:text-muted-foreground"
              )}
            >
              {itemConfig?.icon && !hideIcon ? (
                <itemConfig.icon />
              ) : (
                <div
                  className="h-2 w-2 shrink-0 rounded-[2px]"
                  style={{
                    backgroundColor: item.color,
                  }}
                />
              )}
              {itemConfig?.label}
            </div>
          )
        })}
      </div>
    )
  }
)
ChartLegendContent.displayName = "ChartLegend"
// Helper to extract item config from a payload.
function getPayloadConfigFromPayload(
  config: ChartConfig,
  payload: unknown,
  key: string
) {
  if (typeof payload !== "object" || payload === null) {
    return undefined
  }
  const payloadPayload =
    "payload" in payload &&
    typeof payload.payload === "object" &&
    payload.payload !== null
      ? payload.payload
      : undefined
  let configLabelKey: string = key
  if (
    key in payload &&
    typeof payload[key as keyof typeof payload] === "string"
  ) {
    configLabelKey = payload[key as keyof typeof payload] as string
  } else if (
    payloadPayload &&
    key in payloadPayload &&
    typeof payloadPayload[key as keyof typeof payloadPayload] === "string"
  ) {
    configLabelKey = payloadPayload[
      key as keyof typeof payloadPayload
    ] as string
  }
  return configLabelKey in config
    ? config[configLabelKey]
    : config[key as keyof typeof config]
}
export {
  ChartContainer,
  ChartTooltip,
  ChartTooltipContent,
  ChartLegend,
  ChartLegendContent,
  ChartStyle,
}
</file>

<file path="src/components/ui/checkbox.tsx">
import * as React from "react"
import * as CheckboxPrimitive from "@radix-ui/react-checkbox"
import { Check } from "lucide-react"
import { cn } from "@/lib/utils"
const Checkbox = React.forwardRef<
  React.ElementRef<typeof CheckboxPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof CheckboxPrimitive.Root>
>(({ className, ...props }, ref) => (
  <CheckboxPrimitive.Root
    ref={ref}
    className={cn(
      "peer h-4 w-4 shrink-0 rounded-sm border border-primary ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-primary data-[state=checked]:text-primary-foreground",
      className
    )}
    {...props}
  >
    <CheckboxPrimitive.Indicator
      className={cn("flex items-center justify-center text-current")}
    >
      <Check className="h-4 w-4" />
    </CheckboxPrimitive.Indicator>
  </CheckboxPrimitive.Root>
))
Checkbox.displayName = CheckboxPrimitive.Root.displayName
export { Checkbox }
</file>

<file path="src/components/ui/collapsible.tsx">
import * as CollapsiblePrimitive from "@radix-ui/react-collapsible"
const Collapsible = CollapsiblePrimitive.Root
const CollapsibleTrigger = CollapsiblePrimitive.CollapsibleTrigger
const CollapsibleContent = CollapsiblePrimitive.CollapsibleContent
export { Collapsible, CollapsibleTrigger, CollapsibleContent }
</file>

<file path="src/components/ui/command.tsx">
import * as React from "react"
import { type DialogProps } from "@radix-ui/react-dialog"
import { Command as CommandPrimitive } from "cmdk"
import { Search } from "lucide-react"
import { cn } from "@/lib/utils"
import { Dialog, DialogContent } from "@/components/ui/dialog"
const Command = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive>
>(({ className, ...props }, ref) => (
  <CommandPrimitive
    ref={ref}
    className={cn(
      "flex h-full w-full flex-col overflow-hidden rounded-md bg-popover text-popover-foreground",
      className
    )}
    {...props}
  />
))
Command.displayName = CommandPrimitive.displayName
interface CommandDialogProps extends DialogProps {}
const CommandDialog = ({ children, ...props }: CommandDialogProps) => {
  return (
    <Dialog {...props}>
      <DialogContent className="overflow-hidden p-0 shadow-lg">
        <Command className="[&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group-heading]]:text-muted-foreground [&_[cmdk-group]:not([hidden])_~[cmdk-group]]:pt-0 [&_[cmdk-group]]:px-2 [&_[cmdk-input-wrapper]_svg]:h-5 [&_[cmdk-input-wrapper]_svg]:w-5 [&_[cmdk-input]]:h-12 [&_[cmdk-item]]:px-2 [&_[cmdk-item]]:py-3 [&_[cmdk-item]_svg]:h-5 [&_[cmdk-item]_svg]:w-5">
          {children}
        </Command>
      </DialogContent>
    </Dialog>
  )
}
const CommandInput = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Input>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Input>
>(({ className, ...props }, ref) => (
  <div className="flex items-center border-b px-3" cmdk-input-wrapper="">
    <Search className="mr-2 h-4 w-4 shrink-0 opacity-50" />
    <CommandPrimitive.Input
      ref={ref}
      className={cn(
        "flex h-11 w-full rounded-md bg-transparent py-3 text-sm outline-none placeholder:text-muted-foreground disabled:cursor-not-allowed disabled:opacity-50",
        className
      )}
      {...props}
    />
  </div>
))
CommandInput.displayName = CommandPrimitive.Input.displayName
const CommandList = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.List>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.List
    ref={ref}
    className={cn("max-h-[300px] overflow-y-auto overflow-x-hidden", className)}
    {...props}
  />
))
CommandList.displayName = CommandPrimitive.List.displayName
const CommandEmpty = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Empty>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Empty>
>((props, ref) => (
  <CommandPrimitive.Empty
    ref={ref}
    className="py-6 text-center text-sm"
    {...props}
  />
))
CommandEmpty.displayName = CommandPrimitive.Empty.displayName
const CommandGroup = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Group>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Group>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Group
    ref={ref}
    className={cn(
      "overflow-hidden p-1 text-foreground [&_[cmdk-group-heading]]:px-2 [&_[cmdk-group-heading]]:py-1.5 [&_[cmdk-group-heading]]:text-xs [&_[cmdk-group-heading]]:font-medium [&_[cmdk-group-heading]]:text-muted-foreground",
      className
    )}
    {...props}
  />
))
CommandGroup.displayName = CommandPrimitive.Group.displayName
const CommandSeparator = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 h-px bg-border", className)}
    {...props}
  />
))
CommandSeparator.displayName = CommandPrimitive.Separator.displayName
const CommandItem = React.forwardRef<
  React.ElementRef<typeof CommandPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof CommandPrimitive.Item>
>(({ className, ...props }, ref) => (
  <CommandPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none data-[disabled=true]:pointer-events-none data-[selected='true']:bg-accent data-[selected=true]:text-accent-foreground data-[disabled=true]:opacity-50",
      className
    )}
    {...props}
  />
))
CommandItem.displayName = CommandPrimitive.Item.displayName
const CommandShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn(
        "ml-auto text-xs tracking-widest text-muted-foreground",
        className
      )}
      {...props}
    />
  )
}
CommandShortcut.displayName = "CommandShortcut"
export {
  Command,
  CommandDialog,
  CommandInput,
  CommandList,
  CommandEmpty,
  CommandGroup,
  CommandItem,
  CommandShortcut,
  CommandSeparator,
}
</file>

<file path="src/components/ui/context-menu.tsx">
import * as React from "react"
import * as ContextMenuPrimitive from "@radix-ui/react-context-menu"
import { Check, ChevronRight, Circle } from "lucide-react"
import { cn } from "@/lib/utils"
const ContextMenu = ContextMenuPrimitive.Root
const ContextMenuTrigger = ContextMenuPrimitive.Trigger
const ContextMenuGroup = ContextMenuPrimitive.Group
const ContextMenuPortal = ContextMenuPrimitive.Portal
const ContextMenuSub = ContextMenuPrimitive.Sub
const ContextMenuRadioGroup = ContextMenuPrimitive.RadioGroup
const ContextMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.SubTrigger> & {
    inset?: boolean
  }
>(({ className, inset, children, ...props }, ref) => (
  <ContextMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground",
      inset && "pl-8",
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto h-4 w-4" />
  </ContextMenuPrimitive.SubTrigger>
))
ContextMenuSubTrigger.displayName = ContextMenuPrimitive.SubTrigger.displayName
const ContextMenuSubContent = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <ContextMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
ContextMenuSubContent.displayName = ContextMenuPrimitive.SubContent.displayName
const ContextMenuContent = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.Content>
>(({ className, ...props }, ref) => (
  <ContextMenuPrimitive.Portal>
    <ContextMenuPrimitive.Content
      ref={ref}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md animate-in fade-in-80 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </ContextMenuPrimitive.Portal>
))
ContextMenuContent.displayName = ContextMenuPrimitive.Content.displayName
const ContextMenuItem = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.Item> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <ContextMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
ContextMenuItem.displayName = ContextMenuPrimitive.Item.displayName
const ContextMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <ContextMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <ContextMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </ContextMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </ContextMenuPrimitive.CheckboxItem>
))
ContextMenuCheckboxItem.displayName =
  ContextMenuPrimitive.CheckboxItem.displayName
const ContextMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <ContextMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <ContextMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </ContextMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </ContextMenuPrimitive.RadioItem>
))
ContextMenuRadioItem.displayName = ContextMenuPrimitive.RadioItem.displayName
const ContextMenuLabel = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.Label> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <ContextMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold text-foreground",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
ContextMenuLabel.displayName = ContextMenuPrimitive.Label.displayName
const ContextMenuSeparator = React.forwardRef<
  React.ElementRef<typeof ContextMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof ContextMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <ContextMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-border", className)}
    {...props}
  />
))
ContextMenuSeparator.displayName = ContextMenuPrimitive.Separator.displayName
const ContextMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn(
        "ml-auto text-xs tracking-widest text-muted-foreground",
        className
      )}
      {...props}
    />
  )
}
ContextMenuShortcut.displayName = "ContextMenuShortcut"
export {
  ContextMenu,
  ContextMenuTrigger,
  ContextMenuContent,
  ContextMenuItem,
  ContextMenuCheckboxItem,
  ContextMenuRadioItem,
  ContextMenuLabel,
  ContextMenuSeparator,
  ContextMenuShortcut,
  ContextMenuGroup,
  ContextMenuPortal,
  ContextMenuSub,
  ContextMenuSubContent,
  ContextMenuSubTrigger,
  ContextMenuRadioGroup,
}
</file>

<file path="src/components/ui/context-window.tsx">
import React, { useState, useCallback, useRef, useEffect } from "react";
import { Command } from "cmdk";
import { Search, X, ExternalLink, Code, BookOpen } from "lucide-react";
import { cn } from "@/lib/utils";
import { useHotkeys } from "@/hooks/use-hotkeys";
interface ContextWindowProps {
  className?: string;
}
export function ContextWindow({ className }: ContextWindowProps) {
  const [open, setOpen] = useState(false);
  const [search, setSearch] = useState("");
  const [loading, setLoading] = useState(false);
  const [results, setResults] = useState<any[]>([]);
  const inputRef = useRef<HTMLInputElement>(null);
  // Toggle with ⌘K
  useHotkeys("meta+k", (e) => {
    e.preventDefault();
    setOpen((open) => !open);
  });
  // Close with escape
  useHotkeys("escape", () => {
    if (open) setOpen(false);
  });
  const searchPerplexity = useCallback(async (query: string) => {
    if (!query) return;
    setLoading(true);
    try {
      // Replace with your actual Perplexity API endpoint
      const response = await fetch(
        `/api/perplexity-search?q=${encodeURIComponent(query)}`
      );
      const data = await response.json();
      setResults(data.results);
    } catch (error) {
      console.error("Perplexity search error:", error);
      setResults([]);
    } finally {
      setLoading(false);
    }
  }, []);
  // Debounced search
  useEffect(() => {
    const timer = setTimeout(() => {
      if (search) {
        searchPerplexity(search);
      }
    }, 300);
    return () => clearTimeout(timer);
  }, [search, searchPerplexity]);
  if (!open) return null;
  return (
    <div className="fixed inset-0 z-50 bg-background/80 backdrop-blur-sm">
      <div className="fixed inset-x-0 top-[10%] mx-auto max-w-2xl overflow-hidden rounded-xl bg-background shadow-lg border border-border">
        <Command
          className={cn(
            "flex h-full w-full flex-col overflow-hidden",
            className
          )}
          loop
        >
          <div className="flex items-center border-b border-border px-3">
            <Search className="mr-2 h-4 w-4 shrink-0 opacity-50" />
            <Command.Input
              ref={inputRef}
              value={search}
              onValueChange={setSearch}
              placeholder="Search documentation, APIs, or ask questions..."
              className="flex h-11 w-full rounded-md bg-transparent py-3 text-sm outline-none placeholder:text-muted-foreground disabled:cursor-not-allowed disabled:opacity-50"
            />
            {search && (
              <button
                onClick={() => setSearch("")}
                className="ml-2 p-1 rounded-md hover:bg-accent"
              >
                <X className="h-4 w-4 opacity-50" />
              </button>
            )}
          </div>
          <Command.List className="max-h-[300px] overflow-y-auto overflow-x-hidden">
            <Command.Empty className="py-6 text-center text-sm">
              {loading ? "Searching..." : "No results found."}
            </Command.Empty>
            {results.map((result, index) => (
              <Command.Item
                key={index}
                value={result.title}
                className="flex items-center px-4 py-2 hover:bg-accent cursor-pointer"
                onSelect={() => {
                  // Handle result selection
                  if (result.url) {
                    window.open(result.url, "_blank");
                  }
                }}
              >
                {result.type === "documentation" ? (
                  <BookOpen className="mr-2 h-4 w-4" />
                ) : (
                  <Code className="mr-2 h-4 w-4" />
                )}
                <div className="flex flex-col">
                  <span className="text-sm font-medium">{result.title}</span>
                  <span className="text-xs text-muted-foreground">
                    {result.description}
                  </span>
                </div>
                {result.url && (
                  <ExternalLink className="ml-auto h-4 w-4 opacity-50" />
                )}
              </Command.Item>
            ))}
          </Command.List>
        </Command>
      </div>
    </div>
  );
}
</file>

<file path="src/components/ui/dialog.tsx">
import * as React from "react"
import * as DialogPrimitive from "@radix-ui/react-dialog"
import { X } from "lucide-react"
import { cn } from "@/lib/utils"
const Dialog = DialogPrimitive.Root
const DialogTrigger = DialogPrimitive.Trigger
const DialogPortal = DialogPrimitive.Portal
const DialogClose = DialogPrimitive.Close
const DialogOverlay = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Overlay
    ref={ref}
    className={cn(
      "fixed inset-0 z-50 bg-black/80 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className
    )}
    {...props}
  />
))
DialogOverlay.displayName = DialogPrimitive.Overlay.displayName
const DialogContent = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <DialogPortal>
    <DialogOverlay />
    <DialogPrimitive.Content
      ref={ref}
      className={cn(
        "fixed left-[50%] top-[50%] z-50 grid w-full max-w-lg translate-x-[-50%] translate-y-[-50%] gap-4 border bg-background p-6 shadow-lg duration-200 data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[state=closed]:slide-out-to-left-1/2 data-[state=closed]:slide-out-to-top-[48%] data-[state=open]:slide-in-from-left-1/2 data-[state=open]:slide-in-from-top-[48%] sm:rounded-lg",
        className
      )}
      {...props}
    >
      {children}
      <DialogPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-accent data-[state=open]:text-muted-foreground">
        <X className="h-4 w-4" />
        <span className="sr-only">Close</span>
      </DialogPrimitive.Close>
    </DialogPrimitive.Content>
  </DialogPortal>
))
DialogContent.displayName = DialogPrimitive.Content.displayName
const DialogHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-1.5 text-center sm:text-left",
      className
    )}
    {...props}
  />
)
DialogHeader.displayName = "DialogHeader"
const DialogFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className
    )}
    {...props}
  />
)
DialogFooter.displayName = "DialogFooter"
const DialogTitle = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Title>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Title
    ref={ref}
    className={cn(
      "text-lg font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
DialogTitle.displayName = DialogPrimitive.Title.displayName
const DialogDescription = React.forwardRef<
  React.ElementRef<typeof DialogPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof DialogPrimitive.Description>
>(({ className, ...props }, ref) => (
  <DialogPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
DialogDescription.displayName = DialogPrimitive.Description.displayName
export {
  Dialog,
  DialogPortal,
  DialogOverlay,
  DialogClose,
  DialogTrigger,
  DialogContent,
  DialogHeader,
  DialogFooter,
  DialogTitle,
  DialogDescription,
}
</file>

<file path="src/components/ui/drawer.tsx">
import * as React from "react"
import { Drawer as DrawerPrimitive } from "vaul"
import { cn } from "@/lib/utils"
const Drawer = ({
  shouldScaleBackground = true,
  ...props
}: React.ComponentProps<typeof DrawerPrimitive.Root>) => (
  <DrawerPrimitive.Root
    shouldScaleBackground={shouldScaleBackground}
    {...props}
  />
)
Drawer.displayName = "Drawer"
const DrawerTrigger = DrawerPrimitive.Trigger
const DrawerPortal = DrawerPrimitive.Portal
const DrawerClose = DrawerPrimitive.Close
const DrawerOverlay = React.forwardRef<
  React.ElementRef<typeof DrawerPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof DrawerPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <DrawerPrimitive.Overlay
    ref={ref}
    className={cn("fixed inset-0 z-50 bg-black/80", className)}
    {...props}
  />
))
DrawerOverlay.displayName = DrawerPrimitive.Overlay.displayName
const DrawerContent = React.forwardRef<
  React.ElementRef<typeof DrawerPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DrawerPrimitive.Content>
>(({ className, children, ...props }, ref) => (
  <DrawerPortal>
    <DrawerOverlay />
    <DrawerPrimitive.Content
      ref={ref}
      className={cn(
        "fixed inset-x-0 bottom-0 z-50 mt-24 flex h-auto flex-col rounded-t-[10px] border bg-background",
        className
      )}
      {...props}
    >
      <div className="mx-auto mt-4 h-2 w-[100px] rounded-full bg-muted" />
      {children}
    </DrawerPrimitive.Content>
  </DrawerPortal>
))
DrawerContent.displayName = "DrawerContent"
const DrawerHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn("grid gap-1.5 p-4 text-center sm:text-left", className)}
    {...props}
  />
)
DrawerHeader.displayName = "DrawerHeader"
const DrawerFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn("mt-auto flex flex-col gap-2 p-4", className)}
    {...props}
  />
)
DrawerFooter.displayName = "DrawerFooter"
const DrawerTitle = React.forwardRef<
  React.ElementRef<typeof DrawerPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof DrawerPrimitive.Title>
>(({ className, ...props }, ref) => (
  <DrawerPrimitive.Title
    ref={ref}
    className={cn(
      "text-lg font-semibold leading-none tracking-tight",
      className
    )}
    {...props}
  />
))
DrawerTitle.displayName = DrawerPrimitive.Title.displayName
const DrawerDescription = React.forwardRef<
  React.ElementRef<typeof DrawerPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof DrawerPrimitive.Description>
>(({ className, ...props }, ref) => (
  <DrawerPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
DrawerDescription.displayName = DrawerPrimitive.Description.displayName
export {
  Drawer,
  DrawerPortal,
  DrawerOverlay,
  DrawerTrigger,
  DrawerClose,
  DrawerContent,
  DrawerHeader,
  DrawerFooter,
  DrawerTitle,
  DrawerDescription,
}
</file>

<file path="src/components/ui/dropdown-menu.tsx">
import * as React from "react"
import * as DropdownMenuPrimitive from "@radix-ui/react-dropdown-menu"
import { Check, ChevronRight, Circle } from "lucide-react"
import { cn } from "@/lib/utils"
const DropdownMenu = DropdownMenuPrimitive.Root
const DropdownMenuTrigger = DropdownMenuPrimitive.Trigger
const DropdownMenuGroup = DropdownMenuPrimitive.Group
const DropdownMenuPortal = DropdownMenuPrimitive.Portal
const DropdownMenuSub = DropdownMenuPrimitive.Sub
const DropdownMenuRadioGroup = DropdownMenuPrimitive.RadioGroup
const DropdownMenuSubTrigger = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubTrigger> & {
    inset?: boolean
  }
>(({ className, inset, children, ...props }, ref) => (
  <DropdownMenuPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent data-[state=open]:bg-accent",
      inset && "pl-8",
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto h-4 w-4" />
  </DropdownMenuPrimitive.SubTrigger>
))
DropdownMenuSubTrigger.displayName =
  DropdownMenuPrimitive.SubTrigger.displayName
const DropdownMenuSubContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
DropdownMenuSubContent.displayName =
  DropdownMenuPrimitive.SubContent.displayName
const DropdownMenuContent = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <DropdownMenuPrimitive.Portal>
    <DropdownMenuPrimitive.Content
      ref={ref}
      sideOffset={sideOffset}
      className={cn(
        "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </DropdownMenuPrimitive.Portal>
))
DropdownMenuContent.displayName = DropdownMenuPrimitive.Content.displayName
const DropdownMenuItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Item> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuItem.displayName = DropdownMenuPrimitive.Item.displayName
const DropdownMenuCheckboxItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <DropdownMenuPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.CheckboxItem>
))
DropdownMenuCheckboxItem.displayName =
  DropdownMenuPrimitive.CheckboxItem.displayName
const DropdownMenuRadioItem = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <DropdownMenuPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none transition-colors focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <DropdownMenuPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </DropdownMenuPrimitive.ItemIndicator>
    </span>
    {children}
  </DropdownMenuPrimitive.RadioItem>
))
DropdownMenuRadioItem.displayName = DropdownMenuPrimitive.RadioItem.displayName
const DropdownMenuLabel = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Label> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <DropdownMenuPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
DropdownMenuLabel.displayName = DropdownMenuPrimitive.Label.displayName
const DropdownMenuSeparator = React.forwardRef<
  React.ElementRef<typeof DropdownMenuPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof DropdownMenuPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <DropdownMenuPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
DropdownMenuSeparator.displayName = DropdownMenuPrimitive.Separator.displayName
const DropdownMenuShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn("ml-auto text-xs tracking-widest opacity-60", className)}
      {...props}
    />
  )
}
DropdownMenuShortcut.displayName = "DropdownMenuShortcut"
export {
  DropdownMenu,
  DropdownMenuTrigger,
  DropdownMenuContent,
  DropdownMenuItem,
  DropdownMenuCheckboxItem,
  DropdownMenuRadioItem,
  DropdownMenuLabel,
  DropdownMenuSeparator,
  DropdownMenuShortcut,
  DropdownMenuGroup,
  DropdownMenuPortal,
  DropdownMenuSub,
  DropdownMenuSubContent,
  DropdownMenuSubTrigger,
  DropdownMenuRadioGroup,
}
</file>

<file path="src/components/ui/form.tsx">
import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"
import { Slot } from "@radix-ui/react-slot"
import {
  Controller,
  ControllerProps,
  FieldPath,
  FieldValues,
  FormProvider,
  useFormContext,
} from "react-hook-form"
import { cn } from "@/lib/utils"
import { Label } from "@/components/ui/label"
const Form = FormProvider
type FormFieldContextValue<
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>
> = {
  name: TName
}
const FormFieldContext = React.createContext<FormFieldContextValue>(
  {} as FormFieldContextValue
)
const FormField = <
  TFieldValues extends FieldValues = FieldValues,
  TName extends FieldPath<TFieldValues> = FieldPath<TFieldValues>
>({
  ...props
}: ControllerProps<TFieldValues, TName>) => {
  return (
    <FormFieldContext.Provider value={{ name: props.name }}>
      <Controller {...props} />
    </FormFieldContext.Provider>
  )
}
const useFormField = () => {
  const fieldContext = React.useContext(FormFieldContext)
  const itemContext = React.useContext(FormItemContext)
  const { getFieldState, formState } = useFormContext()
  const fieldState = getFieldState(fieldContext.name, formState)
  if (!fieldContext) {
    throw new Error("useFormField should be used within <FormField>")
  }
  const { id } = itemContext
  return {
    id,
    name: fieldContext.name,
    formItemId: `${id}-form-item`,
    formDescriptionId: `${id}-form-item-description`,
    formMessageId: `${id}-form-item-message`,
    ...fieldState,
  }
}
type FormItemContextValue = {
  id: string
}
const FormItemContext = React.createContext<FormItemContextValue>(
  {} as FormItemContextValue
)
const FormItem = React.forwardRef<
  HTMLDivElement,
  React.HTMLAttributes<HTMLDivElement>
>(({ className, ...props }, ref) => {
  const id = React.useId()
  return (
    <FormItemContext.Provider value={{ id }}>
      <div ref={ref} className={cn("space-y-2", className)} {...props} />
    </FormItemContext.Provider>
  )
})
FormItem.displayName = "FormItem"
const FormLabel = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root>
>(({ className, ...props }, ref) => {
  const { error, formItemId } = useFormField()
  return (
    <Label
      ref={ref}
      className={cn(error && "text-destructive", className)}
      htmlFor={formItemId}
      {...props}
    />
  )
})
FormLabel.displayName = "FormLabel"
const FormControl = React.forwardRef<
  React.ElementRef<typeof Slot>,
  React.ComponentPropsWithoutRef<typeof Slot>
>(({ ...props }, ref) => {
  const { error, formItemId, formDescriptionId, formMessageId } = useFormField()
  return (
    <Slot
      ref={ref}
      id={formItemId}
      aria-describedby={
        !error
          ? `${formDescriptionId}`
          : `${formDescriptionId} ${formMessageId}`
      }
      aria-invalid={!!error}
      {...props}
    />
  )
})
FormControl.displayName = "FormControl"
const FormDescription = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, ...props }, ref) => {
  const { formDescriptionId } = useFormField()
  return (
    <p
      ref={ref}
      id={formDescriptionId}
      className={cn("text-sm text-muted-foreground", className)}
      {...props}
    />
  )
})
FormDescription.displayName = "FormDescription"
const FormMessage = React.forwardRef<
  HTMLParagraphElement,
  React.HTMLAttributes<HTMLParagraphElement>
>(({ className, children, ...props }, ref) => {
  const { error, formMessageId } = useFormField()
  const body = error ? String(error?.message) : children
  if (!body) {
    return null
  }
  return (
    <p
      ref={ref}
      id={formMessageId}
      className={cn("text-sm font-medium text-destructive", className)}
      {...props}
    >
      {body}
    </p>
  )
})
FormMessage.displayName = "FormMessage"
export {
  useFormField,
  Form,
  FormItem,
  FormLabel,
  FormControl,
  FormDescription,
  FormMessage,
  FormField,
}
</file>

<file path="src/components/ui/hover-card.tsx">
import * as React from "react"
import * as HoverCardPrimitive from "@radix-ui/react-hover-card"
import { cn } from "@/lib/utils"
const HoverCard = HoverCardPrimitive.Root
const HoverCardTrigger = HoverCardPrimitive.Trigger
const HoverCardContent = React.forwardRef<
  React.ElementRef<typeof HoverCardPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof HoverCardPrimitive.Content>
>(({ className, align = "center", sideOffset = 4, ...props }, ref) => (
  <HoverCardPrimitive.Content
    ref={ref}
    align={align}
    sideOffset={sideOffset}
    className={cn(
      "z-50 w-64 rounded-md border bg-popover p-4 text-popover-foreground shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
HoverCardContent.displayName = HoverCardPrimitive.Content.displayName
export { HoverCard, HoverCardTrigger, HoverCardContent }
</file>

<file path="src/components/ui/input-otp.tsx">
import * as React from "react"
import { OTPInput, OTPInputContext } from "input-otp"
import { Dot } from "lucide-react"
import { cn } from "@/lib/utils"
const InputOTP = React.forwardRef<
  React.ElementRef<typeof OTPInput>,
  React.ComponentPropsWithoutRef<typeof OTPInput>
>(({ className, containerClassName, ...props }, ref) => (
  <OTPInput
    ref={ref}
    containerClassName={cn(
      "flex items-center gap-2 has-[:disabled]:opacity-50",
      containerClassName
    )}
    className={cn("disabled:cursor-not-allowed", className)}
    {...props}
  />
))
InputOTP.displayName = "InputOTP"
const InputOTPGroup = React.forwardRef<
  React.ElementRef<"div">,
  React.ComponentPropsWithoutRef<"div">
>(({ className, ...props }, ref) => (
  <div ref={ref} className={cn("flex items-center", className)} {...props} />
))
InputOTPGroup.displayName = "InputOTPGroup"
const InputOTPSlot = React.forwardRef<
  React.ElementRef<"div">,
  React.ComponentPropsWithoutRef<"div"> & { index: number }
>(({ index, className, ...props }, ref) => {
  const inputOTPContext = React.useContext(OTPInputContext)
  const { char, hasFakeCaret, isActive } = inputOTPContext.slots[index]
  return (
    <div
      ref={ref}
      className={cn(
        "relative flex h-10 w-10 items-center justify-center border-y border-r border-input text-sm transition-all first:rounded-l-md first:border-l last:rounded-r-md",
        isActive && "z-10 ring-2 ring-ring ring-offset-background",
        className
      )}
      {...props}
    >
      {char}
      {hasFakeCaret && (
        <div className="pointer-events-none absolute inset-0 flex items-center justify-center">
          <div className="h-4 w-px animate-caret-blink bg-foreground duration-1000" />
        </div>
      )}
    </div>
  )
})
InputOTPSlot.displayName = "InputOTPSlot"
const InputOTPSeparator = React.forwardRef<
  React.ElementRef<"div">,
  React.ComponentPropsWithoutRef<"div">
>(({ ...props }, ref) => (
  <div ref={ref} role="separator" {...props}>
    <Dot />
  </div>
))
InputOTPSeparator.displayName = "InputOTPSeparator"
export { InputOTP, InputOTPGroup, InputOTPSlot, InputOTPSeparator }
</file>

<file path="src/components/ui/input.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Input                                                                               ║
 * ║ Description: Reusable input component with consistent styling and accessibility                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import { useForm } from "react-hook-form"
import { zodResolver } from "@hookform/resolvers/zod"
import * as z from "zod"
import { cn } from "@/lib/utils"
import {
  Form,
  FormControl,
  FormDescription,
  FormField,
  FormItem,
  FormLabel,
  FormMessage,
} from "@/components/ui/form"
import { Button } from "@/components/ui/button"
export interface InputProps
  extends React.InputHTMLAttributes<HTMLInputElement> {
  error?: boolean
  icon?: React.ReactNode
}
const Input = React.forwardRef<HTMLInputElement, InputProps>(
  ({ className, type, error, icon, ...props }, ref) => {
    // Create wrapper classes for icon positioning
    const wrapperClasses = cn(
      "relative",
      icon && "inline-flex items-center",
      className
    )
    // Create input classes with error state handling
    const inputClasses = cn(
      "flex h-9 w-full rounded-md border border-input bg-background px-3 py-1 text-sm shadow-sm transition-colors",
      "file:border-0 file:bg-transparent file:text-sm file:font-medium",
      "placeholder:text-muted-foreground",
      "focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring",
      "disabled:cursor-not-allowed disabled:opacity-50",
      error && "border-destructive focus-visible:ring-destructive",
      icon && "pl-10", // Add padding when icon is present
      className
    )
    return (
      <div className={wrapperClasses}>
        {icon && (
          <div className="absolute left-3 top-1/2 -translate-y-1/2 transform text-muted-foreground">
            {icon}
          </div>
        )}
        <input
          type={type}
          className={inputClasses}
          ref={ref}
          {...props}
        />
      </div>
    )
  }
)
Input.displayName = "Input"
export { Input }
// Define your form schema
const formSchema = z.object({
  username: z.string().min(2, {
    message: "Username must be at least 2 characters.",
  }),
  email: z.string().email({
    message: "Please enter a valid email address.",
  }),
})
export function ProfileForm() {
  // Initialize form with zod resolver
  const form = useForm<z.infer<typeof formSchema>>({
    resolver: zodResolver(formSchema),
    defaultValues: {
      username: "",
      email: "",
    },
  })
  // Form submission handler
  function onSubmit(values: z.infer<typeof formSchema>) {
    console.log(values)
  }
  return (
    <Form {...form}>
      <form onSubmit={form.handleSubmit(onSubmit)} className="space-y-8">
        <FormField
          control={form.control}
          name="username"
          render={({ field }) => (
            <FormItem>
              <FormLabel>Username</FormLabel>
              <FormControl>
                <Input placeholder="Enter your username" {...field} />
              </FormControl>
              <FormDescription>
                This is your public display name.
              </FormDescription>
              <FormMessage />
            </FormItem>
          )}
        />
        <FormField
          control={form.control}
          name="email"
          render={({ field }) => (
            <FormItem>
              <FormLabel>Email</FormLabel>
              <FormControl>
                <Input placeholder="Enter your email" {...field} />
              </FormControl>
              <FormDescription>
                We'll never share your email with anyone else.
              </FormDescription>
              <FormMessage />
            </FormItem>
          )}
        />
        <Button type="submit">Update profile</Button>
      </form>
    </Form>
  )
}
</file>

<file path="src/components/ui/label.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Label                                                                               ║
 * ║ Description: Form label component with Radix UI integration and consistent styling             ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as LabelPrimitive from "@radix-ui/react-label"
import { cva, type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
const labelVariants = cva(
  "text-sm font-medium leading-none peer-disabled:cursor-not-allowed peer-disabled:opacity-70"
)
const Label = React.forwardRef<
  React.ElementRef<typeof LabelPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof LabelPrimitive.Root> &
    VariantProps<typeof labelVariants>
>(({ className, ...props }, ref) => (
  <LabelPrimitive.Root
    ref={ref}
    className={cn(labelVariants(), className)}
    {...props}
  />
))
Label.displayName = LabelPrimitive.Root.displayName
export { Label }
</file>

<file path="src/components/ui/menubar.tsx">
import * as React from "react"
import * as MenubarPrimitive from "@radix-ui/react-menubar"
import { Check, ChevronRight, Circle } from "lucide-react"
import { cn } from "@/lib/utils"
const MenubarMenu = MenubarPrimitive.Menu
const MenubarGroup = MenubarPrimitive.Group
const MenubarPortal = MenubarPrimitive.Portal
const MenubarSub = MenubarPrimitive.Sub
const MenubarRadioGroup = MenubarPrimitive.RadioGroup
const Menubar = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Root>
>(({ className, ...props }, ref) => (
  <MenubarPrimitive.Root
    ref={ref}
    className={cn(
      "flex h-10 items-center space-x-1 rounded-md border bg-background p-1",
      className
    )}
    {...props}
  />
))
Menubar.displayName = MenubarPrimitive.Root.displayName
const MenubarTrigger = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Trigger>
>(({ className, ...props }, ref) => (
  <MenubarPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-3 py-1.5 text-sm font-medium outline-none focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground",
      className
    )}
    {...props}
  />
))
MenubarTrigger.displayName = MenubarPrimitive.Trigger.displayName
const MenubarSubTrigger = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.SubTrigger>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.SubTrigger> & {
    inset?: boolean
  }
>(({ className, inset, children, ...props }, ref) => (
  <MenubarPrimitive.SubTrigger
    ref={ref}
    className={cn(
      "flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[state=open]:bg-accent data-[state=open]:text-accent-foreground",
      inset && "pl-8",
      className
    )}
    {...props}
  >
    {children}
    <ChevronRight className="ml-auto h-4 w-4" />
  </MenubarPrimitive.SubTrigger>
))
MenubarSubTrigger.displayName = MenubarPrimitive.SubTrigger.displayName
const MenubarSubContent = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.SubContent>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.SubContent>
>(({ className, ...props }, ref) => (
  <MenubarPrimitive.SubContent
    ref={ref}
    className={cn(
      "z-50 min-w-[8rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
MenubarSubContent.displayName = MenubarPrimitive.SubContent.displayName
const MenubarContent = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Content>
>(
  (
    { className, align = "start", alignOffset = -4, sideOffset = 8, ...props },
    ref
  ) => (
    <MenubarPrimitive.Portal>
      <MenubarPrimitive.Content
        ref={ref}
        align={align}
        alignOffset={alignOffset}
        sideOffset={sideOffset}
        className={cn(
          "z-50 min-w-[12rem] overflow-hidden rounded-md border bg-popover p-1 text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
          className
        )}
        {...props}
      />
    </MenubarPrimitive.Portal>
  )
)
MenubarContent.displayName = MenubarPrimitive.Content.displayName
const MenubarItem = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Item> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <MenubarPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm px-2 py-1.5 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
MenubarItem.displayName = MenubarPrimitive.Item.displayName
const MenubarCheckboxItem = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.CheckboxItem>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.CheckboxItem>
>(({ className, children, checked, ...props }, ref) => (
  <MenubarPrimitive.CheckboxItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    checked={checked}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <MenubarPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </MenubarPrimitive.ItemIndicator>
    </span>
    {children}
  </MenubarPrimitive.CheckboxItem>
))
MenubarCheckboxItem.displayName = MenubarPrimitive.CheckboxItem.displayName
const MenubarRadioItem = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.RadioItem>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.RadioItem>
>(({ className, children, ...props }, ref) => (
  <MenubarPrimitive.RadioItem
    ref={ref}
    className={cn(
      "relative flex cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <MenubarPrimitive.ItemIndicator>
        <Circle className="h-2 w-2 fill-current" />
      </MenubarPrimitive.ItemIndicator>
    </span>
    {children}
  </MenubarPrimitive.RadioItem>
))
MenubarRadioItem.displayName = MenubarPrimitive.RadioItem.displayName
const MenubarLabel = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Label> & {
    inset?: boolean
  }
>(({ className, inset, ...props }, ref) => (
  <MenubarPrimitive.Label
    ref={ref}
    className={cn(
      "px-2 py-1.5 text-sm font-semibold",
      inset && "pl-8",
      className
    )}
    {...props}
  />
))
MenubarLabel.displayName = MenubarPrimitive.Label.displayName
const MenubarSeparator = React.forwardRef<
  React.ElementRef<typeof MenubarPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof MenubarPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <MenubarPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
MenubarSeparator.displayName = MenubarPrimitive.Separator.displayName
const MenubarShortcut = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLSpanElement>) => {
  return (
    <span
      className={cn(
        "ml-auto text-xs tracking-widest text-muted-foreground",
        className
      )}
      {...props}
    />
  )
}
MenubarShortcut.displayname = "MenubarShortcut"
export {
  Menubar,
  MenubarMenu,
  MenubarTrigger,
  MenubarContent,
  MenubarItem,
  MenubarSeparator,
  MenubarLabel,
  MenubarCheckboxItem,
  MenubarRadioGroup,
  MenubarRadioItem,
  MenubarPortal,
  MenubarSubContent,
  MenubarSubTrigger,
  MenubarGroup,
  MenubarSub,
  MenubarShortcut,
}
</file>

<file path="src/components/ui/navigation-menu.tsx">
import * as React from "react"
import * as NavigationMenuPrimitive from "@radix-ui/react-navigation-menu"
import { cva } from "class-variance-authority"
import { ChevronDown } from "lucide-react"
import { cn } from "@/lib/utils"
const NavigationMenu = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.Root>
>(({ className, children, ...props }, ref) => (
  <NavigationMenuPrimitive.Root
    ref={ref}
    className={cn(
      "relative z-10 flex max-w-max flex-1 items-center justify-center",
      className
    )}
    {...props}
  >
    {children}
    <NavigationMenuViewport />
  </NavigationMenuPrimitive.Root>
))
NavigationMenu.displayName = NavigationMenuPrimitive.Root.displayName
const NavigationMenuList = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.List>
>(({ className, ...props }, ref) => (
  <NavigationMenuPrimitive.List
    ref={ref}
    className={cn(
      "group flex flex-1 list-none items-center justify-center space-x-1",
      className
    )}
    {...props}
  />
))
NavigationMenuList.displayName = NavigationMenuPrimitive.List.displayName
const NavigationMenuItem = NavigationMenuPrimitive.Item
const navigationMenuTriggerStyle = cva(
  "group inline-flex h-10 w-max items-center justify-center rounded-md bg-background px-4 py-2 text-sm font-medium transition-colors hover:bg-accent hover:text-accent-foreground focus:bg-accent focus:text-accent-foreground focus:outline-none disabled:pointer-events-none disabled:opacity-50 data-[active]:bg-accent/50 data-[state=open]:bg-accent/50"
)
const NavigationMenuTrigger = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <NavigationMenuPrimitive.Trigger
    ref={ref}
    className={cn(navigationMenuTriggerStyle(), "group", className)}
    {...props}
  >
    {children}{" "}
    <ChevronDown
      className="relative top-[1px] ml-1 h-3 w-3 transition duration-200 group-data-[state=open]:rotate-180"
      aria-hidden="true"
    />
  </NavigationMenuPrimitive.Trigger>
))
NavigationMenuTrigger.displayName = NavigationMenuPrimitive.Trigger.displayName
const NavigationMenuContent = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.Content>
>(({ className, ...props }, ref) => (
  <NavigationMenuPrimitive.Content
    ref={ref}
    className={cn(
      "left-0 top-0 w-full data-[motion^=from-]:animate-in data-[motion^=to-]:animate-out data-[motion^=from-]:fade-in data-[motion^=to-]:fade-out data-[motion=from-end]:slide-in-from-right-52 data-[motion=from-start]:slide-in-from-left-52 data-[motion=to-end]:slide-out-to-right-52 data-[motion=to-start]:slide-out-to-left-52 md:absolute md:w-auto ",
      className
    )}
    {...props}
  />
))
NavigationMenuContent.displayName = NavigationMenuPrimitive.Content.displayName
const NavigationMenuLink = NavigationMenuPrimitive.Link
const NavigationMenuViewport = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.Viewport>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.Viewport>
>(({ className, ...props }, ref) => (
  <div className={cn("absolute left-0 top-full flex justify-center")}>
    <NavigationMenuPrimitive.Viewport
      className={cn(
        "origin-top-center relative mt-1.5 h-[var(--radix-navigation-menu-viewport-height)] w-full overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-lg data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-90 md:w-[var(--radix-navigation-menu-viewport-width)]",
        className
      )}
      ref={ref}
      {...props}
    />
  </div>
))
NavigationMenuViewport.displayName =
  NavigationMenuPrimitive.Viewport.displayName
const NavigationMenuIndicator = React.forwardRef<
  React.ElementRef<typeof NavigationMenuPrimitive.Indicator>,
  React.ComponentPropsWithoutRef<typeof NavigationMenuPrimitive.Indicator>
>(({ className, ...props }, ref) => (
  <NavigationMenuPrimitive.Indicator
    ref={ref}
    className={cn(
      "top-full z-[1] flex h-1.5 items-end justify-center overflow-hidden data-[state=visible]:animate-in data-[state=hidden]:animate-out data-[state=hidden]:fade-out data-[state=visible]:fade-in",
      className
    )}
    {...props}
  >
    <div className="relative top-[60%] h-2 w-2 rotate-45 rounded-tl-sm bg-border shadow-md" />
  </NavigationMenuPrimitive.Indicator>
))
NavigationMenuIndicator.displayName =
  NavigationMenuPrimitive.Indicator.displayName
export {
  navigationMenuTriggerStyle,
  NavigationMenu,
  NavigationMenuList,
  NavigationMenuItem,
  NavigationMenuContent,
  NavigationMenuTrigger,
  NavigationMenuLink,
  NavigationMenuIndicator,
  NavigationMenuViewport,
}
</file>

<file path="src/components/ui/pagination.tsx">
import * as React from "react"
import { ChevronLeft, ChevronRight, MoreHorizontal } from "lucide-react"
import { cn } from "@/lib/utils"
import { ButtonProps, buttonVariants } from "@/components/ui/button"
const Pagination = ({ className, ...props }: React.ComponentProps<"nav">) => (
  <nav
    role="navigation"
    aria-label="pagination"
    className={cn("mx-auto flex w-full justify-center", className)}
    {...props}
  />
)
Pagination.displayName = "Pagination"
const PaginationContent = React.forwardRef<
  HTMLUListElement,
  React.ComponentProps<"ul">
>(({ className, ...props }, ref) => (
  <ul
    ref={ref}
    className={cn("flex flex-row items-center gap-1", className)}
    {...props}
  />
))
PaginationContent.displayName = "PaginationContent"
const PaginationItem = React.forwardRef<
  HTMLLIElement,
  React.ComponentProps<"li">
>(({ className, ...props }, ref) => (
  <li ref={ref} className={cn("", className)} {...props} />
))
PaginationItem.displayName = "PaginationItem"
type PaginationLinkProps = {
  isActive?: boolean
} & Pick<ButtonProps, "size"> &
  React.ComponentProps<"a">
const PaginationLink = ({
  className,
  isActive,
  size = "icon",
  ...props
}: PaginationLinkProps) => (
  <a
    aria-current={isActive ? "page" : undefined}
    className={cn(
      buttonVariants({
        variant: isActive ? "outline" : "ghost",
        size,
      }),
      className
    )}
    {...props}
  />
)
PaginationLink.displayName = "PaginationLink"
const PaginationPrevious = ({
  className,
  ...props
}: React.ComponentProps<typeof PaginationLink>) => (
  <PaginationLink
    aria-label="Go to previous page"
    size="default"
    className={cn("gap-1 pl-2.5", className)}
    {...props}
  >
    <ChevronLeft className="h-4 w-4" />
    <span>Previous</span>
  </PaginationLink>
)
PaginationPrevious.displayName = "PaginationPrevious"
const PaginationNext = ({
  className,
  ...props
}: React.ComponentProps<typeof PaginationLink>) => (
  <PaginationLink
    aria-label="Go to next page"
    size="default"
    className={cn("gap-1 pr-2.5", className)}
    {...props}
  >
    <span>Next</span>
    <ChevronRight className="h-4 w-4" />
  </PaginationLink>
)
PaginationNext.displayName = "PaginationNext"
const PaginationEllipsis = ({
  className,
  ...props
}: React.ComponentProps<"span">) => (
  <span
    aria-hidden
    className={cn("flex h-9 w-9 items-center justify-center", className)}
    {...props}
  >
    <MoreHorizontal className="h-4 w-4" />
    <span className="sr-only">More pages</span>
  </span>
)
PaginationEllipsis.displayName = "PaginationEllipsis"
export {
  Pagination,
  PaginationContent,
  PaginationEllipsis,
  PaginationItem,
  PaginationLink,
  PaginationNext,
  PaginationPrevious,
}
</file>

<file path="src/components/ui/popover.tsx">
import * as React from "react"
import * as PopoverPrimitive from "@radix-ui/react-popover"
import { cn } from "@/lib/utils"
const Popover = PopoverPrimitive.Root
const PopoverTrigger = PopoverPrimitive.Trigger
const PopoverContent = React.forwardRef<
  React.ElementRef<typeof PopoverPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof PopoverPrimitive.Content>
>(({ className, align = "center", sideOffset = 4, ...props }, ref) => (
  <PopoverPrimitive.Portal>
    <PopoverPrimitive.Content
      ref={ref}
      align={align}
      sideOffset={sideOffset}
      className={cn(
        "z-50 w-72 rounded-md border bg-popover p-4 text-popover-foreground shadow-md outline-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        className
      )}
      {...props}
    />
  </PopoverPrimitive.Portal>
))
PopoverContent.displayName = PopoverPrimitive.Content.displayName
export { Popover, PopoverTrigger, PopoverContent }
</file>

<file path="src/components/ui/progress.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Progress                                                                            ║
 * ║ Description: Progress bar component with smooth animations                                     ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as ProgressPrimitive from "@radix-ui/react-progress"
import { cn } from "@/lib/utils"
const Progress = React.forwardRef<
  React.ElementRef<typeof ProgressPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof ProgressPrimitive.Root>
>(({ className, value, ...props }, ref) => (
  <ProgressPrimitive.Root
    ref={ref}
    className={cn(
      "relative h-4 w-full overflow-hidden rounded-full bg-secondary",
      className
    )}
    {...props}
  >
    <ProgressPrimitive.Indicator
      className="h-full w-full flex-1 bg-primary transition-all"
      style={{ transform: `translateX(-${100 - (value || 0)}%)` }}
    />
  </ProgressPrimitive.Root>
))
Progress.displayName = ProgressPrimitive.Root.displayName
export { Progress }
</file>

<file path="src/components/ui/radio-group.tsx">
import * as React from "react"
import * as RadioGroupPrimitive from "@radix-ui/react-radio-group"
import { Circle } from "lucide-react"
import { cn } from "@/lib/utils"
const RadioGroup = React.forwardRef<
  React.ElementRef<typeof RadioGroupPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof RadioGroupPrimitive.Root>
>(({ className, ...props }, ref) => {
  return (
    <RadioGroupPrimitive.Root
      className={cn("grid gap-2", className)}
      {...props}
      ref={ref}
    />
  )
})
RadioGroup.displayName = RadioGroupPrimitive.Root.displayName
const RadioGroupItem = React.forwardRef<
  React.ElementRef<typeof RadioGroupPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof RadioGroupPrimitive.Item>
>(({ className, ...props }, ref) => {
  return (
    <RadioGroupPrimitive.Item
      ref={ref}
      className={cn(
        "aspect-square h-4 w-4 rounded-full border border-primary text-primary ring-offset-background focus:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50",
        className
      )}
      {...props}
    >
      <RadioGroupPrimitive.Indicator className="flex items-center justify-center">
        <Circle className="h-2.5 w-2.5 fill-current text-current" />
      </RadioGroupPrimitive.Indicator>
    </RadioGroupPrimitive.Item>
  )
})
RadioGroupItem.displayName = RadioGroupPrimitive.Item.displayName
export { RadioGroup, RadioGroupItem }
</file>

<file path="src/components/ui/resizable.tsx">
import { GripVertical } from "lucide-react"
import * as ResizablePrimitive from "react-resizable-panels"
import { cn } from "@/lib/utils"
const ResizablePanelGroup = ({
  className,
  ...props
}: React.ComponentProps<typeof ResizablePrimitive.PanelGroup>) => (
  <ResizablePrimitive.PanelGroup
    className={cn(
      "flex h-full w-full data-[panel-group-direction=vertical]:flex-col",
      className
    )}
    {...props}
  />
)
const ResizablePanel = ResizablePrimitive.Panel
const ResizableHandle = ({
  withHandle,
  className,
  ...props
}: React.ComponentProps<typeof ResizablePrimitive.PanelResizeHandle> & {
  withHandle?: boolean
}) => (
  <ResizablePrimitive.PanelResizeHandle
    className={cn(
      "relative flex w-px items-center justify-center bg-border after:absolute after:inset-y-0 after:left-1/2 after:w-1 after:-translate-x-1/2 focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring focus-visible:ring-offset-1 data-[panel-group-direction=vertical]:h-px data-[panel-group-direction=vertical]:w-full data-[panel-group-direction=vertical]:after:left-0 data-[panel-group-direction=vertical]:after:h-1 data-[panel-group-direction=vertical]:after:w-full data-[panel-group-direction=vertical]:after:-translate-y-1/2 data-[panel-group-direction=vertical]:after:translate-x-0 [&[data-panel-group-direction=vertical]>div]:rotate-90",
      className
    )}
    {...props}
  >
    {withHandle && (
      <div className="z-10 flex h-4 w-3 items-center justify-center rounded-sm border bg-border">
        <GripVertical className="h-2.5 w-2.5" />
      </div>
    )}
  </ResizablePrimitive.PanelResizeHandle>
)
export { ResizablePanelGroup, ResizablePanel, ResizableHandle }
</file>

<file path="src/components/ui/scroll-area.tsx">
import * as React from "react"
import * as ScrollAreaPrimitive from "@radix-ui/react-scroll-area"
import { cn } from "@/lib/utils"
const ScrollArea = React.forwardRef<
  React.ElementRef<typeof ScrollAreaPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof ScrollAreaPrimitive.Root>
>(({ className, children, ...props }, ref) => (
  <ScrollAreaPrimitive.Root
    ref={ref}
    className={cn("relative overflow-hidden", className)}
    {...props}
  >
    <ScrollAreaPrimitive.Viewport className="h-full w-full rounded-[inherit]">
      {children}
    </ScrollAreaPrimitive.Viewport>
    <ScrollBar />
    <ScrollAreaPrimitive.Corner />
  </ScrollAreaPrimitive.Root>
))
ScrollArea.displayName = ScrollAreaPrimitive.Root.displayName
const ScrollBar = React.forwardRef<
  React.ElementRef<typeof ScrollAreaPrimitive.ScrollAreaScrollbar>,
  React.ComponentPropsWithoutRef<typeof ScrollAreaPrimitive.ScrollAreaScrollbar>
>(({ className, orientation = "vertical", ...props }, ref) => (
  <ScrollAreaPrimitive.ScrollAreaScrollbar
    ref={ref}
    orientation={orientation}
    className={cn(
      "flex touch-none select-none transition-colors",
      orientation === "vertical" &&
        "h-full w-2.5 border-l border-l-transparent p-[1px]",
      orientation === "horizontal" &&
        "h-2.5 flex-col border-t border-t-transparent p-[1px]",
      className
    )}
    {...props}
  >
    <ScrollAreaPrimitive.ScrollAreaThumb className="relative flex-1 rounded-full bg-border" />
  </ScrollAreaPrimitive.ScrollAreaScrollbar>
))
ScrollBar.displayName = ScrollAreaPrimitive.ScrollAreaScrollbar.displayName
export { ScrollArea, ScrollBar }
</file>

<file path="src/components/ui/select.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Select                                                                              ║
 * ║ Description: Select component with smooth animations and keyboard navigation                   ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as SelectPrimitive from "@radix-ui/react-select"
import { Check, ChevronDown, ChevronUp } from "lucide-react"
import { cn } from "@/lib/utils"
const Select = SelectPrimitive.Root
const SelectGroup = SelectPrimitive.Group
const SelectValue = SelectPrimitive.Value
const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex h-10 w-full items-center justify-between rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      className
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <ChevronDown className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
))
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName
const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronUp className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
))
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName
const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <ChevronDown className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
))
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName
const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]"
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
))
SelectContent.displayName = SelectPrimitive.Content.displayName
const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("py-1.5 pl-8 pr-2 text-sm font-semibold", className)}
    {...props}
  />
))
SelectLabel.displayName = SelectPrimitive.Label.displayName
const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <Check className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>
    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
))
SelectItem.displayName = SelectPrimitive.Item.displayName
const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
))
SelectSeparator.displayName = SelectPrimitive.Separator.displayName
export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
}
</file>

<file path="src/components/ui/separator.tsx">
import * as React from "react"
import * as SeparatorPrimitive from "@radix-ui/react-separator"
import { cn } from "@/lib/utils"
const Separator = React.forwardRef<
  React.ElementRef<typeof SeparatorPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof SeparatorPrimitive.Root>
>(
  (
    { className, orientation = "horizontal", decorative = true, ...props },
    ref
  ) => (
    <SeparatorPrimitive.Root
      ref={ref}
      decorative={decorative}
      orientation={orientation}
      className={cn(
        "shrink-0 bg-border",
        orientation === "horizontal" ? "h-[1px] w-full" : "h-full w-[1px]",
        className
      )}
      {...props}
    />
  )
)
Separator.displayName = SeparatorPrimitive.Root.displayName
export { Separator }
</file>

<file path="src/components/ui/sheet.tsx">
import * as SheetPrimitive from "@radix-ui/react-dialog"
import { cva, type VariantProps } from "class-variance-authority"
import { X } from "lucide-react"
import * as React from "react"
import { cn } from "@/lib/utils"
const Sheet = SheetPrimitive.Root
const SheetTrigger = SheetPrimitive.Trigger
const SheetClose = SheetPrimitive.Close
const SheetPortal = SheetPrimitive.Portal
const SheetOverlay = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Overlay>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Overlay>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Overlay
    className={cn(
      "fixed inset-0 z-50 bg-black/80  data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0",
      className
    )}
    {...props}
    ref={ref}
  />
))
SheetOverlay.displayName = SheetPrimitive.Overlay.displayName
const sheetVariants = cva(
  "fixed z-50 gap-4 bg-background p-6 shadow-lg transition ease-in-out data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:duration-300 data-[state=open]:duration-500",
  {
    variants: {
      side: {
        top: "inset-x-0 top-0 border-b data-[state=closed]:slide-out-to-top data-[state=open]:slide-in-from-top",
        bottom:
          "inset-x-0 bottom-0 border-t data-[state=closed]:slide-out-to-bottom data-[state=open]:slide-in-from-bottom",
        left: "inset-y-0 left-0 h-full w-3/4 border-r data-[state=closed]:slide-out-to-left data-[state=open]:slide-in-from-left sm:max-w-sm",
        right:
          "inset-y-0 right-0 h-full w-3/4  border-l data-[state=closed]:slide-out-to-right data-[state=open]:slide-in-from-right sm:max-w-sm",
      },
    },
    defaultVariants: {
      side: "right",
    },
  }
)
interface SheetContentProps
  extends React.ComponentPropsWithoutRef<typeof SheetPrimitive.Content>,
  VariantProps<typeof sheetVariants> { }
const SheetContent = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Content>,
  SheetContentProps
>(({ side = "right", className, children, ...props }, ref) => (
  <SheetPortal>
    <SheetOverlay />
    <SheetPrimitive.Content
      ref={ref}
      className={cn(sheetVariants({ side }), className)}
      {...props}
    >
      {children}
      <SheetPrimitive.Close className="absolute right-4 top-4 rounded-sm opacity-70 ring-offset-background transition-opacity hover:opacity-100 focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none data-[state=open]:bg-secondary">
        <X className="h-4 w-4" />
        <span className="sr-only">Close</span>
      </SheetPrimitive.Close>
    </SheetPrimitive.Content>
  </SheetPortal>
))
SheetContent.displayName = SheetPrimitive.Content.displayName
const SheetHeader = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col space-y-2 text-center sm:text-left",
      className
    )}
    {...props}
  />
)
SheetHeader.displayName = "SheetHeader"
const SheetFooter = ({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) => (
  <div
    className={cn(
      "flex flex-col-reverse sm:flex-row sm:justify-end sm:space-x-2",
      className
    )}
    {...props}
  />
)
SheetFooter.displayName = "SheetFooter"
const SheetTitle = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Title>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Title>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Title
    ref={ref}
    className={cn("text-lg font-semibold text-foreground", className)}
    {...props}
  />
))
SheetTitle.displayName = SheetPrimitive.Title.displayName
const SheetDescription = React.forwardRef<
  React.ElementRef<typeof SheetPrimitive.Description>,
  React.ComponentPropsWithoutRef<typeof SheetPrimitive.Description>
>(({ className, ...props }, ref) => (
  <SheetPrimitive.Description
    ref={ref}
    className={cn("text-sm text-muted-foreground", className)}
    {...props}
  />
))
SheetDescription.displayName = SheetPrimitive.Description.displayName
export {
  Sheet, SheetClose,
  SheetContent, SheetDescription, SheetFooter, SheetHeader, SheetOverlay, SheetPortal, SheetTitle, SheetTrigger
}
</file>

<file path="src/components/ui/sidebar.tsx">
import * as React from "react"
import { Slot } from "@radix-ui/react-slot"
import { VariantProps, cva } from "class-variance-authority"
import { PanelLeft } from "lucide-react"
import { useIsMobile } from "@/hooks/use-mobile"
import { cn } from "@/lib/utils"
import { Button } from "@/components/ui/button"
import { Input } from "@/components/ui/input"
import { Separator } from "@/components/ui/separator"
import { Sheet, SheetContent } from "@/components/ui/sheet"
import { Skeleton } from "@/components/ui/skeleton"
import {
  Tooltip,
  TooltipContent,
  TooltipProvider,
  TooltipTrigger,
} from "@/components/ui/tooltip"
const SIDEBAR_COOKIE_NAME = "sidebar:state"
const SIDEBAR_COOKIE_MAX_AGE = 60 * 60 * 24 * 7
const SIDEBAR_WIDTH = "16rem"
const SIDEBAR_WIDTH_MOBILE = "18rem"
const SIDEBAR_WIDTH_ICON = "3rem"
const SIDEBAR_KEYBOARD_SHORTCUT = "b"
type SidebarContext = {
  state: "expanded" | "collapsed"
  open: boolean
  setOpen: (open: boolean) => void
  openMobile: boolean
  setOpenMobile: (open: boolean) => void
  isMobile: boolean
  toggleSidebar: () => void
}
const SidebarContext = React.createContext<SidebarContext | null>(null)
function useSidebar() {
  const context = React.useContext(SidebarContext)
  if (!context) {
    throw new Error("useSidebar must be used within a SidebarProvider.")
  }
  return context
}
const SidebarProvider = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> & {
    defaultOpen?: boolean
    open?: boolean
    onOpenChange?: (open: boolean) => void
  }
>(
  (
    {
      defaultOpen = true,
      open: openProp,
      onOpenChange: setOpenProp,
      className,
      style,
      children,
      ...props
    },
    ref
  ) => {
    const isMobile = useIsMobile()
    const [openMobile, setOpenMobile] = React.useState(false)
    // This is the internal state of the sidebar.
    // We use openProp and setOpenProp for control from outside the component.
    const [_open, _setOpen] = React.useState(defaultOpen)
    const open = openProp ?? _open
    const setOpen = React.useCallback(
      (value: boolean | ((value: boolean) => boolean)) => {
        const openState = typeof value === "function" ? value(open) : value
        if (setOpenProp) {
          setOpenProp(openState)
        } else {
          _setOpen(openState)
        }
        // This sets the cookie to keep the sidebar state.
        document.cookie = `${SIDEBAR_COOKIE_NAME}=${openState}; path=/; max-age=${SIDEBAR_COOKIE_MAX_AGE}`
      },
      [setOpenProp, open]
    )
    // Helper to toggle the sidebar.
    const toggleSidebar = React.useCallback(() => {
      return isMobile
        ? setOpenMobile((open) => !open)
        : setOpen((open) => !open)
    }, [isMobile, setOpen, setOpenMobile])
    // Adds a keyboard shortcut to toggle the sidebar.
    React.useEffect(() => {
      const handleKeyDown = (event: KeyboardEvent) => {
        if (
          event.key === SIDEBAR_KEYBOARD_SHORTCUT &&
          (event.metaKey || event.ctrlKey)
        ) {
          event.preventDefault()
          toggleSidebar()
        }
      }
      window.addEventListener("keydown", handleKeyDown)
      return () => window.removeEventListener("keydown", handleKeyDown)
    }, [toggleSidebar])
    // We add a state so that we can do data-state="expanded" or "collapsed".
    // This makes it easier to style the sidebar with Tailwind classes.
    const state = open ? "expanded" : "collapsed"
    const contextValue = React.useMemo<SidebarContext>(
      () => ({
        state,
        open,
        setOpen,
        isMobile,
        openMobile,
        setOpenMobile,
        toggleSidebar,
      }),
      [state, open, setOpen, isMobile, openMobile, setOpenMobile, toggleSidebar]
    )
    return (
      <SidebarContext.Provider value={contextValue}>
        <TooltipProvider delayDuration={0}>
          <div
            style={
              {
                "--sidebar-width": SIDEBAR_WIDTH,
                "--sidebar-width-icon": SIDEBAR_WIDTH_ICON,
                ...style,
              } as React.CSSProperties
            }
            className={cn(
              "group/sidebar-wrapper flex min-h-svh w-full has-[[data-variant=inset]]:bg-sidebar",
              className
            )}
            ref={ref}
            {...props}
          >
            {children}
          </div>
        </TooltipProvider>
      </SidebarContext.Provider>
    )
  }
)
SidebarProvider.displayName = "SidebarProvider"
const Sidebar = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> & {
    side?: "left" | "right"
    variant?: "sidebar" | "floating" | "inset"
    collapsible?: "offcanvas" | "icon" | "none"
  }
>(
  (
    {
      side = "left",
      variant = "sidebar",
      collapsible = "offcanvas",
      className,
      children,
      ...props
    },
    ref
  ) => {
    const { isMobile, state, openMobile, setOpenMobile } = useSidebar()
    if (collapsible === "none") {
      return (
        <div
          className={cn(
            "flex h-full w-[--sidebar-width] flex-col bg-sidebar text-sidebar-foreground",
            className
          )}
          ref={ref}
          {...props}
        >
          {children}
        </div>
      )
    }
    if (isMobile) {
      return (
        <Sheet open={openMobile} onOpenChange={setOpenMobile} {...props}>
          <SheetContent
            data-sidebar="sidebar"
            data-mobile="true"
            className="w-[--sidebar-width] bg-sidebar p-0 text-sidebar-foreground [&>button]:hidden"
            style={
              {
                "--sidebar-width": SIDEBAR_WIDTH_MOBILE,
              } as React.CSSProperties
            }
            side={side}
          >
            <div className="flex h-full w-full flex-col">{children}</div>
          </SheetContent>
        </Sheet>
      )
    }
    return (
      <div
        ref={ref}
        className="group peer hidden md:block text-sidebar-foreground"
        data-state={state}
        data-collapsible={state === "collapsed" ? collapsible : ""}
        data-variant={variant}
        data-side={side}
      >
        {/* This is what handles the sidebar gap on desktop */}
        <div
          className={cn(
            "duration-200 relative h-svh w-[--sidebar-width] bg-transparent transition-[width] ease-linear",
            "group-data-[collapsible=offcanvas]:w-0",
            "group-data-[side=right]:rotate-180",
            variant === "floating" || variant === "inset"
              ? "group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)_+_theme(spacing.4))]"
              : "group-data-[collapsible=icon]:w-[--sidebar-width-icon]"
          )}
        />
        <div
          className={cn(
            "duration-200 fixed inset-y-0 z-10 hidden h-svh w-[--sidebar-width] transition-[left,right,width] ease-linear md:flex",
            side === "left"
              ? "left-0 group-data-[collapsible=offcanvas]:left-[calc(var(--sidebar-width)*-1)]"
              : "right-0 group-data-[collapsible=offcanvas]:right-[calc(var(--sidebar-width)*-1)]",
            // Adjust the padding for floating and inset variants.
            variant === "floating" || variant === "inset"
              ? "p-2 group-data-[collapsible=icon]:w-[calc(var(--sidebar-width-icon)_+_theme(spacing.4)_+2px)]"
              : "group-data-[collapsible=icon]:w-[--sidebar-width-icon] group-data-[side=left]:border-r group-data-[side=right]:border-l",
            className
          )}
          {...props}
        >
          <div
            data-sidebar="sidebar"
            className="flex h-full w-full flex-col bg-sidebar group-data-[variant=floating]:rounded-lg group-data-[variant=floating]:border group-data-[variant=floating]:border-sidebar-border group-data-[variant=floating]:shadow"
          >
            {children}
          </div>
        </div>
      </div>
    )
  }
)
Sidebar.displayName = "Sidebar"
const SidebarTrigger = React.forwardRef<
  React.ElementRef<typeof Button>,
  React.ComponentProps<typeof Button>
>(({ className, onClick, ...props }, ref) => {
  const { toggleSidebar } = useSidebar()
  return (
    <Button
      ref={ref}
      data-sidebar="trigger"
      variant="ghost"
      size="icon"
      className={cn("h-7 w-7", className)}
      onClick={(event) => {
        onClick?.(event)
        toggleSidebar()
      }}
      {...props}
    >
      <PanelLeft />
      <span className="sr-only">Toggle Sidebar</span>
    </Button>
  )
})
SidebarTrigger.displayName = "SidebarTrigger"
const SidebarRail = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<"button">
>(({ className, ...props }, ref) => {
  const { toggleSidebar } = useSidebar()
  return (
    <button
      ref={ref}
      data-sidebar="rail"
      aria-label="Toggle Sidebar"
      tabIndex={-1}
      onClick={toggleSidebar}
      title="Toggle Sidebar"
      className={cn(
        "absolute inset-y-0 z-20 hidden w-4 -translate-x-1/2 transition-all ease-linear after:absolute after:inset-y-0 after:left-1/2 after:w-[2px] hover:after:bg-sidebar-border group-data-[side=left]:-right-4 group-data-[side=right]:left-0 sm:flex",
        "[[data-side=left]_&]:cursor-w-resize [[data-side=right]_&]:cursor-e-resize",
        "[[data-side=left][data-state=collapsed]_&]:cursor-e-resize [[data-side=right][data-state=collapsed]_&]:cursor-w-resize",
        "group-data-[collapsible=offcanvas]:translate-x-0 group-data-[collapsible=offcanvas]:after:left-full group-data-[collapsible=offcanvas]:hover:bg-sidebar",
        "[[data-side=left][data-collapsible=offcanvas]_&]:-right-2",
        "[[data-side=right][data-collapsible=offcanvas]_&]:-left-2",
        className
      )}
      {...props}
    />
  )
})
SidebarRail.displayName = "SidebarRail"
const SidebarInset = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"main">
>(({ className, ...props }, ref) => {
  return (
    <main
      ref={ref}
      className={cn(
        "relative flex min-h-svh flex-1 flex-col bg-background",
        "peer-data-[variant=inset]:min-h-[calc(100svh-theme(spacing.4))] md:peer-data-[variant=inset]:m-2 md:peer-data-[state=collapsed]:peer-data-[variant=inset]:ml-2 md:peer-data-[variant=inset]:ml-0 md:peer-data-[variant=inset]:rounded-xl md:peer-data-[variant=inset]:shadow",
        className
      )}
      {...props}
    />
  )
})
SidebarInset.displayName = "SidebarInset"
const SidebarInput = React.forwardRef<
  React.ElementRef<typeof Input>,
  React.ComponentProps<typeof Input>
>(({ className, ...props }, ref) => {
  return (
    <Input
      ref={ref}
      data-sidebar="input"
      className={cn(
        "h-8 w-full bg-background shadow-none focus-visible:ring-2 focus-visible:ring-sidebar-ring",
        className
      )}
      {...props}
    />
  )
})
SidebarInput.displayName = "SidebarInput"
const SidebarHeader = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => {
  return (
    <div
      ref={ref}
      data-sidebar="header"
      className={cn("flex flex-col gap-2 p-2", className)}
      {...props}
    />
  )
})
SidebarHeader.displayName = "SidebarHeader"
const SidebarFooter = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => {
  return (
    <div
      ref={ref}
      data-sidebar="footer"
      className={cn("flex flex-col gap-2 p-2", className)}
      {...props}
    />
  )
})
SidebarFooter.displayName = "SidebarFooter"
const SidebarSeparator = React.forwardRef<
  React.ElementRef<typeof Separator>,
  React.ComponentProps<typeof Separator>
>(({ className, ...props }, ref) => {
  return (
    <Separator
      ref={ref}
      data-sidebar="separator"
      className={cn("mx-2 w-auto bg-sidebar-border", className)}
      {...props}
    />
  )
})
SidebarSeparator.displayName = "SidebarSeparator"
const SidebarContent = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => {
  return (
    <div
      ref={ref}
      data-sidebar="content"
      className={cn(
        "flex min-h-0 flex-1 flex-col gap-2 overflow-auto group-data-[collapsible=icon]:overflow-hidden",
        className
      )}
      {...props}
    />
  )
})
SidebarContent.displayName = "SidebarContent"
const SidebarGroup = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => {
  return (
    <div
      ref={ref}
      data-sidebar="group"
      className={cn("relative flex w-full min-w-0 flex-col p-2", className)}
      {...props}
    />
  )
})
SidebarGroup.displayName = "SidebarGroup"
const SidebarGroupLabel = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> & { asChild?: boolean }
>(({ className, asChild = false, ...props }, ref) => {
  const Comp = asChild ? Slot : "div"
  return (
    <Comp
      ref={ref}
      data-sidebar="group-label"
      className={cn(
        "duration-200 flex h-8 shrink-0 items-center rounded-md px-2 text-xs font-medium text-sidebar-foreground/70 outline-none ring-sidebar-ring transition-[margin,opa] ease-linear focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0",
        "group-data-[collapsible=icon]:-mt-8 group-data-[collapsible=icon]:opacity-0",
        className
      )}
      {...props}
    />
  )
})
SidebarGroupLabel.displayName = "SidebarGroupLabel"
const SidebarGroupAction = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<"button"> & { asChild?: boolean }
>(({ className, asChild = false, ...props }, ref) => {
  const Comp = asChild ? Slot : "button"
  return (
    <Comp
      ref={ref}
      data-sidebar="group-action"
      className={cn(
        "absolute right-3 top-3.5 flex aspect-square w-5 items-center justify-center rounded-md p-0 text-sidebar-foreground outline-none ring-sidebar-ring transition-transform hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 [&>svg]:size-4 [&>svg]:shrink-0",
        // Increases the hit area of the button on mobile.
        "after:absolute after:-inset-2 after:md:hidden",
        "group-data-[collapsible=icon]:hidden",
        className
      )}
      {...props}
    />
  )
})
SidebarGroupAction.displayName = "SidebarGroupAction"
const SidebarGroupContent = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    data-sidebar="group-content"
    className={cn("w-full text-sm", className)}
    {...props}
  />
))
SidebarGroupContent.displayName = "SidebarGroupContent"
const SidebarMenu = React.forwardRef<
  HTMLUListElement,
  React.ComponentProps<"ul">
>(({ className, ...props }, ref) => (
  <ul
    ref={ref}
    data-sidebar="menu"
    className={cn("flex w-full min-w-0 flex-col gap-1", className)}
    {...props}
  />
))
SidebarMenu.displayName = "SidebarMenu"
const SidebarMenuItem = React.forwardRef<
  HTMLLIElement,
  React.ComponentProps<"li">
>(({ className, ...props }, ref) => (
  <li
    ref={ref}
    data-sidebar="menu-item"
    className={cn("group/menu-item relative", className)}
    {...props}
  />
))
SidebarMenuItem.displayName = "SidebarMenuItem"
const sidebarMenuButtonVariants = cva(
  "peer/menu-button flex w-full items-center gap-2 overflow-hidden rounded-md p-2 text-left text-sm outline-none ring-sidebar-ring transition-[width,height,padding] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 group-has-[[data-sidebar=menu-action]]/menu-item:pr-8 aria-disabled:pointer-events-none aria-disabled:opacity-50 data-[active=true]:bg-sidebar-accent data-[active=true]:font-medium data-[active=true]:text-sidebar-accent-foreground data-[state=open]:hover:bg-sidebar-accent data-[state=open]:hover:text-sidebar-accent-foreground group-data-[collapsible=icon]:!size-8 group-data-[collapsible=icon]:!p-2 [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0",
  {
    variants: {
      variant: {
        default: "hover:bg-sidebar-accent hover:text-sidebar-accent-foreground",
        outline:
          "bg-background shadow-[0_0_0_1px_hsl(var(--sidebar-border))] hover:bg-sidebar-accent hover:text-sidebar-accent-foreground hover:shadow-[0_0_0_1px_hsl(var(--sidebar-accent))]",
      },
      size: {
        default: "h-8 text-sm",
        sm: "h-7 text-xs",
        lg: "h-12 text-sm group-data-[collapsible=icon]:!p-0",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)
const SidebarMenuButton = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<"button"> & {
    asChild?: boolean
    isActive?: boolean
    tooltip?: string | React.ComponentProps<typeof TooltipContent>
  } & VariantProps<typeof sidebarMenuButtonVariants>
>(
  (
    {
      asChild = false,
      isActive = false,
      variant = "default",
      size = "default",
      tooltip,
      className,
      ...props
    },
    ref
  ) => {
    const Comp = asChild ? Slot : "button"
    const { isMobile, state } = useSidebar()
    const button = (
      <Comp
        ref={ref}
        data-sidebar="menu-button"
        data-size={size}
        data-active={isActive}
        className={cn(sidebarMenuButtonVariants({ variant, size }), className)}
        {...props}
      />
    )
    if (!tooltip) {
      return button
    }
    if (typeof tooltip === "string") {
      tooltip = {
        children: tooltip,
      }
    }
    return (
      <Tooltip>
        <TooltipTrigger asChild>{button}</TooltipTrigger>
        <TooltipContent
          side="right"
          align="center"
          hidden={state !== "collapsed" || isMobile}
          {...tooltip}
        />
      </Tooltip>
    )
  }
)
SidebarMenuButton.displayName = "SidebarMenuButton"
const SidebarMenuAction = React.forwardRef<
  HTMLButtonElement,
  React.ComponentProps<"button"> & {
    asChild?: boolean
    showOnHover?: boolean
  }
>(({ className, asChild = false, showOnHover = false, ...props }, ref) => {
  const Comp = asChild ? Slot : "button"
  return (
    <Comp
      ref={ref}
      data-sidebar="menu-action"
      className={cn(
        "absolute right-1 top-1.5 flex aspect-square w-5 items-center justify-center rounded-md p-0 text-sidebar-foreground outline-none ring-sidebar-ring transition-transform hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 peer-hover/menu-button:text-sidebar-accent-foreground [&>svg]:size-4 [&>svg]:shrink-0",
        // Increases the hit area of the button on mobile.
        "after:absolute after:-inset-2 after:md:hidden",
        "peer-data-[size=sm]/menu-button:top-1",
        "peer-data-[size=default]/menu-button:top-1.5",
        "peer-data-[size=lg]/menu-button:top-2.5",
        "group-data-[collapsible=icon]:hidden",
        showOnHover &&
          "group-focus-within/menu-item:opacity-100 group-hover/menu-item:opacity-100 data-[state=open]:opacity-100 peer-data-[active=true]/menu-button:text-sidebar-accent-foreground md:opacity-0",
        className
      )}
      {...props}
    />
  )
})
SidebarMenuAction.displayName = "SidebarMenuAction"
const SidebarMenuBadge = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div">
>(({ className, ...props }, ref) => (
  <div
    ref={ref}
    data-sidebar="menu-badge"
    className={cn(
      "absolute right-1 flex h-5 min-w-5 items-center justify-center rounded-md px-1 text-xs font-medium tabular-nums text-sidebar-foreground select-none pointer-events-none",
      "peer-hover/menu-button:text-sidebar-accent-foreground peer-data-[active=true]/menu-button:text-sidebar-accent-foreground",
      "peer-data-[size=sm]/menu-button:top-1",
      "peer-data-[size=default]/menu-button:top-1.5",
      "peer-data-[size=lg]/menu-button:top-2.5",
      "group-data-[collapsible=icon]:hidden",
      className
    )}
    {...props}
  />
))
SidebarMenuBadge.displayName = "SidebarMenuBadge"
const SidebarMenuSkeleton = React.forwardRef<
  HTMLDivElement,
  React.ComponentProps<"div"> & {
    showIcon?: boolean
  }
>(({ className, showIcon = false, ...props }, ref) => {
  // Random width between 50 to 90%.
  const width = React.useMemo(() => {
    return `${Math.floor(Math.random() * 40) + 50}%`
  }, [])
  return (
    <div
      ref={ref}
      data-sidebar="menu-skeleton"
      className={cn("rounded-md h-8 flex gap-2 px-2 items-center", className)}
      {...props}
    >
      {showIcon && (
        <Skeleton
          className="size-4 rounded-md"
          data-sidebar="menu-skeleton-icon"
        />
      )}
      <Skeleton
        className="h-4 flex-1 max-w-[--skeleton-width]"
        data-sidebar="menu-skeleton-text"
        style={
          {
            "--skeleton-width": width,
          } as React.CSSProperties
        }
      />
    </div>
  )
})
SidebarMenuSkeleton.displayName = "SidebarMenuSkeleton"
const SidebarMenuSub = React.forwardRef<
  HTMLUListElement,
  React.ComponentProps<"ul">
>(({ className, ...props }, ref) => (
  <ul
    ref={ref}
    data-sidebar="menu-sub"
    className={cn(
      "mx-3.5 flex min-w-0 translate-x-px flex-col gap-1 border-l border-sidebar-border px-2.5 py-0.5",
      "group-data-[collapsible=icon]:hidden",
      className
    )}
    {...props}
  />
))
SidebarMenuSub.displayName = "SidebarMenuSub"
const SidebarMenuSubItem = React.forwardRef<
  HTMLLIElement,
  React.ComponentProps<"li">
>(({ ...props }, ref) => <li ref={ref} {...props} />)
SidebarMenuSubItem.displayName = "SidebarMenuSubItem"
const SidebarMenuSubButton = React.forwardRef<
  HTMLAnchorElement,
  React.ComponentProps<"a"> & {
    asChild?: boolean
    size?: "sm" | "md"
    isActive?: boolean
  }
>(({ asChild = false, size = "md", isActive, className, ...props }, ref) => {
  const Comp = asChild ? Slot : "a"
  return (
    <Comp
      ref={ref}
      data-sidebar="menu-sub-button"
      data-size={size}
      data-active={isActive}
      className={cn(
        "flex h-7 min-w-0 -translate-x-px items-center gap-2 overflow-hidden rounded-md px-2 text-sidebar-foreground outline-none ring-sidebar-ring hover:bg-sidebar-accent hover:text-sidebar-accent-foreground focus-visible:ring-2 active:bg-sidebar-accent active:text-sidebar-accent-foreground disabled:pointer-events-none disabled:opacity-50 aria-disabled:pointer-events-none aria-disabled:opacity-50 [&>span:last-child]:truncate [&>svg]:size-4 [&>svg]:shrink-0 [&>svg]:text-sidebar-accent-foreground",
        "data-[active=true]:bg-sidebar-accent data-[active=true]:text-sidebar-accent-foreground",
        size === "sm" && "text-xs",
        size === "md" && "text-sm",
        "group-data-[collapsible=icon]:hidden",
        className
      )}
      {...props}
    />
  )
})
SidebarMenuSubButton.displayName = "SidebarMenuSubButton"
export {
  Sidebar,
  SidebarContent,
  SidebarFooter,
  SidebarGroup,
  SidebarGroupAction,
  SidebarGroupContent,
  SidebarGroupLabel,
  SidebarHeader,
  SidebarInput,
  SidebarInset,
  SidebarMenu,
  SidebarMenuAction,
  SidebarMenuBadge,
  SidebarMenuButton,
  SidebarMenuItem,
  SidebarMenuSkeleton,
  SidebarMenuSub,
  SidebarMenuSubButton,
  SidebarMenuSubItem,
  SidebarProvider,
  SidebarRail,
  SidebarSeparator,
  SidebarTrigger,
  useSidebar,
}
</file>

<file path="src/components/ui/skeleton.tsx">
import { cn } from "@/lib/utils"
function Skeleton({
  className,
  ...props
}: React.HTMLAttributes<HTMLDivElement>) {
  return (
    <div
      className={cn("animate-pulse rounded-md bg-muted", className)}
      {...props}
    />
  )
}
export { Skeleton }
</file>

<file path="src/components/ui/slider.tsx">
import * as React from "react"
import * as SliderPrimitive from "@radix-ui/react-slider"
import { cn } from "@/lib/utils"
const Slider = React.forwardRef<
  React.ElementRef<typeof SliderPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof SliderPrimitive.Root>
>(({ className, ...props }, ref) => (
  <SliderPrimitive.Root
    ref={ref}
    className={cn(
      "relative flex w-full touch-none select-none items-center",
      className
    )}
    {...props}
  >
    <SliderPrimitive.Track className="relative h-2 w-full grow overflow-hidden rounded-full bg-secondary">
      <SliderPrimitive.Range className="absolute h-full bg-primary" />
    </SliderPrimitive.Track>
    <SliderPrimitive.Thumb className="block h-5 w-5 rounded-full border-2 border-primary bg-background ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50" />
  </SliderPrimitive.Root>
))
Slider.displayName = SliderPrimitive.Root.displayName
export { Slider }
</file>

<file path="src/components/ui/sonner.tsx">
import { useTheme } from "next-themes"
import { Toaster as Sonner } from "sonner"
type ToasterProps = React.ComponentProps<typeof Sonner>
const Toaster = ({ ...props }: ToasterProps) => {
  const { theme = "system" } = useTheme()
  return (
    <Sonner
      theme={theme as ToasterProps["theme"]}
      className="toaster group"
      toastOptions={{
        classNames: {
          toast:
            "group toast group-[.toaster]:bg-background group-[.toaster]:text-foreground group-[.toaster]:border-border group-[.toaster]:shadow-lg",
          description: "group-[.toast]:text-muted-foreground",
          actionButton:
            "group-[.toast]:bg-primary group-[.toast]:text-primary-foreground",
          cancelButton:
            "group-[.toast]:bg-muted group-[.toast]:text-muted-foreground",
        },
      }}
      {...props}
    />
  )
}
export { Toaster }
</file>

<file path="src/components/ui/spinner.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Spinner                                                                             ║
 * ║ Description: Loading spinner component with size variants and smooth animations                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { cva, type VariantProps } from 'class-variance-authority';
import { cn } from '@/lib/utils';
const spinnerVariants = cva(
  'inline-block animate-spin rounded-full border-solid border-current border-r-transparent motion-reduce:animate-[spin_1.5s_linear_infinite]',
  {
    variants: {
      size: {
        default: 'h-4 w-4 border-2',
        sm: 'h-3 w-3 border-2',
        lg: 'h-6 w-6 border-3',
        xl: 'h-8 w-8 border-4',
      },
      variant: {
        default: 'text-primary',
        secondary: 'text-secondary',
        destructive: 'text-destructive',
        muted: 'text-muted-foreground',
      },
    },
    defaultVariants: {
      size: 'default',
      variant: 'default',
    },
  }
);
export interface SpinnerProps
  extends React.HTMLAttributes<HTMLDivElement>,
    VariantProps<typeof spinnerVariants> {
  label?: string;
}
const Spinner = React.forwardRef<HTMLDivElement, SpinnerProps>(
  ({ className, size, variant, label = 'Loading...', ...props }, ref) => {
    return (
      <div
        ref={ref}
        role="status"
        aria-label={label}
        className={cn('relative', className)}
        {...props}
      >
        <div className={cn(spinnerVariants({ size, variant }))} />
        <span className="sr-only">{label}</span>
      </div>
    );
  }
);
Spinner.displayName = 'Spinner';
export { Spinner, spinnerVariants };
</file>

<file path="src/components/ui/switch.tsx">
import * as React from "react"
import * as SwitchPrimitives from "@radix-ui/react-switch"
import { cn } from "@/lib/utils"
const Switch = React.forwardRef<
  React.ElementRef<typeof SwitchPrimitives.Root>,
  React.ComponentPropsWithoutRef<typeof SwitchPrimitives.Root>
>(({ className, ...props }, ref) => (
  <SwitchPrimitives.Root
    className={cn(
      "peer inline-flex h-6 w-11 shrink-0 cursor-pointer items-center rounded-full border-2 border-transparent transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 focus-visible:ring-offset-background disabled:cursor-not-allowed disabled:opacity-50 data-[state=checked]:bg-primary data-[state=unchecked]:bg-input",
      className
    )}
    {...props}
    ref={ref}
  >
    <SwitchPrimitives.Thumb
      className={cn(
        "pointer-events-none block h-5 w-5 rounded-full bg-background shadow-lg ring-0 transition-transform data-[state=checked]:translate-x-5 data-[state=unchecked]:translate-x-0"
      )}
    />
  </SwitchPrimitives.Root>
))
Switch.displayName = SwitchPrimitives.Root.displayName
export { Switch }
</file>

<file path="src/components/ui/table.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Table                                                                               ║
 * ║ Description: Accessible table component with consistent styling                                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import { cn } from "@/lib/utils"
const Table = React.forwardRef<
  HTMLTableElement,
  React.HTMLAttributes<HTMLTableElement>
>(({ className, ...props }, ref) => (
  <div className="relative w-full overflow-auto">
    <table
      ref={ref}
      className={cn("w-full caption-bottom text-sm", className)}
      {...props}
    />
  </div>
))
Table.displayName = "Table"
const TableHeader = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <thead ref={ref} className={cn("[&_tr]:border-b", className)} {...props} />
))
TableHeader.displayName = "TableHeader"
const TableBody = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <tbody
    ref={ref}
    className={cn("[&_tr:last-child]:border-0", className)}
    {...props}
  />
))
TableBody.displayName = "TableBody"
const TableFooter = React.forwardRef<
  HTMLTableSectionElement,
  React.HTMLAttributes<HTMLTableSectionElement>
>(({ className, ...props }, ref) => (
  <tfoot
    ref={ref}
    className={cn(
      "border-t bg-muted/50 font-medium [&>tr]:last:border-b-0",
      className
    )}
    {...props}
  />
))
TableFooter.displayName = "TableFooter"
const TableRow = React.forwardRef<
  HTMLTableRowElement,
  React.HTMLAttributes<HTMLTableRowElement>
>(({ className, ...props }, ref) => (
  <tr
    ref={ref}
    className={cn(
      "border-b transition-colors hover:bg-muted/50 data-[state=selected]:bg-muted",
      className
    )}
    {...props}
  />
))
TableRow.displayName = "TableRow"
const TableHead = React.forwardRef<
  HTMLTableCellElement,
  React.ThHTMLAttributes<HTMLTableCellElement>
>(({ className, ...props }, ref) => (
  <th
    ref={ref}
    className={cn(
      "h-12 px-4 text-left align-middle font-medium text-muted-foreground [&:has([role=checkbox])]:pr-0",
      className
    )}
    {...props}
  />
))
TableHead.displayName = "TableHead"
const TableCell = React.forwardRef<
  HTMLTableCellElement,
  React.TdHTMLAttributes<HTMLTableCellElement>
>(({ className, ...props }, ref) => (
  <td
    ref={ref}
    className={cn("p-4 align-middle [&:has([role=checkbox])]:pr-0", className)}
    {...props}
  />
))
TableCell.displayName = "TableCell"
const TableCaption = React.forwardRef<
  HTMLTableCaptionElement,
  React.HTMLAttributes<HTMLTableCaptionElement>
>(({ className, ...props }, ref) => (
  <caption
    ref={ref}
    className={cn("mt-4 text-sm text-muted-foreground", className)}
    {...props}
  />
))
TableCaption.displayName = "TableCaption"
export {
  Table,
  TableHeader,
  TableBody,
  TableFooter,
  TableHead,
  TableRow,
  TableCell,
  TableCaption,
}
</file>

<file path="src/components/ui/tabs.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Tabs                                                                                ║
 * ║ Description: Tabs component with smooth transitions and keyboard navigation                    ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as TabsPrimitive from "@radix-ui/react-tabs"
import { cn } from "@/lib/utils"
const Tabs = TabsPrimitive.Root
const TabsList = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.List>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.List>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.List
    ref={ref}
    className={cn(
      "inline-flex h-10 items-center justify-center rounded-md bg-muted p-1 text-muted-foreground",
      className
    )}
    {...props}
  />
))
TabsList.displayName = TabsPrimitive.List.displayName
const TabsTrigger = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Trigger>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Trigger
    ref={ref}
    className={cn(
      "inline-flex items-center justify-center whitespace-nowrap rounded-sm px-3 py-1.5 text-sm font-medium ring-offset-background transition-all focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 data-[state=active]:bg-background data-[state=active]:text-foreground data-[state=active]:shadow-sm",
      className
    )}
    {...props}
  />
))
TabsTrigger.displayName = TabsPrimitive.Trigger.displayName
const TabsContent = React.forwardRef<
  React.ElementRef<typeof TabsPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TabsPrimitive.Content>
>(({ className, ...props }, ref) => (
  <TabsPrimitive.Content
    ref={ref}
    className={cn(
      "mt-2 ring-offset-background focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2",
      className
    )}
    {...props}
  />
))
TabsContent.displayName = TabsPrimitive.Content.displayName
export { Tabs, TabsList, TabsTrigger, TabsContent }
</file>

<file path="src/components/ui/textarea.tsx">
import * as React from "react"
import { cn } from "@/lib/utils"
export interface TextareaProps
  extends React.TextareaHTMLAttributes<HTMLTextAreaElement> {}
const Textarea = React.forwardRef<HTMLTextAreaElement, TextareaProps>(
  ({ className, ...props }, ref) => {
    return (
      <textarea
        className={cn(
          "flex min-h-[80px] w-full rounded-md border border-input bg-background px-3 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50",
          className
        )}
        ref={ref}
        {...props}
      />
    )
  }
)
Textarea.displayName = "Textarea"
export { Textarea }
</file>

<file path="src/components/ui/toast.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Toast Component                                                                        ║
 * ║ Description: Toast notification component with variants and animations                         ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as ToastPrimitives from "@radix-ui/react-toast"
import { cva, type VariantProps } from "class-variance-authority"
import { X } from "lucide-react"
import { cn } from "@/lib/utils"
const ToastProvider = ToastPrimitives.Provider
const ToastViewport = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Viewport>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Viewport>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Viewport
    ref={ref}
    className={cn(
      "fixed top-0 z-[100] flex max-h-screen w-full flex-col-reverse p-4 sm:bottom-0 sm:right-0 sm:top-auto sm:flex-col md:max-w-[420px]",
      className
    )}
    {...props}
  />
))
ToastViewport.displayName = ToastPrimitives.Viewport.displayName
const toastVariants = cva(
  "group pointer-events-auto relative flex w-full items-center justify-between space-x-4 overflow-hidden rounded-md border p-6 pr-8 shadow-lg transition-all data-[swipe=cancel]:translate-x-0 data-[swipe=end]:translate-x-[var(--radix-toast-swipe-end-x)] data-[swipe=move]:translate-x-[var(--radix-toast-swipe-move-x)] data-[swipe=move]:transition-none data-[state=open]:animate-in data-[state=closed]:animate-out data-[swipe=end]:animate-out data-[state=closed]:fade-out-80 data-[state=closed]:slide-out-to-right-full data-[state=open]:slide-in-from-top-full data-[state=open]:sm:slide-in-from-bottom-full",
  {
    variants: {
      variant: {
        default: "border bg-background text-foreground",
        destructive:
          "destructive group border-destructive bg-destructive text-destructive-foreground",
      },
    },
    defaultVariants: {
      variant: "default",
    },
  }
)
const Toast = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Root>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Root> &
    VariantProps<typeof toastVariants>
>(({ className, variant, ...props }, ref) => {
  return (
    <ToastPrimitives.Root
      ref={ref}
      className={cn(toastVariants({ variant }), className)}
      {...props}
    />
  )
})
Toast.displayName = ToastPrimitives.Root.displayName
const ToastAction = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Action>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Action>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Action
    ref={ref}
    className={cn(
      "inline-flex h-8 shrink-0 items-center justify-center rounded-md border bg-transparent px-3 text-sm font-medium ring-offset-background transition-colors hover:bg-secondary focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 group-[.destructive]:border-muted/40 group-[.destructive]:hover:border-destructive/30 group-[.destructive]:hover:bg-destructive group-[.destructive]:hover:text-destructive-foreground group-[.destructive]:focus:ring-destructive",
      className
    )}
    {...props}
  />
))
ToastAction.displayName = ToastPrimitives.Action.displayName
const ToastClose = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Close>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Close>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Close
    ref={ref}
    className={cn(
      "absolute right-2 top-2 rounded-md p-1 text-foreground/50 opacity-0 transition-opacity hover:text-foreground focus:opacity-100 focus:outline-none focus:ring-2 group-hover:opacity-100 group-[.destructive]:text-red-300 group-[.destructive]:hover:text-red-50 group-[.destructive]:focus:ring-red-400 group-[.destructive]:focus:ring-offset-red-600",
      className
    )}
    toast-close=""
    {...props}
  >
    <X className="h-4 w-4" />
  </ToastPrimitives.Close>
))
ToastClose.displayName = ToastPrimitives.Close.displayName
const ToastTitle = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Title>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Title>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Title
    ref={ref}
    className={cn("text-sm font-semibold", className)}
    {...props}
  />
))
ToastTitle.displayName = ToastPrimitives.Title.displayName
const ToastDescription = React.forwardRef<
  React.ElementRef<typeof ToastPrimitives.Description>,
  React.ComponentPropsWithoutRef<typeof ToastPrimitives.Description>
>(({ className, ...props }, ref) => (
  <ToastPrimitives.Description
    ref={ref}
    className={cn("text-sm opacity-90", className)}
    {...props}
  />
))
ToastDescription.displayName = ToastPrimitives.Description.displayName
type ToastProps = React.ComponentPropsWithoutRef<typeof Toast>
type ToastActionElement = React.ReactElement<typeof ToastAction>
export {
  type ToastProps,
  type ToastActionElement,
  ToastProvider,
  ToastViewport,
  Toast,
  ToastTitle,
  ToastDescription,
  ToastClose,
  ToastAction,
}
</file>

<file path="src/components/ui/toaster.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Toast Provider                                                                         ║
 * ║ Description: Toast notification provider component with animations                             ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import {
  Toast,
  ToastClose,
  ToastDescription,
  ToastProvider,
  ToastTitle,
  ToastViewport,
} from "@/components/ui/toast"
import { useToast } from "@/components/ui/use-toast"
export function Toaster() {
  const { toasts } = useToast()
  return (
    <ToastProvider>
      {toasts.map(function ({ id, title, description, action, ...props }) {
        return (
          <Toast key={id} {...props}>
            <div className="grid gap-1">
              {title && <ToastTitle>{title}</ToastTitle>}
              {description && (
                <ToastDescription>{description}</ToastDescription>
              )}
            </div>
            {action}
            <ToastClose />
          </Toast>
        )
      })}
      <ToastViewport />
    </ToastProvider>
  )
}
</file>

<file path="src/components/ui/toggle-group.tsx">
import * as React from "react"
import * as ToggleGroupPrimitive from "@radix-ui/react-toggle-group"
import { type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
import { toggleVariants } from "@/components/ui/toggle"
const ToggleGroupContext = React.createContext<
  VariantProps<typeof toggleVariants>
>({
  size: "default",
  variant: "default",
})
const ToggleGroup = React.forwardRef<
  React.ElementRef<typeof ToggleGroupPrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof ToggleGroupPrimitive.Root> &
    VariantProps<typeof toggleVariants>
>(({ className, variant, size, children, ...props }, ref) => (
  <ToggleGroupPrimitive.Root
    ref={ref}
    className={cn("flex items-center justify-center gap-1", className)}
    {...props}
  >
    <ToggleGroupContext.Provider value={{ variant, size }}>
      {children}
    </ToggleGroupContext.Provider>
  </ToggleGroupPrimitive.Root>
))
ToggleGroup.displayName = ToggleGroupPrimitive.Root.displayName
const ToggleGroupItem = React.forwardRef<
  React.ElementRef<typeof ToggleGroupPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof ToggleGroupPrimitive.Item> &
    VariantProps<typeof toggleVariants>
>(({ className, children, variant, size, ...props }, ref) => {
  const context = React.useContext(ToggleGroupContext)
  return (
    <ToggleGroupPrimitive.Item
      ref={ref}
      className={cn(
        toggleVariants({
          variant: context.variant || variant,
          size: context.size || size,
        }),
        className
      )}
      {...props}
    >
      {children}
    </ToggleGroupPrimitive.Item>
  )
})
ToggleGroupItem.displayName = ToggleGroupPrimitive.Item.displayName
export { ToggleGroup, ToggleGroupItem }
</file>

<file path="src/components/ui/toggle.tsx">
import * as React from "react"
import * as TogglePrimitive from "@radix-ui/react-toggle"
import { cva, type VariantProps } from "class-variance-authority"
import { cn } from "@/lib/utils"
const toggleVariants = cva(
  "inline-flex items-center justify-center rounded-md text-sm font-medium ring-offset-background transition-colors hover:bg-muted hover:text-muted-foreground focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 data-[state=on]:bg-accent data-[state=on]:text-accent-foreground",
  {
    variants: {
      variant: {
        default: "bg-transparent",
        outline:
          "border border-input bg-transparent hover:bg-accent hover:text-accent-foreground",
      },
      size: {
        default: "h-10 px-3",
        sm: "h-9 px-2.5",
        lg: "h-11 px-5",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
)
const Toggle = React.forwardRef<
  React.ElementRef<typeof TogglePrimitive.Root>,
  React.ComponentPropsWithoutRef<typeof TogglePrimitive.Root> &
    VariantProps<typeof toggleVariants>
>(({ className, variant, size, ...props }, ref) => (
  <TogglePrimitive.Root
    ref={ref}
    className={cn(toggleVariants({ variant, size, className }))}
    {...props}
  />
))
Toggle.displayName = TogglePrimitive.Root.displayName
export { Toggle, toggleVariants }
</file>

<file path="src/components/ui/tooltip.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Tooltip                                                                             ║
 * ║ Description: Tooltip component with smooth animations and positioning                          ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import * as TooltipPrimitive from "@radix-ui/react-tooltip"
import { cn } from "@/lib/utils"
const TooltipProvider = TooltipPrimitive.Provider
const Tooltip = TooltipPrimitive.Root
const TooltipTrigger = TooltipPrimitive.Trigger
const TooltipContent = React.forwardRef<
  React.ElementRef<typeof TooltipPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof TooltipPrimitive.Content>
>(({ className, sideOffset = 4, ...props }, ref) => (
  <TooltipPrimitive.Content
    ref={ref}
    sideOffset={sideOffset}
    className={cn(
      "z-50 overflow-hidden rounded-md bg-primary px-3 py-1.5 text-xs text-primary-foreground animate-in fade-in-0 zoom-in-95 data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=closed]:zoom-out-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
      className
    )}
    {...props}
  />
))
TooltipContent.displayName = TooltipPrimitive.Content.displayName
export { Tooltip, TooltipTrigger, TooltipContent, TooltipProvider }
</file>

<file path="src/components/ui/use-toast.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Toast Hook                                                                             ║
 * ║ Description: React hook for managing toast notifications                                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import * as React from "react"
import type {
  ToastActionElement,
  ToastProps,
} from "@/components/ui/toast"
const TOAST_LIMIT = 1
const TOAST_REMOVE_DELAY = 1000000
type ToasterToast = ToastProps & {
  id: string
  title?: React.ReactNode
  description?: React.ReactNode
  action?: ToastActionElement
}
const actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
} as const
let count = 0
function genId() {
  count = (count + 1) % Number.MAX_VALUE
  return count.toString()
}
type ActionType = typeof actionTypes
type Action =
  | {
      type: ActionType["ADD_TOAST"]
      toast: ToasterToast
    }
  | {
      type: ActionType["UPDATE_TOAST"]
      toast: Partial<ToasterToast>
    }
  | {
      type: ActionType["DISMISS_TOAST"]
      toastId?: ToasterToast["id"]
    }
  | {
      type: ActionType["REMOVE_TOAST"]
      toastId?: ToasterToast["id"]
    }
interface State {
  toasts: ToasterToast[]
}
const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>()
const addToRemoveQueue = (toastId: string) => {
  if (toastTimeouts.has(toastId)) {
    return
  }
  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId)
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    })
  }, TOAST_REMOVE_DELAY)
  toastTimeouts.set(toastId, timeout)
}
export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST":
      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      }
    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t
        ),
      }
    case "DISMISS_TOAST": {
      const { toastId } = action
      // ! Side effects ! - This could be extracted into a dismissToast() action,
      // but I'll keep it here for simplicity
      if (toastId) {
        addToRemoveQueue(toastId)
      } else {
        state.toasts.forEach((toast) => {
          addToRemoveQueue(toast.id)
        })
      }
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t
        ),
      }
    }
    case "REMOVE_TOAST":
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        }
      }
      return {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
      }
  }
}
const listeners: Array<(state: State) => void> = []
let memoryState: State = { toasts: [] }
function dispatch(action: Action) {
  memoryState = reducer(memoryState, action)
  listeners.forEach((listener) => {
    listener(memoryState)
  })
}
type Toast = Omit<ToasterToast, "id">
function toast({ ...props }: Toast) {
  const id = genId()
  const update = (props: ToasterToast) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    })
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id })
  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      open: true,
      onOpenChange: (open) => {
        if (!open) dismiss()
      },
    },
  })
  return {
    id: id,
    dismiss,
    update,
  }
}
function useToast() {
  const [state, setState] = React.useState<State>(memoryState)
  React.useEffect(() => {
    listeners.push(setState)
    return () => {
      const index = listeners.indexOf(setState)
      if (index > -1) {
        listeners.splice(index, 1)
      }
    }
  }, [state])
  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
  }
}
export { useToast, toast }
</file>

<file path="src/components/AccessibleTypingTutor.tsx">
/*
 * ████████╗██╗   ██╗██████╗ ██╗███╗   ██╗ ██████╗     ████████╗██╗   ██╗████████╗ ██████╗ ██████╗
 * ╚══██╔══╝╚██╗ ██╔╝██╔══██╗██║████╗  ██║██╔════╝     ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗
 *    ██║    ╚████╔╝ ██████╔╝██║██╔██╗ ██║██║  ███╗       ██║   ██║   ██║   ██║   ██║   ██║██████╔╝
 *    ██║     ╚██╔╝  ██╔═══╝ ██║██║╚██╗██║██║   ██║       ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗
 *    ██║      ██║   ██║     ██║██║ ╚████║╚██████╔╝       ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║
 *    ╚═╝      ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═══╝ ╚═════╝        ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: AccessibleTypingTutor                                                               ║
 * ║ Description: Core typing tutor component with accessibility features, voice control,           ║
 * ║              and adaptive learning capabilities. Provides real-time feedback and analytics     ║
 * ║              for an enhanced typing learning experience.                                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useState, useEffect, useCallback, useRef } from "react";
import { useToast } from "@/hooks/use-toast";
import { supabase } from "@/integrations/supabase/client";
import { User } from "@supabase/supabase-js";
import { Volume2, VolumeX, Trophy } from "lucide-react";
import VisualKeyboard from "./VisualKeyboard";
import PerformanceChart from "./PerformanceChart";
import { analyzeTypingPatterns, generateAdaptiveLessons, type TypingAnalysis } from "@/utils/typingAnalytics";
const LESSON_SETS = {
  beginner: [
    "Hello world!",
    "The quick brown fox jumps over the lazy dog.",
    "Practice makes perfect.",
    "Type with confidence and accuracy.",
    "Keep your fingers on the home row keys.",
  ],
  intermediate: [
    "JavaScript is a popular programming language.",
    "React helps build interactive user interfaces.",
    "Programming requires attention to detail.",
    "Learning to type faster improves productivity.",
    "Good developers write clean, readable code.",
  ],
  advanced: [
    "The function takes a callback as an argument and executes it asynchronously.",
    "Object-oriented programming emphasizes code reusability and maintainability.",
    "TypeScript adds static typing to JavaScript for better development experience.",
    "Version control systems help teams collaborate on code effectively.",
    "Regular expressions are powerful tools for pattern matching in strings.",
  ],
};
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}
interface TypingHistory {
  id: string;
  user_id: string;
  created_at: string;
  words_per_minute: number;
  accuracy_percentage: number;
  lesson_level: string;
}
const AccessibleTypingTutor = () => {
  const [text, setText] = useState("");
  const [target, setTarget] = useState("Press any key to begin");
  const [isListening, setIsListening] = useState(false);
  const [startTime, setStartTime] = useState<number | null>(null);
  const [errorCount, setErrorCount] = useState(0);
  const [feedback, setFeedback] = useState<string | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [isTutorEnabled, setIsTutorEnabled] = useState(true);
  const [voiceIndex, setVoiceIndex] = useState(0);
  const [isMuted, setIsMuted] = useState(false);
  const [currentLevel, setCurrentLevel] = useState<'beginner' | 'intermediate' | 'advanced'>('beginner');
  const [lessonIndex, setLessonIndex] = useState(0);
  const [user, setUser] = useState<User | null>(null);
  const [stats, setStats] = useState<{ averageWpm: number; averageAccuracy: number } | null>(null);
  const [pressedKey, setPressedKey] = useState<string | null>(null);
  const [performanceHistory, setPerformanceHistory] = useState<Array<{
    date: string;
    wpm: number;
    accuracy: number;
  }>>([]);
  const [showWelcome, setShowWelcome] = useState(true);
  const [mistakes, setMistakes] = useState<Array<{ actual: string; expected: string }>>([]);
  const [recentWPMs, setRecentWPMs] = useState<number[]>([]);
  const [analysis, setAnalysis] = useState<TypingAnalysis | null>(null);
  const [streak, setStreak] = useState(0);
  const { toast } = useToast();
  const announcer = useRef<HTMLDivElement>(null);
  const inputRef = useRef<HTMLInputElement>(null);
  const voices = useRef<SpeechSynthesisVoice[]>([]);
  useEffect(() => {
    supabase.auth.getSession().then(({ data: { session } }) => {
      setUser(session?.user ?? null);
      if (session?.user) {
        fetchUserStats(session.user.id);
      }
    });
    const { data: { subscription } } = supabase.auth.onAuthStateChange((_event, session) => {
      setUser(session?.user ?? null);
      if (session?.user) {
        fetchUserStats(session.user.id);
      }
    });
    return () => subscription.unsubscribe();
  }, []);
  const fetchUserStats = async (userId: string) => {
    const { data: profile, error } = await supabase
      .from('profiles')
      .select('words_per_minute, accuracy_percentage')
      .eq('id', userId)
      .single();
    if (!error && profile) {
      setStats({
        averageWpm: profile.words_per_minute || 0,
        averageAccuracy: profile.accuracy_percentage || 0
      });
    }
  };
  useEffect(() => {
    const loadVoices = () => {
      voices.current = window.speechSynthesis.getVoices();
      const preferredVoice = voices.current.findIndex(
        voice => voice.name.includes('Natural') || voice.name.includes('Premium')
      );
      if (preferredVoice !== -1) {
        setVoiceIndex(preferredVoice);
      }
    };
    window.speechSynthesis.onvoiceschanged = loadVoices;
    loadVoices();
    return () => {
      window.speechSynthesis.onvoiceschanged = null;
    };
  }, []);
  const announce = (message: string) => {
    if (announcer.current) {
      announcer.current.textContent = message;
    }
    if (window.speechSynthesis && !isMuted) {
      window.speechSynthesis.cancel();
      const utterance = new SpeechSynthesisUtterance(message);
      if (voices.current.length > 0) {
        utterance.voice = voices.current[voiceIndex];
        utterance.rate = 0.9; // Slightly slower for better clarity
        utterance.pitch = 1;
      }
      window.speechSynthesis.speak(utterance);
    }
  };
  const getNextLesson = () => {
    const currentLessons = LESSON_SETS[currentLevel];
    if (lessonIndex >= currentLessons.length - 1) {
      if (currentLevel === 'beginner') {
        setCurrentLevel('intermediate');
        setLessonIndex(0);
        return LESSON_SETS.intermediate[0];
      } else if (currentLevel === 'intermediate') {
        setCurrentLevel('advanced');
        setLessonIndex(0);
        return LESSON_SETS.advanced[0];
      } else {
        setLessonIndex(0);
        return currentLessons[0];
      }
    } else {
      setLessonIndex(prev => prev + 1);
      return currentLessons[lessonIndex + 1];
    }
  };
  const calculateResults = useCallback(async () => {
    if (!startTime || !user) return;
    const endTime = Date.now();
    const timeInMinutes = (endTime - startTime) / 1000 / 60;
    const wordsTyped = target.split(' ').length;
    const wpm = Math.round(wordsTyped / timeInMinutes);
    const accuracy = Math.round(((target.length - errorCount) / target.length) * 100);
    setRecentWPMs(prev => [...prev.slice(-9), wpm]);
    const newAnalysis = analyzeTypingPatterns(mistakes, [...recentWPMs, wpm]);
    setAnalysis(newAnalysis);
    if (accuracy > 90) {
      setStreak(prev => prev + 1);
    } else {
      setStreak(0);
    }
    const adaptiveLessons = generateAdaptiveLessons(newAnalysis, currentLevel);
    announce(`Lesson completed! Your speed was ${wpm} words per minute with ${accuracy}% accuracy.`);
    if (isTutorEnabled) {
      setIsLoading(true);
      try {
        const { data, error } = await supabase.functions.invoke('gemini-tutor', {
          body: {
            text,
            target,
            wpm,
            accuracy
          },
        });
        if (error) throw error;
        setFeedback(data.feedback);
        announce(data.feedback);
      } catch (error) {
        console.error('Error getting AI feedback:', error);
        toast({
          title: "Error getting AI feedback",
          description: "Could not get AI feedback at this time",
          variant: "destructive",
        });
      }
    }
    if (user) {
      try {
        const { error: profileError } = await supabase
          .from('profiles')
          .update({
            words_per_minute: wpm,
            accuracy_percentage: accuracy,
            last_lesson_date: new Date().toISOString()
          })
          .eq('id', user.id);
        const { error: historyError } = await supabase
          .from('typing_history')
          .insert({
            user_id: user.id,
            words_per_minute: wpm,
            accuracy_percentage: accuracy,
            lesson_level: currentLevel
          });
        if (profileError || historyError) {
          throw profileError || historyError;
        }
        setPerformanceHistory(prev => [...prev, {
          date: new Date().toISOString(),
          wpm,
          accuracy
        }]);
        await fetchUserStats(user.id);
      } catch (error) {
        console.error('Error saving results:', error);
        toast({
          title: "Error saving results",
          description: "Your progress couldn't be saved. Please try again.",
          variant: "destructive",
        });
      }
    }
    toast({
      title: "Lesson completed!",
      description: `Speed: ${wpm} WPM | Accuracy: ${accuracy}%`,
      duration: 5000,
    });
    setStartTime(null);
    setErrorCount(0);
    setText("");
    setTarget(getNextLesson());
    setIsLoading(false);
  }, [startTime, target, errorCount, text, toast, isTutorEnabled, user, currentLevel, mistakes, recentWPMs]);
  const handleInput = (e: React.ChangeEvent<HTMLInputElement>) => {
    const value = e.target.value;
    if (showWelcome) {
      setShowWelcome(false);
      setTarget(LESSON_SETS.beginner[0]);
      setText("");
      return;
    }
    if (!startTime) {
      setStartTime(Date.now());
      announce("Starting timer. Begin typing.");
    }
    if (target.startsWith(value)) {
      setText(value);
      if (value === target) {
        calculateResults();
      }
    } else {
      setErrorCount(prev => prev + 1);
      const expectedChar = target[text.length];
      const lastChar = value[value.length - 1];
      setMistakes(prev => [...prev, { actual: lastChar, expected: expectedChar }]);
      announce(`Incorrect. Expected ${expectedChar}, got ${lastChar}`);
      toast({
        title: "Incorrect key",
        description: `Expected "${expectedChar}" but got "${lastChar}"`,
        variant: "destructive",
        duration: 2000,
      });
    }
  };
  useEffect(() => {
    const handleKeyDown = (e: KeyboardEvent) => {
      if (showWelcome) {
        setShowWelcome(false);
        setTarget(LESSON_SETS.beginner[0]);
        setText("");
      }
      setPressedKey(e.key);
    };
    const handleKeyUp = () => {
      setPressedKey(null);
    };
    window.addEventListener('keydown', handleKeyDown);
    window.addEventListener('keyup', handleKeyUp);
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
      window.removeEventListener('keyup', handleKeyUp);
    };
  }, [showWelcome]);
  const toggleVoiceControl = () => {
    setIsListening(prev => !prev);
    const message = isListening ? "Voice control disabled" : "Voice control enabled";
    announce(message);
  };
  useEffect(() => {
    if (!isListening) return;
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      announce("Speech recognition is not supported in this browser");
      setIsListening(false);
      return;
    }
    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.onstart = () => {
      announce("Voice recognition started");
    };
    recognition.onresult = (event) => {
      const transcript = Array.from(event.results)
        .map(result => result[0].transcript)
        .join(' ')
        .toLowerCase();
      console.log("Voice command detected:", transcript);
      if (transcript.includes('start')) {
        if (target.includes("Press any key")) {
          setTarget("Hello world!");
          setText("");
          setStartTime(Date.now());
          announce("Starting new lesson");
        }
      } else if (transcript.includes('stop') || transcript.includes('end')) {
        setIsListening(false);
        announce("Voice control disabled");
      }
    };
    recognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error);
      announce("Voice recognition error. Please try again.");
    };
    try {
      recognition.start();
    } catch (error) {
      console.error('Error starting speech recognition:', error);
    }
    return () => {
      try {
        recognition.stop();
      } catch (error) {
        console.error('Error stopping speech recognition:', error);
      }
    };
  }, [isListening, target]);
  useEffect(() => {
    inputRef.current?.focus();
  }, [target]);
  useEffect(() => {
    const fetchPerformanceHistory = async () => {
      if (!user) return;
      const { data, error } = await supabase
        .from('typing_history')
        .select('*')
        .eq('user_id', user.id)
        .order('created_at', { ascending: true });
      if (!error && data) {
        setPerformanceHistory(data.map((record: TypingHistory) => ({
          date: record.created_at,
          wpm: record.words_per_minute,
          accuracy: record.accuracy_percentage
        })));
      }
    };
    fetchPerformanceHistory();
  }, [user]);
  const getNextExpectedKey = () => {
    if (!target || text.length >= target.length) return null;
    return target[text.length];
  };
  return (
    <div className="min-h-screen bg-background text-foreground p-8 flex flex-col items-center justify-center space-y-8 animate-fade-in">
      <div
        ref={announcer}
        className="sr-only"
        role="status"
        aria-live="polite"
      />
      {showWelcome && (
        <div className="fixed inset-0 bg-background/95 backdrop-blur-sm flex items-center justify-center z-50 animate-fade-in">
          <div className="text-center space-y-4">
            <h1 className="text-4xl font-bold mb-8">Welcome to the Typing Tutor</h1>
            <p className="text-xl text-secondary animate-pulse">Press any key to begin</p>
          </div>
        </div>
      )}
      <div className="w-full max-w-2xl mb-8">
        <div className="flex justify-between items-center mb-6">
          <div className="flex gap-4">
            <button
              onClick={() => setIsListening(prev => !prev)}
              className={`p-3 rounded-lg transition-all duration-300 ${
                isListening ? "bg-primary/20 text-primary" : "bg-secondary/10"
              }`}
            >
              {isListening ? "🎤 Listening..." : "🎤"}
            </button>
            <button
              onClick={() => setIsMuted(prev => !prev)}
              className="p-3 rounded-lg bg-secondary/10"
            >
              {isMuted ? <VolumeX className="h-5 w-5" /> : <Volume2 className="h-5 w-5" />}
            </button>
          </div>
          {user && stats && (
            <div className="text-sm text-secondary">
              Average: {stats.averageWpm} WPM | {stats.averageAccuracy}% accuracy
            </div>
          )}
          {streak > 0 && (
            <div className="flex items-center gap-2 text-primary animate-scale-in">
              <Trophy className="h-5 w-5" />
              <span>Streak: {streak}</span>
            </div>
          )}
          {analysis && (
            <div className="text-sm text-secondary">
              Consistency: {analysis.consistencyScore}%
            </div>
          )}
        </div>
        <div className="space-y-8 p-8 rounded-lg border border-border/50 bg-black/30 backdrop-blur-sm">
          <div className="relative">
            <p className="text-4xl font-mono text-secondary mb-4 transition-all duration-300">
              {target}
            </p>
            <div className="h-px bg-border/50 my-8" />
            <div className="relative">
              <input
                ref={inputRef}
                type="text"
                value={text}
                onChange={handleInput}
                className="w-full bg-transparent text-4xl font-mono focus:outline-none focus:ring-0 transition-all duration-300"
                style={{
                  caretColor: 'currentcolor',
                  animation: 'cursor-blink 1s step-end infinite'
                }}
                autoComplete="off"
                autoCapitalize="off"
                autoCorrect="off"
                spellCheck="false"
              />
              <div
                className="absolute bottom-0 left-0 w-full h-0.5 bg-primary/50 origin-left transition-transform duration-300"
                style={{
                  transform: `scaleX(${text.length / target.length})`
                }}
              />
            </div>
          </div>
        </div>
      </div>
      <div className="w-full max-w-6xl mt-8">
        <VisualKeyboard
          pressedKey={pressedKey}
          nextKey={getNextExpectedKey()}
        />
      </div>
      {user && performanceHistory.length > 0 && (
        <div className="w-full max-w-6xl">
          <h2 className="text-xl font-semibold mb-4">Your Progress</h2>
          <PerformanceChart data={performanceHistory} />
        </div>
      )}
      {analysis && (
        <div className="w-full max-w-2xl p-4 rounded-lg border border-border/50 bg-black/30 backdrop-blur-sm">
          <h3 className="text-lg font-semibold mb-4">Typing Analysis</h3>
          <div className="grid grid-cols-2 gap-4">
            <div>
              <h4 className="font-medium mb-2">Common Mistakes</h4>
              <ul className="space-y-2">
                {analysis.commonMistakes.map((mistake, i) => (
                  <li key={i} className="text-sm text-secondary">
                    Typed "{mistake.character}" instead of "{mistake.expectedChar}" ({mistake.count}x)
                  </li>
                ))}
              </ul>
            </div>
            <div>
              <h4 className="font-medium mb-2">Problem Keys</h4>
              <div className="flex flex-wrap gap-2">
                {analysis.problemKeys.map((key, i) => (
                  <span key={i} className="px-2 py-1 rounded-md bg-destructive/20 text-destructive text-sm">
                    {key}
                  </span>
                ))}
              </div>
            </div>
          </div>
        </div>
      )}
      {feedback && isTutorEnabled && (
        <div
          className="p-4 rounded-lg border border-primary/50 bg-primary/5"
          role="alert"
          aria-live="polite"
        >
          <h2 className="text-lg font-semibold mb-2">AI Feedback</h2>
          <p className="text-secondary">{feedback}</p>
        </div>
      )}
      {isLoading && isTutorEnabled && (
        <div
          className="text-center text-secondary animate-pulse"
          role="alert"
          aria-live="polite"
        >
          Getting AI feedback...
        </div>
      )}
      <div
        id="typing-instructions"
        className="mt-8 text-center text-secondary animate-slide-up"
      >
        <p>Press Tab to navigate, Space to select, and use arrow keys for navigation.</p>
        <p>Voice commands: Say "start" to begin, "stop" to disable voice control.</p>
      </div>
    </div>
  );
};
export default AccessibleTypingTutor;
</file>

<file path="src/components/ChartTooltip.tsx">
import { cn } from '@/lib/utils'; // Utility for merging class names
// ChartTooltip component with enhanced visual clarity and built-in debug logging
function ChartTooltip(): JSX.Element {
  console.debug("ChartTooltip: Rendering with classes 'text-muted-foreground' and 'bg-muted/50'");
  return (
    // The cn() utility ensures that Tailwind CSS classes are merged correctly for both text and background styling
    <div className={cn('text-muted-foreground', 'bg-muted/50', 'p-4', 'rounded-lg', 'shadow')}>
      <p className="font-semibold">This is your tooltip!</p>
      <small className="block mt-1">Additional info goes here.</small>
    </div>
  );
}
export { ChartTooltip };
</file>

<file path="src/components/ErrorBoundary.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: ErrorBoundary                                                                       ║
 * ║ Description: Error boundary component for handling runtime errors gracefully                   ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { AlertTriangle } from 'lucide-react';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardDescription, CardFooter, CardHeader, CardTitle } from '@/components/ui/card';
interface Props {
  children: React.ReactNode;
  fallback?: React.ReactNode;
}
interface State {
  hasError: boolean;
  error: Error | null;
}
class ErrorBoundary extends React.Component<Props, State> {
  public state: State = {
    hasError: false,
    error: null,
  };
  public static getDerivedStateFromError(error: Error): State {
    return {
      hasError: true,
      error,
    };
  }
  public componentDidCatch(error: Error, errorInfo: React.ErrorInfo) {
    console.error('Uncaught error:', error, errorInfo);
  }
  private handleReset = () => {
    this.setState({
      hasError: false,
      error: null,
    });
  };
  private handleReload = () => {
    window.location.reload();
  };
  public render() {
    if (this.state.hasError) {
      if (this.props.fallback) {
        return this.props.fallback;
      }
      return (
        <div className="min-h-screen flex items-center justify-center p-4">
          <Card className="w-full max-w-md">
            <CardHeader>
              <div className="flex items-center gap-2 text-destructive">
                <AlertTriangle className="h-5 w-5" />
                <CardTitle>Something went wrong</CardTitle>
              </div>
              <CardDescription>
                An error occurred while rendering this component
              </CardDescription>
            </CardHeader>
            <CardContent>
              <div className="bg-muted p-4 rounded-lg overflow-auto">
                <code className="text-sm whitespace-pre-wrap font-mono">
                  {this.state.error?.message}
                </code>
              </div>
            </CardContent>
            <CardFooter className="flex justify-end gap-4">
              <Button
                variant="outline"
                onClick={this.handleReset}
              >
                Try Again
              </Button>
              <Button
                variant="default"
                onClick={this.handleReload}
              >
                Reload Page
              </Button>
            </CardFooter>
          </Card>
        </div>
      );
    }
    return this.props.children;
  }
}
export default ErrorBoundary;
</file>

<file path="src/components/LoadingState.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: LoadingState                                                                        ║
 * ║ Description: Loading state component with beautiful animations                                 ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { Keyboard } from 'lucide-react';
import { cn } from '@/lib/utils';
interface LoadingStateProps {
  className?: string;
  message?: string;
  fullScreen?: boolean;
  size?: 'sm' | 'md' | 'lg';
  variant?: 'default' | 'minimal' | 'fancy';
}
const LoadingState: React.FC<LoadingStateProps> = ({
  className,
  message = 'Loading...',
  fullScreen = false,
  size = 'md',
  variant = 'default',
}) => {
  const containerClasses = cn(
    "flex flex-col items-center justify-center gap-4",
    fullScreen && "min-h-screen",
    className
  );
  const iconSizes = {
    sm: 'w-8 h-8',
    md: 'w-12 h-12',
    lg: 'w-16 h-16',
  };
  const dotSizes = {
    sm: 'w-1.5 h-1.5',
    md: 'w-2 h-2',
    lg: 'w-3 h-3',
  };
  if (variant === 'minimal') {
    return (
      <div className={containerClasses}>
        <div className="flex gap-1">
          {[...Array(3)].map((_, i) => (
            <div
              key={i}
              className={cn(
                "rounded-full bg-primary/50 animate-bounce",
                dotSizes[size]
              )}
              style={{
                animationDelay: `${i * 0.2}s`,
              }}
            />
          ))}
        </div>
      </div>
    );
  }
  if (variant === 'fancy') {
    return (
      <div className={containerClasses}>
        <div className="relative">
          <div className={cn(
            "relative rounded-lg overflow-hidden bg-gradient-to-r from-background via-primary/5 to-background p-8",
            "before:absolute before:inset-0 before:bg-shimmer before:animate-shimmer"
          )}>
            <Keyboard
              className={cn(
                "text-primary animate-bounce-subtle",
                iconSizes[size]
              )}
              strokeWidth={1.5}
            />
          </div>
        </div>
        <div className="flex flex-col items-center gap-2">
          <p className={cn(
            "font-medium text-foreground animate-pulse",
            {
              'text-sm': size === 'sm',
              'text-lg': size === 'md',
              'text-xl': size === 'lg',
            }
          )}>
            {message}
          </p>
          <div className="flex gap-2">
            {[...Array(3)].map((_, i) => (
              <div
                key={i}
                className={cn(
                  "rounded-full bg-primary/50",
                  dotSizes[size],
                  "animate-[bounce_1s_ease-in-out_infinite]"
                )}
                style={{
                  animationDelay: `${i * 0.2}s`,
                }}
              />
            ))}
          </div>
        </div>
      </div>
    );
  }
  // Default variant
  return (
    <div className={containerClasses}>
      <div className="relative">
        <Keyboard
          className={cn(
            "text-primary animate-pulse",
            iconSizes[size]
          )}
          strokeWidth={1.5}
        />
        <div className="absolute inset-0 bg-shimmer animate-shimmer" />
      </div>
      <div className="flex flex-col items-center gap-2">
        <p className={cn(
          "font-medium text-foreground",
          {
            'text-sm': size === 'sm',
            'text-lg': size === 'md',
            'text-xl': size === 'lg',
          }
        )}>
          {message}
        </p>
        <div className="flex gap-1">
          {[...Array(3)].map((_, i) => (
            <div
              key={i}
              className={cn(
                "rounded-full bg-primary/50 animate-bounce",
                dotSizes[size]
              )}
              style={{
                animationDelay: `${i * 0.2}s`,
              }}
            />
          ))}
        </div>
      </div>
    </div>
  );
};
export default LoadingState;
</file>

<file path="src/components/PerformanceChart.tsx">
import React from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';
interface PerformanceData {
  date: string;
  wpm: number;
  accuracy: number;
}
interface PerformanceChartProps {
  data: PerformanceData[];
}
const PerformanceChart: React.FC<PerformanceChartProps> = ({ data }) => {
  return (
    <div className="w-full h-[300px] bg-black/30 p-4 rounded-xl backdrop-blur-sm border border-border/50">
      <ResponsiveContainer width="100%" height="100%">
        <LineChart data={data} margin={{ top: 5, right: 30, left: 20, bottom: 5 }}>
          <CartesianGrid strokeDasharray="3 3" stroke="#333" />
          <XAxis 
            dataKey="date" 
            stroke="#666"
            tickFormatter={(value) => new Date(value).toLocaleDateString()}
          />
          <YAxis yAxisId="left" stroke="#9b87f5" />
          <YAxis yAxisId="right" orientation="right" stroke="#0EA5E9" />
          <Tooltip 
            contentStyle={{ 
              backgroundColor: 'rgba(0,0,0,0.8)', 
              border: '1px solid #333',
              borderRadius: '8px'
            }}
          />
          <Legend />
          <Line
            yAxisId="left"
            type="monotone"
            dataKey="wpm"
            name="WPM"
            stroke="#9b87f5"
            activeDot={{ r: 8 }}
          />
          <Line
            yAxisId="right"
            type="monotone"
            dataKey="accuracy"
            name="Accuracy %"
            stroke="#0EA5E9"
            activeDot={{ r: 8 }}
          />
        </LineChart>
      </ResponsiveContainer>
    </div>
  );
};
export default PerformanceChart;
</file>

<file path="src/components/VisualKeyboard.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: VisualKeyboard                                                                      ║
 * ║ Description: Interactive keyboard visualization with key highlighting and finger indicators     ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { cn } from '@/lib/utils';
interface KeyProps {
  label: string;
  width?: string;
  isPressed?: boolean;
  isNext?: boolean;
  isSpecial?: boolean;
  fingerIndex?: number; // 1-8 for fingers (1=left pinky, 4=left index, 5=right index, 8=right pinky)
}
// Finger mapping for each key
const FINGER_MAP: Record<string, number> = {
  '`': 1, '1': 1, '2': 2, '3': 3, '4': 4, '5': 4, '6': 5, '7': 5, '8': 6, '9': 7, '0': 8, '-': 8, '=': 8,
  'q': 1, 'w': 2, 'e': 3, 'r': 4, 't': 4, 'y': 5, 'u': 5, 'i': 6, 'o': 7, 'p': 8, '[': 8, ']': 8, '\\': 8,
  'a': 1, 's': 2, 'd': 3, 'f': 4, 'g': 4, 'h': 5, 'j': 5, 'k': 6, 'l': 7, ';': 8, "'": 8,
  'z': 1, 'x': 2, 'c': 3, 'v': 4, 'b': 4, 'n': 5, 'm': 5, ',': 6, '.': 7, '/': 8
};
const FINGER_COLORS = {
  1: 'rgb(255, 100, 100)', // Left pinky - Red
  2: 'rgb(255, 150, 50)',  // Left ring - Orange
  3: 'rgb(255, 200, 0)',   // Left middle - Yellow
  4: 'rgb(100, 255, 100)', // Left index - Green
  5: 'rgb(100, 200, 255)', // Right index - Light Blue
  6: 'rgb(150, 150, 255)', // Right middle - Blue
  7: 'rgb(200, 100, 255)', // Right ring - Purple
  8: 'rgb(255, 100, 255)'  // Right pinky - Pink
};
const Key = ({ label, width = "w-12", isPressed = false, isNext = false, isSpecial = false }: KeyProps) => {
  const fingerIndex = FINGER_MAP[label.toLowerCase()];
  const fingerColor = fingerIndex ? FINGER_COLORS[fingerIndex] : undefined;
  return (
    <div
      role="button"
      tabIndex={0}
      aria-pressed={isPressed}
      aria-label={`${label} key ${isNext ? '- next key to press' : ''} ${fingerIndex ? `- use ${fingerIndex <= 4 ? 'left' : 'right'} ${['pinky', 'ring', 'middle', 'index'][Math.min(3, (fingerIndex - 1) % 4)]} finger` : ''}`}
      className={cn(
        "relative h-12",
        width,
        "rounded-lg border border-border/50",
        "flex items-center justify-center",
        "transition-all duration-300",
        isPressed
          ? "bg-primary/20 text-primary border-primary shadow-[0_0_10px_rgba(250,250,250,0.2)]"
          : isNext
            ? "bg-green-500/10 text-green-400 border-green-500 shadow-[0_0_10px_rgba(0,255,0,0.1)] animate-[pulse_3s_ease-in-out_infinite]"
            : "bg-black/50 text-secondary hover:bg-black/70",
        isSpecial ? "text-xs" : "text-sm",
        "backdrop-blur-sm",
        "focus:ring-2 focus:ring-primary focus:outline-none",
        "motion-safe:transition-all"
      )}
      style={{
        borderTopColor: fingerColor,
        borderTopWidth: fingerIndex ? '3px' : '1px'
      }}
      data-key={label.toLowerCase()}
      data-finger={fingerIndex}
      onKeyDown={(e) => {
        if (e.key === 'Enter' || e.key === ' ') {
          console.info(`Key ${label} activated via keyboard`);
        }
      }}
    >
      {label}
      {isNext && (
        <span className="sr-only">
          (Next key to press)
        </span>
      )}
      {fingerIndex && (
        <div
          className="absolute -top-4 left-1/2 transform -translate-x-1/2 text-[10px] opacity-50"
          style={{ color: fingerColor }}
        >
          {fingerIndex <= 4 ? 'L' : 'R'}{(fingerIndex - 1) % 4 + 1}
        </div>
      )}
    </div>
  );
};
interface VisualKeyboardProps {
  pressedKey: string | null;
  nextKey?: string | null;
}
const VisualKeyboard: React.FC<VisualKeyboardProps> = ({ pressedKey, nextKey }) => {
  const isKeyPressed = (key: string) => pressedKey?.toLowerCase() === key.toLowerCase();
  const isNextKey = (key: string) => nextKey?.toLowerCase() === key.toLowerCase();
  return (
    <div className="w-full max-w-6xl mx-auto p-4 bg-black/30 rounded-xl backdrop-blur-sm border border-border/50">
      <div className="grid gap-1">
        {/* Function Key Row */}
        <div className="flex gap-1 mb-4">
          <Key label="esc" width="w-12" isPressed={isKeyPressed("Escape")} isNext={isNextKey("Escape")} isSpecial />
          <div className="w-14" /> {/* Spacing */}
          <Key label="F1" width="w-10" isPressed={isKeyPressed("F1")} isNext={isNextKey("F1")} isSpecial />
          <Key label="F2" width="w-10" isPressed={isKeyPressed("F2")} isNext={isNextKey("F2")} isSpecial />
          <Key label="F3" width="w-10" isPressed={isKeyPressed("F3")} isNext={isNextKey("F3")} isSpecial />
          <Key label="F4" width="w-10" isPressed={isKeyPressed("F4")} isNext={isNextKey("F4")} isSpecial />
          <div className="w-7" /> {/* Spacing */}
          <Key label="F5" width="w-10" isPressed={isKeyPressed("F5")} isNext={isNextKey("F5")} isSpecial />
          <Key label="F6" width="w-10" isPressed={isKeyPressed("F6")} isNext={isNextKey("F6")} isSpecial />
          <Key label="F7" width="w-10" isPressed={isKeyPressed("F7")} isNext={isNextKey("F7")} isSpecial />
          <Key label="F8" width="w-10" isPressed={isKeyPressed("F8")} isNext={isNextKey("F8")} isSpecial />
          <div className="w-7" /> {/* Spacing */}
          <Key label="F9" width="w-10" isPressed={isKeyPressed("F9")} isNext={isNextKey("F9")} isSpecial />
          <Key label="F10" width="w-10" isPressed={isKeyPressed("F10")} isNext={isNextKey("F10")} isSpecial />
          <Key label="F11" width="w-10" isPressed={isKeyPressed("F11")} isNext={isNextKey("F11")} isSpecial />
          <Key label="F12" width="w-10" isPressed={isKeyPressed("F12")} isNext={isNextKey("F12")} isSpecial />
        </div>
        {/* Number Row */}
        <div className="flex gap-1">
          <Key label="`" isPressed={isKeyPressed("`")} isNext={isNextKey("`")} />
          <Key label="1" isPressed={isKeyPressed("1")} isNext={isNextKey("1")} />
          <Key label="2" isPressed={isKeyPressed("2")} isNext={isNextKey("2")} />
          <Key label="3" isPressed={isKeyPressed("3")} isNext={isNextKey("3")} />
          <Key label="4" isPressed={isKeyPressed("4")} isNext={isNextKey("4")} />
          <Key label="5" isPressed={isKeyPressed("5")} isNext={isNextKey("5")} />
          <Key label="6" isPressed={isKeyPressed("6")} isNext={isNextKey("6")} />
          <Key label="7" isPressed={isKeyPressed("7")} isNext={isNextKey("7")} />
          <Key label="8" isPressed={isKeyPressed("8")} isNext={isNextKey("8")} />
          <Key label="9" isPressed={isKeyPressed("9")} isNext={isNextKey("9")} />
          <Key label="0" isPressed={isKeyPressed("0")} isNext={isNextKey("0")} />
          <Key label="-" isPressed={isKeyPressed("-")} isNext={isNextKey("-")} />
          <Key label="=" isPressed={isKeyPressed("=")} isNext={isNextKey("=")} />
          <Key label="delete" width="w-16" isPressed={isKeyPressed("Backspace")} isNext={isNextKey("Backspace")} isSpecial />
        </div>
        {/* QWERTY Row */}
        <div className="flex gap-1">
          <Key label="tab" width="w-16" isPressed={isKeyPressed("Tab")} isNext={isNextKey("Tab")} isSpecial />
          <Key label="Q" isPressed={isKeyPressed("q")} isNext={isNextKey("q")} />
          <Key label="W" isPressed={isKeyPressed("w")} isNext={isNextKey("w")} />
          <Key label="E" isPressed={isKeyPressed("e")} isNext={isNextKey("e")} />
          <Key label="R" isPressed={isKeyPressed("r")} isNext={isNextKey("r")} />
          <Key label="T" isPressed={isKeyPressed("t")} isNext={isNextKey("t")} />
          <Key label="Y" isPressed={isKeyPressed("y")} isNext={isNextKey("y")} />
          <Key label="U" isPressed={isKeyPressed("u")} isNext={isNextKey("u")} />
          <Key label="I" isPressed={isKeyPressed("i")} isNext={isNextKey("i")} />
          <Key label="O" isPressed={isKeyPressed("o")} isNext={isNextKey("o")} />
          <Key label="P" isPressed={isKeyPressed("p")} isNext={isNextKey("p")} />
          <Key label="[" isPressed={isKeyPressed("[")} isNext={isNextKey("[")} />
          <Key label="]" isPressed={isKeyPressed("]")} isNext={isNextKey("]")} />
          <Key label="\\" isPressed={isKeyPressed("\\")} isNext={isNextKey("\\")} />
        </div>
        {/* ASDF Row */}
        <div className="flex gap-1">
          <Key label="caps lock" width="w-20" isPressed={isKeyPressed("CapsLock")} isNext={isNextKey("CapsLock")} isSpecial />
          <Key label="A" isPressed={isKeyPressed("a")} isNext={isNextKey("a")} />
          <Key label="S" isPressed={isKeyPressed("s")} isNext={isNextKey("s")} />
          <Key label="D" isPressed={isKeyPressed("d")} isNext={isNextKey("d")} />
          <Key label="F" isPressed={isKeyPressed("f")} isNext={isNextKey("f")} />
          <Key label="G" isPressed={isKeyPressed("g")} isNext={isNextKey("g")} />
          <Key label="H" isPressed={isKeyPressed("h")} isNext={isNextKey("h")} />
          <Key label="J" isPressed={isKeyPressed("j")} isNext={isNextKey("j")} />
          <Key label="K" isPressed={isKeyPressed("k")} isNext={isNextKey("k")} />
          <Key label="L" isPressed={isKeyPressed("l")} isNext={isNextKey("l")} />
          <Key label=";" isPressed={isKeyPressed(";")} isNext={isNextKey(";")} />
          <Key label="'" isPressed={isKeyPressed("'")} isNext={isNextKey("'")} />
          <Key label="return" width="w-20" isPressed={isKeyPressed("Enter")} isNext={isNextKey("Enter")} isSpecial />
        </div>
        {/* ZXCV Row */}
        <div className="flex gap-1">
          <Key label="shift" width="w-28" isPressed={isKeyPressed("Shift")} isNext={isNextKey("Shift")} isSpecial />
          <Key label="Z" isPressed={isKeyPressed("z")} isNext={isNextKey("z")} />
          <Key label="X" isPressed={isKeyPressed("x")} isNext={isNextKey("x")} />
          <Key label="C" isPressed={isKeyPressed("c")} isNext={isNextKey("c")} />
          <Key label="V" isPressed={isKeyPressed("v")} isNext={isNextKey("v")} />
          <Key label="B" isPressed={isKeyPressed("b")} isNext={isNextKey("b")} />
          <Key label="N" isPressed={isKeyPressed("n")} isNext={isNextKey("n")} />
          <Key label="M" isPressed={isKeyPressed("m")} isNext={isNextKey("m")} />
          <Key label="," isPressed={isKeyPressed(",")} isNext={isNextKey(",")} />
          <Key label="." isPressed={isKeyPressed(".")} isNext={isNextKey(".")} />
          <Key label="/" isPressed={isKeyPressed("/")} isNext={isNextKey("/")} />
          <Key label="shift" width="w-28" isPressed={isKeyPressed("Shift")} isNext={isNextKey("Shift")} isSpecial />
        </div>
        {/* Space Row */}
        <div className="flex gap-1">
          <Key label="fn" width="w-12" isSpecial />
          <Key label="control" width="w-14" isPressed={isKeyPressed("Control")} isNext={isNextKey("Control")} isSpecial />
          <Key label="option" width="w-14" isPressed={isKeyPressed("Alt")} isNext={isNextKey("Alt")} isSpecial />
          <Key label="command" width="w-16" isPressed={isKeyPressed("Meta")} isNext={isNextKey("Meta")} isSpecial />
          <Key label="" width="w-64" isPressed={isKeyPressed(" ")} isNext={isNextKey(" ")} /> {/* Spacebar */}
          <Key label="command" width="w-16" isPressed={isKeyPressed("Meta")} isNext={isNextKey("Meta")} isSpecial />
          <Key label="option" width="w-14" isPressed={isKeyPressed("Alt")} isNext={isNextKey("Alt")} isSpecial />
          <div className="flex gap-1">
            <Key label="←" width="w-10" isPressed={isKeyPressed("ArrowLeft")} isNext={isNextKey("ArrowLeft")} />
            <div className="flex flex-col gap-1">
              <Key label="↑" width="w-10" isPressed={isKeyPressed("ArrowUp")} isNext={isNextKey("ArrowUp")} />
              <Key label="↓" width="w-10" isPressed={isKeyPressed("ArrowDown")} isNext={isNextKey("ArrowDown")} />
            </div>
            <Key label="→" width="w-10" isPressed={isKeyPressed("ArrowRight")} isNext={isNextKey("ArrowRight")} />
          </div>
        </div>
      </div>
    </div>
  );
};
export default VisualKeyboard;
</file>

<file path="src/config/ai.config.ts">
/*
 *  █████╗ ██╗     ██████╗ ██████╗ ███╗   ██╗███████╗██╗ ██████╗
 * ██╔══██╗██║    ██╔════╝██╔═══██╗████╗  ██║██╔════╝██║██╔════╝
 * ███████║██║    ██║     ██║   ██║██╔██╗ ██║█████╗  ██║██║  ███╗
 * ██╔══██║██║    ██║     ██║   ██║██║╚██╗██║██╔══╝  ██║██║   ██║
 * ██║  ██║██║    ╚██████╗╚██████╔╝██║ ╚████║██║     ██║╚██████╔╝
 * ╚═╝  ╚═╝╚═╝     ╚═════╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝     ╚═╝ ╚═════╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Config: AI Service Configuration                                                               ║
 * ║ Description: Configuration settings for AI-powered typing tutor                                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { AIAdapterConfig } from '../adapters/base.adapter';
/**
 * Default configuration for AI services
 */
export const DEFAULT_AI_CONFIG: AIAdapterConfig = {
  // API Configuration
  apiKey: process.env.OPENAI_API_KEY,
  endpoint: 'https://api.openai.com/v1/chat/completions',
  modelName: 'gpt-4-turbo-preview',
  temperature: 0.7,
  maxTokens: 1000,
  requestTimeout: 30000, // 30 seconds
  // Retry Configuration
  retryConfig: {
    maxRetries: 3,
    backoffFactor: 1.5,
    initialDelay: 1000, // 1 second
  },
  // Cache Configuration
  cacheConfig: {
    enabled: true,
    ttl: 3600, // 1 hour
    maxSize: 1000, // number of items
  },
};
/**
 * Lesson generation parameters
 */
export const LESSON_CONFIG = {
  minLength: 100, // characters
  maxLength: 500, // characters
  difficultyLevels: {
    beginner: {
      maxWordLength: 5,
      commonWordsOnly: true,
      includeNumbers: false,
      includeSpecialChars: false,
    },
    intermediate: {
      maxWordLength: 8,
      commonWordsOnly: false,
      includeNumbers: true,
      includeSpecialChars: false,
    },
    advanced: {
      maxWordLength: 12,
      commonWordsOnly: false,
      includeNumbers: true,
      includeSpecialChars: true,
    },
  },
  contentTypes: {
    code: {
      languages: ['typescript', 'javascript', 'python', 'java'],
      includeComments: true,
      maxLineLength: 80,
    },
    text: {
      categories: ['general', 'business', 'technical', 'creative'],
      formatOptions: ['sentences', 'paragraphs', 'dialogue'],
    },
  },
};
/**
 * Performance analysis thresholds
 */
export const ANALYSIS_CONFIG = {
  accuracy: {
    excellent: 98,
    good: 95,
    needsImprovement: 90,
  },
  wpm: {
    beginner: {
      target: 30,
      minimum: 20,
    },
    intermediate: {
      target: 50,
      minimum: 40,
    },
    advanced: {
      target: 80,
      minimum: 60,
    },
    professional: {
      target: 100,
      minimum: 80,
    },
  },
  errorPatterns: {
    significantThreshold: 3, // occurrences
    timeWindow: 300000, // 5 minutes
  },
};
/**
 * Feedback generation settings
 */
export const FEEDBACK_CONFIG = {
  realTime: {
    enabled: true,
    minInterval: 1000, // 1 second
    maxSuggestions: 3,
  },
  sessionEnd: {
    detailedAnalysis: true,
    includeVisualizations: true,
    recommendationCount: 5,
  },
  adaptiveThresholds: {
    enabled: true,
    adjustmentFactor: 0.1,
    minThreshold: 0.5,
    maxThreshold: 1.5,
  },
};
/**
 * AI model prompts and templates
 */
export const AI_PROMPTS = {
  lessonGeneration: {
    system: `You are an expert typing tutor, specialized in creating personalized typing lessons.
Focus on the user's skill level, learning style, and specific areas for improvement.`,
    template: `Create a typing lesson for a {level} user focusing on {focusAreas}.
The lesson should be {contentType} with {difficulty} difficulty.
Include specific exercises for {weaknesses}.`,
  },
  feedback: {
    system: `You are a supportive typing coach, providing constructive feedback to help users improve.
Balance encouragement with specific, actionable suggestions.`,
    template: `Based on the recent performance:
- Accuracy: {accuracy}%
- Speed: {wpm} WPM
- Error Patterns: {errorPatterns}
Provide personalized feedback and recommendations.`,
  },
  analysis: {
    system: `You are an analytical typing assessment tool, identifying patterns and trends in typing behavior.
Focus on data-driven insights and practical improvement strategies.`,
    template: `Analyze the following typing session data:
- Duration: {duration}
- Key Metrics: {metrics}
- Error Distribution: {errorDistribution}
Provide a detailed analysis with actionable insights.`,
  },
};
/**
 * Environment-specific configurations
 */
export const ENV_CONFIG = {
  development: {
    ...DEFAULT_AI_CONFIG,
    cacheConfig: {
      enabled: true,
      ttl: 300, // 5 minutes
      maxSize: 100,
    },
    logging: {
      level: 'debug',
      detailed: true,
    },
  },
  production: {
    ...DEFAULT_AI_CONFIG,
    temperature: 0.5, // More deterministic
    cacheConfig: {
      enabled: true,
      ttl: 3600, // 1 hour
      maxSize: 1000,
    },
    logging: {
      level: 'error',
      detailed: false,
    },
  },
  test: {
    ...DEFAULT_AI_CONFIG,
    apiKey: 'test-key',
    endpoint: 'http://localhost:3000/mock-ai',
    cacheConfig: {
      enabled: false,
    },
    logging: {
      level: 'debug',
      detailed: true,
    },
  },
};
</file>

<file path="src/contexts/theme-context.tsx">
import React, { createContext, useContext, useEffect, useState } from 'react';
type Theme = 'light' | 'dark';
interface ThemeContextType {
  theme: Theme;
  setTheme: (theme: Theme) => void;
}
const ThemeContext = createContext<ThemeContextType | undefined>(undefined);
export function ThemeProvider({ children }: { children: React.ReactNode }) {
  const [theme, setTheme] = useState<Theme>(() => {
    // Check local storage or system preference
    if (typeof window !== 'undefined') {
      const stored = localStorage.getItem('theme') as Theme;
      if (stored) return stored;
      if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        return 'dark';
      }
    }
    return 'light';
  });
  useEffect(() => {
    const root = window.document.documentElement;
    root.classList.remove('light', 'dark');
    root.classList.add(theme);
    localStorage.setItem('theme', theme);
  }, [theme]);
  return (
    <ThemeContext.Provider value={{ theme, setTheme }}>
      {children}
    </ThemeContext.Provider>
  );
}
export function useTheme() {
  const context = useContext(ThemeContext);
  if (context === undefined) {
    throw new Error('useTheme must be used within a ThemeProvider');
  }
  return context;
}
</file>

<file path="src/hooks/use-focus-management.ts">
import { useCallback, useEffect, useRef } from "react";
interface FocusOptions {
  highlightDuration?: number;
  pulseEffect?: boolean;
  soundFeedback?: boolean;
}
export function useFocusManagement(options: FocusOptions = {}) {
  const {
    highlightDuration = 1000,
    pulseEffect = true,
    soundFeedback = true,
  } = options;
  const audioRef = useRef<HTMLAudioElement | null>(null);
  useEffect(() => {
    if (soundFeedback) {
      audioRef.current = new Audio("/sounds/focus-change.mp3");
      audioRef.current.volume = 0.2;
    }
    return () => {
      if (audioRef.current) {
        audioRef.current = null;
      }
    };
  }, [soundFeedback]);
  const handleFocus = useCallback(
    (element: HTMLElement) => {
      // Add subtle highlight effect
      element.style.transition = "box-shadow 0.2s ease";
      element.style.boxShadow = "0 0 0 2px hsl(var(--ring))";
      if (pulseEffect) {
        element.classList.add("pulse-focus");
      }
      if (soundFeedback && audioRef.current) {
        audioRef.current.play().catch(() => {
          // Silently handle autoplay restrictions
        });
      }
      // Remove highlight after duration
      setTimeout(() => {
        element.style.boxShadow = "";
        if (pulseEffect) {
          element.classList.remove("pulse-focus");
        }
      }, highlightDuration);
    },
    [highlightDuration, pulseEffect, soundFeedback]
  );
  const setFocus = useCallback(
    (elementOrSelector: HTMLElement | string) => {
      const element =
        typeof elementOrSelector === "string"
          ? (document.querySelector(elementOrSelector) as HTMLElement)
          : elementOrSelector;
      if (element) {
        element.focus();
        handleFocus(element);
      }
    },
    [handleFocus]
  );
  return {
    setFocus,
    handleFocus,
  };
}
// Add this to your globals.css:
/*
.pulse-focus {
  animation: pulse 1s cubic-bezier(0.4, 0, 0.6, 1);
}
@keyframes pulse {
  0%, 100% {
    opacity: 1;
  }
  50% {
    opacity: .7;
  }
}
*/
</file>

<file path="src/hooks/use-haptic-feedback.ts">
/*
 * ██╗  ██╗ █████╗ ██████╗ ████████╗██╗ ██████╗███████╗
 * ██║  ██║██╔══██╗██╔══██╗╚══██╔══╝██║██╔════╝██╔════╝
 * ███████║███████║██████╔╝   ██║   ██║██║     ███████╗
 * ██╔══██║██╔══██║██╔═══╝    ██║   ██║██║     ╚════██║
 * ██║  ██║██║  ██║██║        ██║   ██║╚██████╗███████║
 * ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝        ╚═╝   ╚═╝ ╚═════╝╚══════╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Hook: Haptic Feedback                                                                          ║
 * ║ Description: Custom hook for managing haptic feedback in the typing tutor                      ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useCallback, useEffect, useRef, useState } from 'react';
import { HapticPattern } from '@/types/typing';
interface HapticConfig {
  enabled: boolean;
  intensity: number;
  customPatterns?: Record<string, HapticPattern>;
}
/**
 * Default haptic patterns
 */
const DEFAULT_PATTERNS: Record<string, HapticPattern> = {
  keyPress: {
    intensity: 0.5,
    duration: 50,
    pattern: [1],
    description: 'Standard key press feedback'
  },
  error: {
    intensity: 1.0,
    duration: 100,
    pattern: [1, 0, 1, 0, 1],
    description: 'Error feedback pattern'
  },
  success: {
    intensity: 0.7,
    duration: 150,
    pattern: [1, 0, 1],
    description: 'Success feedback pattern'
  },
  warning: {
    intensity: 0.8,
    duration: 75,
    pattern: [1, 0, 1],
    description: 'Warning feedback pattern'
  }
};
/**
 * Default configuration
 */
const DEFAULT_CONFIG: HapticConfig = {
  enabled: true,
  intensity: 1.0
};
/**
 * Custom hook for managing haptic feedback
 */
export function useHapticFeedback(config: Partial<HapticConfig> = {}) {
  // Merge default config with provided config
  const hapticConfig = { ...DEFAULT_CONFIG, ...config };
  // State for tracking support and patterns
  const [isSupported, setIsSupported] = useState(false);
  const [isReady, setIsReady] = useState(false);
  // Refs for tracking state and timing
  const lastTriggerTime = useRef<number>(0);
  const patterns = useRef<Record<string, HapticPattern>>({
    ...DEFAULT_PATTERNS,
    ...hapticConfig.customPatterns
  });
  /**
   * Initialize haptic feedback system
   */
  useEffect(() => {
    checkHapticSupport().then(supported => {
      setIsSupported(supported);
      setIsReady(true);
      if (supported) {
        console.log('Haptic feedback is supported');
      } else {
        console.warn('Haptic feedback is not supported on this device');
      }
    });
  }, []);
  /**
   * Check if haptic feedback is supported
   */
  const checkHapticSupport = async (): Promise<boolean> => {
    // Check for vibration API support
    if ('vibrate' in navigator) {
      try {
        // Test vibration
        await navigator.vibrate(0);
        return true;
      } catch (error) {
        console.error('Error testing vibration:', error);
        return false;
      }
    }
    return false;
  };
  /**
   * Trigger haptic feedback with a specific pattern
   */
  const triggerHapticFeedback = useCallback(async (
    pattern: HapticPattern,
    options: { force?: boolean; minInterval?: number } = {}
  ) => {
    if (!hapticConfig.enabled || !isSupported) return;
    const now = Date.now();
    const minInterval = options.minInterval || 50; // Default minimum interval
    // Check if enough time has passed since last trigger
    if (!options.force && now - lastTriggerTime.current < minInterval) {
      return;
    }
    try {
      // Apply intensity scaling
      const scaledPattern = pattern.pattern.map(value =>
        value === 0 ? 0 : Math.round(value * pattern.duration * hapticConfig.intensity)
      );
      // Trigger vibration
      await navigator.vibrate(scaledPattern);
      lastTriggerTime.current = now;
      console.log('Triggered haptic feedback:', {
        pattern: pattern.description,
        duration: pattern.duration,
        intensity: hapticConfig.intensity
      });
    } catch (error) {
      console.error('Error triggering haptic feedback:', error);
    }
  }, [hapticConfig.enabled, hapticConfig.intensity, isSupported]);
  /**
   * Add a custom haptic pattern
   */
  const addPattern = useCallback((
    id: string,
    pattern: HapticPattern
  ) => {
    patterns.current[id] = pattern;
    console.log(`Added custom haptic pattern: ${id}`, pattern);
  }, []);
  /**
   * Remove a custom haptic pattern
   */
  const removePattern = useCallback((id: string) => {
    if (id in DEFAULT_PATTERNS) {
      console.warn(`Cannot remove default pattern: ${id}`);
      return;
    }
    delete patterns.current[id];
    console.log(`Removed custom haptic pattern: ${id}`);
  }, []);
  /**
   * Get a specific haptic pattern
   */
  const getPattern = useCallback((id: string): HapticPattern | undefined => {
    return patterns.current[id];
  }, []);
  /**
   * Trigger a predefined haptic pattern
   */
  const triggerPattern = useCallback(async (
    patternId: string,
    options?: { force?: boolean; minInterval?: number }
  ) => {
    const pattern = getPattern(patternId);
    if (pattern) {
      await triggerHapticFeedback(pattern, options);
    } else {
      console.warn(`Haptic pattern not found: ${patternId}`);
    }
  }, [triggerHapticFeedback, getPattern]);
  /**
   * Create a composite haptic pattern from multiple patterns
   */
  const createCompositePattern = useCallback((
    patternIds: string[],
    spacing: number = 100
  ): HapticPattern => {
    const compositePattern: number[] = [];
    let totalDuration = 0;
    let description = 'Composite pattern: ';
    patternIds.forEach((id, index) => {
      const pattern = getPattern(id);
      if (pattern) {
        // Add spacing if not first pattern
        if (index > 0) {
          compositePattern.push(0);
          totalDuration += spacing;
        }
        // Add pattern
        compositePattern.push(...pattern.pattern);
        totalDuration += pattern.duration;
        description += `${pattern.description}, `;
      }
    });
    return {
      intensity: Math.max(...patternIds.map(id => getPattern(id)?.intensity || 0)),
      duration: totalDuration,
      pattern: compositePattern,
      description: description.slice(0, -2)
    };
  }, [getPattern]);
  return {
    isSupported,
    isReady,
    triggerHapticFeedback,
    triggerPattern,
    addPattern,
    removePattern,
    getPattern,
    createCompositePattern,
    patterns: patterns.current
  };
}
</file>

<file path="src/hooks/use-keyboard-layout.ts">
/*
 * ██╗  ██╗███████╗██╗   ██╗██████╗  ██████╗  █████╗ ██████╗ ██████╗
 * ██║ ██╔╝██╔════╝╚██╗ ██╔╝██╔══██╗██╔═══██╗██╔══██╗██╔══██╗██╔══██╗
 * █████╔╝ █████╗   ╚████╔╝ ██████╔╝██║   ██║███████║██████╔╝██║  ██║
 * ██╔═██╗ ██╔══╝    ╚██╔╝  ██╔══██╗██║   ██║██╔══██║██╔══██╗██║  ██║
 * ██║  ██╗███████╗   ██║   ██████╔╝╚██████╔╝██║  ██║██║  ██║██████╔╝
 * ╚═╝  ╚═╝╚══════╝   ╚═╝   ╚═════╝  ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═════╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Hook: Keyboard Layout                                                                          ║
 * ║ Description: Custom hook for managing keyboard layouts and key metadata                        ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useMemo } from 'react';
import { KeyMetadata, FingerPosition } from '@/types/typing';
type KeyboardLayout = 'qwerty' | 'dvorak' | 'colemak';
/**
 * Keyboard layout data
 */
const LAYOUTS: Record<KeyboardLayout, string[][]> = {
  qwerty: [
    ['q', 'w', 'e', 'r', 't', 'y', 'u', 'i', 'o', 'p'],
    ['a', 's', 'd', 'f', 'g', 'h', 'j', 'k', 'l', ';'],
    ['z', 'x', 'c', 'v', 'b', 'n', 'm', ',', '.', '/']
  ],
  dvorak: [
    ['\'', ',', '.', 'p', 'y', 'f', 'g', 'c', 'r', 'l'],
    ['a', 'o', 'e', 'u', 'i', 'd', 'h', 't', 'n', 's'],
    [';', 'q', 'j', 'k', 'x', 'b', 'm', 'w', 'v', 'z']
  ],
  colemak: [
    ['q', 'w', 'f', 'p', 'g', 'j', 'l', 'u', 'y', ';'],
    ['a', 'r', 's', 't', 'd', 'h', 'n', 'e', 'i', 'o'],
    ['z', 'x', 'c', 'v', 'b', 'k', 'm', ',', '.', '/']
  ]
};
/**
 * Finger position mappings for each key
 */
const FINGER_POSITIONS: Record<string, FingerPosition> = {
  // Left hand
  'q': 'left-pinky',
  'a': 'left-pinky',
  'z': 'left-pinky',
  'w': 'left-ring',
  's': 'left-ring',
  'x': 'left-ring',
  'e': 'left-middle',
  'd': 'left-middle',
  'c': 'left-middle',
  'r': 'left-index',
  'f': 'left-index',
  'v': 'left-index',
  't': 'left-index',
  'g': 'left-index',
  'b': 'left-index',
  // Right hand
  'y': 'right-index',
  'h': 'right-index',
  'n': 'right-index',
  'u': 'right-index',
  'j': 'right-index',
  'm': 'right-index',
  'i': 'right-middle',
  'k': 'right-middle',
  ',': 'right-middle',
  'o': 'right-ring',
  'l': 'right-ring',
  '.': 'right-ring',
  'p': 'right-pinky',
  ';': 'right-pinky',
  '/': 'right-pinky'
};
/**
 * Sound profiles for different key types
 */
const SOUND_PROFILES: Record<string, string> = {
  default: 'key-press-1',
  space: 'key-press-2',
  enter: 'key-press-3',
  backspace: 'key-press-4',
  shift: 'key-press-5',
  error: 'key-error'
};
/**
 * Haptic patterns for different key types
 */
const HAPTIC_PATTERNS: Record<string, { intensity: number; duration: number; pattern: number[] }> = {
  default: {
    intensity: 0.5,
    duration: 50,
    pattern: [1]
  },
  space: {
    intensity: 0.7,
    duration: 70,
    pattern: [1, 0, 1]
  },
  error: {
    intensity: 1.0,
    duration: 100,
    pattern: [1, 0, 1, 0, 1]
  }
};
/**
 * Letter frequency data for English text
 */
const LETTER_FREQUENCIES: Record<string, number> = {
  'e': 12.7, 't': 9.1, 'a': 8.2, 'o': 7.5, 'i': 7.0,
  'n': 6.7, 's': 6.3, 'h': 6.1, 'r': 6.0, 'd': 4.3,
  'l': 4.0, 'c': 2.8, 'u': 2.8, 'm': 2.4, 'w': 2.4,
  'f': 2.2, 'g': 2.0, 'y': 2.0, 'p': 1.9, 'b': 1.5,
  'v': 1.0, 'k': 0.8, 'j': 0.15, 'x': 0.15, 'q': 0.10,
  'z': 0.07, ',': 1.0, '.': 1.0, ';': 0.5, '/': 0.3
};
/**
 * Custom hook for managing keyboard layouts
 */
export function useKeyboardLayout(layout: KeyboardLayout = 'qwerty') {
  /**
   * Get metadata for a specific key
   */
  const getKeyMetadata = (key: string): KeyMetadata | null => {
    const normalizedKey = key.toLowerCase();
    if (!FINGER_POSITIONS[normalizedKey]) {
      console.warn(`No finger position mapping found for key: ${key}`);
      return null;
    }
    return {
      character: normalizedKey,
      finger: FINGER_POSITIONS[normalizedKey],
      frequencyScore: LETTER_FREQUENCIES[normalizedKey] || 0,
      soundProfile: SOUND_PROFILES[normalizedKey] || SOUND_PROFILES.default,
      hapticPattern: {
        ...HAPTIC_PATTERNS.default,
        description: `Haptic feedback for ${normalizedKey} key`
      }
    };
  };
  /**
   * Get all keys for the current layout
   */
  const getAllKeys = useMemo(() => {
    return LAYOUTS[layout].flat();
  }, [layout]);
  /**
   * Get neighboring keys for a given key
   */
  const getNeighboringKeys = (key: string): string[] => {
    const neighbors: string[] = [];
    const keyboardRows = LAYOUTS[layout];
    for (let rowIndex = 0; rowIndex < keyboardRows.length; rowIndex++) {
      const row = keyboardRows[rowIndex];
      const keyIndex = row.indexOf(key.toLowerCase());
      if (keyIndex !== -1) {
        // Add horizontal neighbors
        if (keyIndex > 0) neighbors.push(row[keyIndex - 1]);
        if (keyIndex < row.length - 1) neighbors.push(row[keyIndex + 1]);
        // Add vertical neighbors
        if (rowIndex > 0) neighbors.push(keyboardRows[rowIndex - 1][keyIndex]);
        if (rowIndex < keyboardRows.length - 1) neighbors.push(keyboardRows[rowIndex + 1][keyIndex]);
      }
    }
    return neighbors;
  };
  /**
   * Get keys assigned to a specific finger
   */
  const getKeysForFinger = (finger: FingerPosition): string[] => {
    return Object.entries(FINGER_POSITIONS)
      .filter(([_, pos]) => pos === finger)
      .map(([key]) => key);
  };
  /**
   * Calculate difficulty score for a key transition
   */
  const getTransitionDifficulty = (fromKey: string, toKey: string): number => {
    const from = getKeyMetadata(fromKey);
    const to = getKeyMetadata(toKey);
    if (!from || !to) return 1;
    // Base difficulty
    let difficulty = 1;
    // Increase difficulty for same finger
    if (from.finger === to.finger) {
      difficulty *= 2;
    }
    // Increase difficulty for pinky fingers
    if (from.finger.includes('pinky') || to.finger.includes('pinky')) {
      difficulty *= 1.5;
    }
    // Decrease difficulty for alternating hands
    if (from.finger.startsWith('left') !== to.finger.startsWith('left')) {
      difficulty *= 0.8;
    }
    return difficulty;
  };
  return {
    keyboardData: LAYOUTS[layout],
    getKeyMetadata,
    getAllKeys,
    getNeighboringKeys,
    getKeysForFinger,
    getTransitionDifficulty,
    fingerPositions: FINGER_POSITIONS,
    soundProfiles: SOUND_PROFILES,
    hapticPatterns: HAPTIC_PATTERNS,
    letterFrequencies: LETTER_FREQUENCIES
  };
}
</file>

<file path="src/hooks/use-mobile.tsx">
import * as React from "react"
const MOBILE_BREAKPOINT = 768
export function useIsMobile() {
  const [isMobile, setIsMobile] = React.useState<boolean | undefined>(undefined)
  React.useEffect(() => {
    const mql = window.matchMedia(`(max-width: ${MOBILE_BREAKPOINT - 1}px)`)
    const onChange = () => {
      setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)
    }
    mql.addEventListener("change", onChange)
    setIsMobile(window.innerWidth < MOBILE_BREAKPOINT)
    return () => mql.removeEventListener("change", onChange)
  }, [])
  return !!isMobile
}
</file>

<file path="src/hooks/use-screen-reader.ts">
/*
 * ███████╗ ██████╗██████╗ ███████╗███████╗███╗   ██╗
 * ██╔════╝██╔════╝██╔══██╗██╔════╝██╔════╝████╗  ██║
 * ███████╗██║     ██████╔╝█████╗  █████╗  ██╔██╗ ██║
 * ╚════██║██║     ██╔══██╗██╔══╝  ██╔══╝  ██║╚██╗██║
 * ███████║╚██████╗██║  ██║███████╗███████╗██║ ╚████║
 * ╚══════╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚══════╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Hook: Screen Reader                                                                            ║
 * ║ Description: Custom hook for managing screen reader announcements                              ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useCallback, useEffect, useRef, useState } from 'react';
interface ScreenReaderConfig {
  enabled: boolean;
  volume: number;
  rate: number;
  pitch: number;
  voice?: string;
  language?: string;
}
interface AnnouncementOptions {
  priority?: 'high' | 'medium' | 'low';
  interrupt?: boolean;
  delay?: number;
}
/**
 * Default configuration
 */
const DEFAULT_CONFIG: ScreenReaderConfig = {
  enabled: true,
  volume: 1.0,
  rate: 1.0,
  pitch: 1.0,
  language: 'en-US'
};
/**
 * Custom hook for managing screen reader announcements
 */
export function useScreenReader(config: Partial<ScreenReaderConfig> = {}) {
  // Merge default config with provided config
  const screenReaderConfig = { ...DEFAULT_CONFIG, ...config };
  // State for tracking support and status
  const [isSupported, setIsSupported] = useState(false);
  const [isReady, setIsReady] = useState(false);
  const [selectedVoice, setSelectedVoice] = useState<SpeechSynthesisVoice | null>(null);
  // Refs for managing announcements
  const synthesis = useRef<SpeechSynthesis | null>(null);
  const announcementQueue = useRef<Array<{
    text: string;
    options: AnnouncementOptions;
  }>>([]);
  const isProcessingQueue = useRef(false);
  /**
   * Initialize screen reader
   */
  useEffect(() => {
    checkScreenReaderSupport().then(supported => {
      setIsSupported(supported);
      if (supported) {
        synthesis.current = window.speechSynthesis;
        initializeVoice();
      }
      setIsReady(true);
    });
    return () => {
      if (synthesis.current) {
        synthesis.current.cancel();
      }
    };
  }, []);
  /**
   * Check if screen reader is supported
   */
  const checkScreenReaderSupport = async (): Promise<boolean> => {
    return 'speechSynthesis' in window;
  };
  /**
   * Initialize voice selection
   */
  const initializeVoice = () => {
    if (!synthesis.current) return;
    // Wait for voices to be loaded
    const handleVoicesChanged = () => {
      const voices = synthesis.current?.getVoices() || [];
      console.log('Available voices:', voices.length);
      // Select voice based on config
      const voice = voices.find(v =>
        v.name === screenReaderConfig.voice ||
        v.lang === screenReaderConfig.language
      ) || voices.find(v => v.default) || voices[0];
      if (voice) {
        setSelectedVoice(voice);
        console.log('Selected voice:', voice.name);
      } else {
        console.warn('No suitable voice found');
      }
    };
    synthesis.current.addEventListener('voiceschanged', handleVoicesChanged);
    handleVoicesChanged(); // Initial check
  };
  /**
   * Process the announcement queue
   */
  const processQueue = useCallback(async () => {
    if (isProcessingQueue.current || !synthesis.current || !selectedVoice) return;
    isProcessingQueue.current = true;
    while (announcementQueue.current.length > 0) {
      const announcement = announcementQueue.current[0];
      try {
        await speak(announcement.text, announcement.options);
        announcementQueue.current.shift();
      } catch (error) {
        console.error('Error processing announcement:', error);
        break;
      }
    }
    isProcessingQueue.current = false;
  }, [selectedVoice]);
  /**
   * Speak text using screen reader
   */
  const speak = (text: string, options: AnnouncementOptions = {}): Promise<void> => {
    return new Promise((resolve, reject) => {
      if (!synthesis.current || !selectedVoice) {
        reject(new Error('Screen reader not initialized'));
        return;
      }
      // Create utterance
      const utterance = new SpeechSynthesisUtterance(text);
      // Configure utterance
      utterance.voice = selectedVoice;
      utterance.volume = screenReaderConfig.volume;
      utterance.rate = screenReaderConfig.rate;
      utterance.pitch = screenReaderConfig.pitch;
      // Handle events
      utterance.onend = () => {
        console.log('Finished speaking:', text);
        resolve();
      };
      utterance.onerror = (event) => {
        console.error('Speech synthesis error:', event);
        reject(event);
      };
      // Speak
      if (options.interrupt) {
        synthesis.current.cancel();
      }
      if (options.delay) {
        setTimeout(() => {
          synthesis.current?.speak(utterance);
        }, options.delay);
      } else {
        synthesis.current.speak(utterance);
      }
    });
  };
  /**
   * Add announcement to queue
   */
  const announce = useCallback((
    text: string,
    options: AnnouncementOptions = {}
  ) => {
    if (!screenReaderConfig.enabled) return;
    // Add to queue based on priority
    if (options.priority === 'high') {
      announcementQueue.current.unshift({ text, options });
    } else {
      announcementQueue.current.push({ text, options });
    }
    // Process queue
    processQueue();
  }, [screenReaderConfig.enabled, processQueue]);
  /**
   * Announce key press
   */
  const announceKey = useCallback((key: string) => {
    const keyName = key === ' ' ? 'space' : key;
    announce(keyName, { priority: 'high', interrupt: true });
  }, [announce]);
  /**
   * Announce error
   */
  const announceError = useCallback((key: string) => {
    announce(`Incorrect key: ${key}`, { priority: 'high', interrupt: true });
  }, [announce]);
  /**
   * Update screen reader configuration
   */
  const updateConfig = useCallback((newConfig: Partial<ScreenReaderConfig>) => {
    Object.assign(screenReaderConfig, newConfig);
    if (newConfig.voice || newConfig.language) {
      initializeVoice();
    }
  }, []);
  /**
   * Clear announcement queue
   */
  const clearQueue = useCallback(() => {
    if (synthesis.current) {
      synthesis.current.cancel();
    }
    announcementQueue.current = [];
    isProcessingQueue.current = false;
  }, []);
  return {
    isSupported,
    isReady,
    announce,
    announceKey,
    announceError,
    updateConfig,
    clearQueue,
    selectedVoice
  };
}
</file>

<file path="src/hooks/use-toast.ts">
import * as React from "react"
import type {
  ToastActionElement,
  ToastProps,
} from "@/components/ui/toast"
const TOAST_LIMIT = 1
const TOAST_REMOVE_DELAY = 1000000
type ToasterToast = ToastProps & {
  id: string
  title?: React.ReactNode
  description?: React.ReactNode
  action?: ToastActionElement
}
const actionTypes = {
  ADD_TOAST: "ADD_TOAST",
  UPDATE_TOAST: "UPDATE_TOAST",
  DISMISS_TOAST: "DISMISS_TOAST",
  REMOVE_TOAST: "REMOVE_TOAST",
} as const
let count = 0
function genId() {
  count = (count + 1) % Number.MAX_SAFE_INTEGER
  return count.toString()
}
type ActionType = typeof actionTypes
type Action =
  | {
      type: ActionType["ADD_TOAST"]
      toast: ToasterToast
    }
  | {
      type: ActionType["UPDATE_TOAST"]
      toast: Partial<ToasterToast>
    }
  | {
      type: ActionType["DISMISS_TOAST"]
      toastId?: ToasterToast["id"]
    }
  | {
      type: ActionType["REMOVE_TOAST"]
      toastId?: ToasterToast["id"]
    }
interface State {
  toasts: ToasterToast[]
}
const toastTimeouts = new Map<string, ReturnType<typeof setTimeout>>()
const addToRemoveQueue = (toastId: string) => {
  if (toastTimeouts.has(toastId)) {
    return
  }
  const timeout = setTimeout(() => {
    toastTimeouts.delete(toastId)
    dispatch({
      type: "REMOVE_TOAST",
      toastId: toastId,
    })
  }, TOAST_REMOVE_DELAY)
  toastTimeouts.set(toastId, timeout)
}
export const reducer = (state: State, action: Action): State => {
  switch (action.type) {
    case "ADD_TOAST":
      return {
        ...state,
        toasts: [action.toast, ...state.toasts].slice(0, TOAST_LIMIT),
      }
    case "UPDATE_TOAST":
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === action.toast.id ? { ...t, ...action.toast } : t
        ),
      }
    case "DISMISS_TOAST": {
      const { toastId } = action
      // ! Side effects ! - This could be extracted into a dismissToast() action,
      // but I'll keep it here for simplicity
      if (toastId) {
        addToRemoveQueue(toastId)
      } else {
        state.toasts.forEach((toast) => {
          addToRemoveQueue(toast.id)
        })
      }
      return {
        ...state,
        toasts: state.toasts.map((t) =>
          t.id === toastId || toastId === undefined
            ? {
                ...t,
                open: false,
              }
            : t
        ),
      }
    }
    case "REMOVE_TOAST":
      if (action.toastId === undefined) {
        return {
          ...state,
          toasts: [],
        }
      }
      return {
        ...state,
        toasts: state.toasts.filter((t) => t.id !== action.toastId),
      }
  }
}
const listeners: Array<(state: State) => void> = []
let memoryState: State = { toasts: [] }
function dispatch(action: Action) {
  memoryState = reducer(memoryState, action)
  listeners.forEach((listener) => {
    listener(memoryState)
  })
}
type Toast = Omit<ToasterToast, "id">
function toast({ ...props }: Toast) {
  const id = genId()
  const update = (props: ToasterToast) =>
    dispatch({
      type: "UPDATE_TOAST",
      toast: { ...props, id },
    })
  const dismiss = () => dispatch({ type: "DISMISS_TOAST", toastId: id })
  dispatch({
    type: "ADD_TOAST",
    toast: {
      ...props,
      id,
      open: true,
      onOpenChange: (open) => {
        if (!open) dismiss()
      },
    },
  })
  return {
    id: id,
    dismiss,
    update,
  }
}
function useToast() {
  const [state, setState] = React.useState<State>(memoryState)
  React.useEffect(() => {
    listeners.push(setState)
    return () => {
      const index = listeners.indexOf(setState)
      if (index > -1) {
        listeners.splice(index, 1)
      }
    }
  }, [state])
  return {
    ...state,
    toast,
    dismiss: (toastId?: string) => dispatch({ type: "DISMISS_TOAST", toastId }),
  }
}
export { useToast, toast }
</file>

<file path="src/hooks/useAchievements.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Achievement Hook                                                                       ║
 * ║ Description: React hook for managing achievements and progress                                 ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useEffect, useMemo } from 'react';
import { useQuery, useQueryClient } from '@tanstack/react-query';
import { useSupabase } from './useSupabase';
import { useToast } from '@/components/ui/use-toast';
import { createAchievementManager, type AchievementProgress } from '@/lib/achievements';
interface UseAchievementsReturn {
  achievements: AchievementProgress[];
  isLoading: boolean;
  error: Error | null;
  checkAchievements: (stats: {
    wpm: number;
    accuracy: number;
    lessons: number;
    streak: number;
    practiceTime: number;
    isPerfectScore: boolean;
  }) => Promise<void>;
}
export function useAchievements(): UseAchievementsReturn {
  const { supabase } = useSupabase();
  const { toast } = useToast();
  const queryClient = useQueryClient();
  // Get current user
  const { data: user } = useQuery({
    queryKey: ['user'],
    queryFn: async () => {
      const { data: { user } } = await supabase.auth.getUser();
      if (!user) throw new Error('No user found');
      return user;
    },
  });
  // Create achievement manager instance
  const achievementManager = useMemo(() => {
    if (!user) return null;
    return createAchievementManager(supabase, user.id, toast);
  }, [user, supabase, toast]);
  // Fetch achievements progress
  const { data: achievements, isLoading, error } = useQuery({
    queryKey: ['achievements', user?.id],
    queryFn: async () => {
      if (!achievementManager) throw new Error('Achievement manager not initialized');
      return achievementManager.getProgress();
    },
    enabled: !!achievementManager,
  });
  // Function to check achievements
  const checkAchievements = async (stats: {
    wpm: number;
    accuracy: number;
    lessons: number;
    streak: number;
    practiceTime: number;
    isPerfectScore: boolean;
  }) => {
    if (!achievementManager) {
      console.error('Achievement manager not initialized');
      return;
    }
    await achievementManager.checkAndUnlockAchievements(stats);
    // Invalidate achievements query to refresh the data
    queryClient.invalidateQueries({ queryKey: ['achievements', user?.id] });
  };
  // Log achievement system initialization
  useEffect(() => {
    if (achievementManager) {
      console.info('Achievement system initialized for user:', user?.id);
    }
  }, [achievementManager, user?.id]);
  return {
    achievements: achievements || [],
    isLoading,
    error: error as Error | null,
    checkAchievements,
  };
}
</file>

<file path="src/hooks/useAI.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: AI Hook                                                                                ║
 * ║ Description: React hook for using AI capabilities in components                                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useState, useCallback, useEffect, useRef } from 'react';
import { useSupabase } from './useSupabase';
import { getAICore } from '@/lib/ai-utils';
import { TutorResponse, TypingAnalysis } from '@/lib/ai-core';
import { useToast } from '@/components/ui/use-toast';
interface UseAIOptions {
  onStreamUpdate?: (text: string) => void;
  enableStreaming?: boolean;
}
interface UseAIReturn {
  generateLesson: (level: number, mistakes: string[], style: string) => Promise<string>;
  getTutoring: (
    current: string,
    target: string,
    mistakes: Array<{ actual: string; expected: string }>,
    level: number
  ) => Promise<TutorResponse>;
  analyzePatterns: (
    mistakes: Array<{ actual: string; expected: string }>,
    speeds: number[]
  ) => Promise<TypingAnalysis>;
  isLoading: boolean;
  error: Error | null;
  clearError: () => void;
}
export function useAI(options: UseAIOptions = {}): UseAIReturn {
  const { supabase } = useSupabase();
  const { toast } = useToast();
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const abortControllerRef = useRef<AbortController | null>(null);
  // Initialize AI core with streaming if enabled
  const ai = getAICore(
    supabase,
    options.enableStreaming ? options.onStreamUpdate : undefined
  );
  // Cleanup function for aborting ongoing requests
  useEffect(() => {
    return () => {
      if (abortControllerRef.current) {
        abortControllerRef.current.abort();
      }
    };
  }, []);
  // Error handling utility
  const handleError = useCallback((error: unknown) => {
    const errorMessage = error instanceof Error ? error.message : 'An unexpected error occurred';
    console.error('AI Error:', error);
    setError(new Error(errorMessage));
    toast({
      title: 'AI Error',
      description: errorMessage,
      variant: 'destructive'
    });
    setIsLoading(false);
  }, [toast]);
  // Clear error state
  const clearError = useCallback(() => {
    setError(null);
  }, []);
  // Generate lesson content
  const generateLesson = useCallback(async (
    level: number,
    mistakes: string[],
    style: string
  ): Promise<string> => {
    try {
      setIsLoading(true);
      setError(null);
      // Create new abort controller for this request
      abortControllerRef.current = new AbortController();
      const content = await ai.generateLessonContent(level, mistakes, style);
      console.info('Generated lesson content:', {
        contentLength: content.length,
        level,
        mistakeCount: mistakes.length,
        style
      });
      return content;
    } catch (error) {
      handleError(error);
      return 'Failed to generate lesson content. Please try again.';
    } finally {
      setIsLoading(false);
    }
  }, [ai, handleError]);
  // Get real-time tutoring feedback
  const getTutoring = useCallback(async (
    current: string,
    target: string,
    mistakes: Array<{ actual: string; expected: string }>,
    level: number
  ): Promise<TutorResponse> => {
    try {
      setIsLoading(true);
      setError(null);
      // Create new abort controller for this request
      abortControllerRef.current = new AbortController();
      const response = await ai.provideTutoring(current, target, mistakes, level);
      console.info('Generated tutoring response:', {
        hasFeedback: !!response.feedback,
        hasNextLesson: !!response.nextLesson,
        hintCount: response.adaptiveHints?.length
      });
      return response;
    } catch (error) {
      handleError(error);
      return {
        feedback: 'Failed to generate tutoring feedback. Please continue practicing.',
        confidenceScore: 0.5
      };
    } finally {
      setIsLoading(false);
    }
  }, [ai, handleError]);
  // Analyze typing patterns
  const analyzePatterns = useCallback(async (
    mistakes: Array<{ actual: string; expected: string }>,
    speeds: number[]
  ): Promise<TypingAnalysis> => {
    try {
      setIsLoading(true);
      setError(null);
      // Create new abort controller for this request
      abortControllerRef.current = new AbortController();
      const analysis = await ai.analyzeTypingPatterns(mistakes, speeds);
      console.info('Generated pattern analysis:', {
        patternCount: analysis.errorPatterns.length,
        speedDataPoints: analysis.speedTrends.length,
        focusAreas: analysis.recommendedFocus
      });
      return analysis;
    } catch (error) {
      handleError(error);
      return {
        errorPatterns: [],
        speedTrends: [],
        recommendedFocus: ['Focus on accuracy and consistent speed']
      };
    } finally {
      setIsLoading(false);
    }
  }, [ai, handleError]);
  return {
    generateLesson,
    getTutoring,
    analyzePatterns,
    isLoading,
    error,
    clearError
  };
}
</file>

<file path="src/hooks/useLeaderboard.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Leaderboard Hook                                                                       ║
 * ║ Description: React hook for managing leaderboard data with filtering and sorting               ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { useQuery, useQueryClient } from '@tanstack/react-query';
import { useSupabase } from './useSupabase';
import { useToast } from '@/components/ui/use-toast';
export type TimePeriod = 'all' | 'today' | 'week' | 'month';
export type SortBy = 'wpm' | 'accuracy' | 'lessons' | 'achievements';
export interface LeaderboardEntry {
  id: string;
  username: string;
  full_name: string | null;
  avatar_url: string | null;
  average_wpm: number;
  average_accuracy: number;
  total_lessons_completed: number;
  level: number;
  achievements_count: number;
  rank: number;
}
interface UseLeaderboardOptions {
  limit?: number;
  timePeriod?: TimePeriod;
  sortBy?: SortBy;
  refetchInterval?: number;
  onError?: (error: Error) => void;
}
interface UseLeaderboardReturn {
  leaderboard: LeaderboardEntry[];
  isLoading: boolean;
  error: Error | null;
  currentUserRank: number | null;
  refetch: () => Promise<void>;
  invalidate: () => Promise<void>;
}
export function useLeaderboard({
  limit = 100,
  timePeriod = 'all',
  sortBy = 'wpm',
  refetchInterval = 30000,
  onError,
}: UseLeaderboardOptions = {}): UseLeaderboardReturn {
  const { supabase } = useSupabase();
  const { toast } = useToast();
  const queryClient = useQueryClient();
  // Get current user for rank highlighting
  const { data: currentUser } = useQuery({
    queryKey: ['user'],
    queryFn: async () => {
      const { data: { user } } = await supabase.auth.getUser();
      return user;
    },
  });
  // Fetch leaderboard data
  const { data: leaderboard = [], isLoading, error } = useQuery({
    queryKey: ['leaderboard', timePeriod, sortBy, limit],
    queryFn: async () => {
      try {
        const { data, error } = await supabase
          .rpc('get_leaderboard', {
            p_limit: limit,
            p_time_period: timePeriod,
            p_sort_by: sortBy
          });
        if (error) throw error;
        return data as LeaderboardEntry[];
      } catch (error) {
        console.error('Error fetching leaderboard:', error);
        const errorMessage = error instanceof Error ? error.message : 'Failed to fetch leaderboard';
        toast({
          title: 'Error',
          description: errorMessage,
          variant: 'destructive',
        });
        onError?.(error as Error);
        throw error;
      }
    },
    refetchInterval,
  });
  // Find current user's rank
  const currentUserRank = currentUser
    ? leaderboard.find(entry => entry.id === currentUser.id)?.rank ?? null
    : null;
  // Function to manually refetch data
  const refetch = async () => {
    try {
      await queryClient.invalidateQueries({
        queryKey: ['leaderboard', timePeriod, sortBy, limit],
      });
    } catch (error) {
      console.error('Error refetching leaderboard:', error);
      const errorMessage = error instanceof Error ? error.message : 'Failed to refetch leaderboard';
      toast({
        title: 'Error',
        description: errorMessage,
        variant: 'destructive',
      });
      onError?.(error as Error);
    }
  };
  // Function to invalidate cache
  const invalidate = async () => {
    await queryClient.invalidateQueries({
      queryKey: ['leaderboard'],
    });
  };
  return {
    leaderboard,
    isLoading,
    error: error as Error | null,
    currentUserRank,
    refetch,
    invalidate,
  };
}
// Export a function to format rank display
export function formatRank(rank: number): string {
  const suffixes = ['th', 'st', 'nd', 'rd'];
  const v = rank % 100;
  return rank + (suffixes[(v - 20) % 10] || suffixes[v] || suffixes[0]);
}
// Export a function to get rank color
export function getRankColor(rank: number): string {
  switch (rank) {
    case 1:
      return 'text-yellow-500';
    case 2:
      return 'text-slate-400';
    case 3:
      return 'text-amber-600';
    default:
      return 'text-muted-foreground';
  }
}
// Export a function to get rank icon
export function getRankIcon(rank: number): string {
  switch (rank) {
    case 1:
      return '🏆';
    case 2:
      return '🥈';
    case 3:
      return '🥉';
    default:
      return `#${rank}`;
  }
}
</file>

<file path="src/hooks/useSupabase.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Supabase Hook                                                                          ║
 * ║ Description: React hook for accessing Supabase client and authentication                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { createClient } from '@supabase/supabase-js';
import { useEffect, useState } from 'react';
import { getConfig } from '@/lib/config';
// Initialize Supabase client
const config = getConfig();
const supabase = createClient(
  config.supabaseUrl,
  config.supabaseAnonKey
);
interface UseSupabaseReturn {
  supabase: typeof supabase;
  isInitialized: boolean;
}
export function useSupabase(): UseSupabaseReturn {
  const [isInitialized, setIsInitialized] = useState(false);
  useEffect(() => {
    // Verify connection and set initialized state
    const checkConnection = async () => {
      try {
        const { data, error } = await supabase.from('health_check').select('*').limit(1);
        if (error) throw error;
        console.info('Supabase connection verified');
        setIsInitialized(true);
      } catch (error) {
        console.error('Failed to connect to Supabase:', error);
        setIsInitialized(false);
      }
    };
    checkConnection();
  }, []);
  return {
    supabase,
    isInitialized
  };
}
// Export singleton instance for non-React usage
export { supabase };
</file>

<file path="src/integrations/supabase/client.ts">
// This file is automatically generated. Do not edit it directly.
import { createClient } from '@supabase/supabase-js';
import type { Database } from './types';
const SUPABASE_URL = "https://olvtlevlbgtalcnhcnvh.supabase.co";
const SUPABASE_PUBLISHABLE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im9sdnRsZXZsYmd0YWxjbmhjbnZoIiwicm9sZSI6ImFub24iLCJpYXQiOjE3Mzk4NTQ5MjAsImV4cCI6MjA1NTQzMDkyMH0.Xa_PvI1tU4c_vsHH6FAO4Km65w4eY9UjiQ6f0jbdjDE";
export const supabase = createClient<Database>(
  SUPABASE_URL,
  SUPABASE_PUBLISHABLE_KEY,
  {
    auth: {
      flowType: 'pkce',
      autoRefreshToken: true,
      detectSessionInUrl: true,
      persistSession: true,
    },
  }
);
</file>

<file path="src/integrations/supabase/types.ts">
export type Json =
  | string
  | number
  | boolean
  | null
  | { [key: string]: Json | undefined }
  | Json[]
export type Database = {
  public: {
    Tables: {
      profiles: {
        Row: {
          accuracy_percentage: number | null
          created_at: string | null
          display_name: string | null
          id: string
          last_lesson_date: string | null
          words_per_minute: number | null
        }
        Insert: {
          accuracy_percentage?: number | null
          created_at?: string | null
          display_name?: string | null
          id: string
          last_lesson_date?: string | null
          words_per_minute?: number | null
        }
        Update: {
          accuracy_percentage?: number | null
          created_at?: string | null
          display_name?: string | null
          id?: string
          last_lesson_date?: string | null
          words_per_minute?: number | null
        }
        Relationships: []
      }
      typing_history: {
        Row: {
          accuracy_percentage: number
          created_at: string
          id: string
          lesson_level: string
          user_id: string
          words_per_minute: number
        }
        Insert: {
          accuracy_percentage: number
          created_at?: string
          id?: string
          lesson_level: string
          user_id: string
          words_per_minute: number
        }
        Update: {
          accuracy_percentage?: number
          created_at?: string
          id?: string
          lesson_level?: string
          user_id?: string
          words_per_minute?: number
        }
        Relationships: []
      }
    }
    Views: {
      [_ in never]: never
    }
    Functions: {
      [_ in never]: never
    }
    Enums: {
      [_ in never]: never
    }
    CompositeTypes: {
      [_ in never]: never
    }
  }
}
type PublicSchema = Database[Extract<keyof Database, "public">]
export type Tables<
  PublicTableNameOrOptions extends
    | keyof (PublicSchema["Tables"] & PublicSchema["Views"])
    | { schema: keyof Database },
  TableName extends PublicTableNameOrOptions extends { schema: keyof Database }
    ? keyof (Database[PublicTableNameOrOptions["schema"]]["Tables"] &
        Database[PublicTableNameOrOptions["schema"]]["Views"])
    : never = never,
> = PublicTableNameOrOptions extends { schema: keyof Database }
  ? (Database[PublicTableNameOrOptions["schema"]]["Tables"] &
      Database[PublicTableNameOrOptions["schema"]]["Views"])[TableName] extends {
      Row: infer R
    }
    ? R
    : never
  : PublicTableNameOrOptions extends keyof (PublicSchema["Tables"] &
        PublicSchema["Views"])
    ? (PublicSchema["Tables"] &
        PublicSchema["Views"])[PublicTableNameOrOptions] extends {
        Row: infer R
      }
      ? R
      : never
    : never
export type TablesInsert<
  PublicTableNameOrOptions extends
    | keyof PublicSchema["Tables"]
    | { schema: keyof Database },
  TableName extends PublicTableNameOrOptions extends { schema: keyof Database }
    ? keyof Database[PublicTableNameOrOptions["schema"]]["Tables"]
    : never = never,
> = PublicTableNameOrOptions extends { schema: keyof Database }
  ? Database[PublicTableNameOrOptions["schema"]]["Tables"][TableName] extends {
      Insert: infer I
    }
    ? I
    : never
  : PublicTableNameOrOptions extends keyof PublicSchema["Tables"]
    ? PublicSchema["Tables"][PublicTableNameOrOptions] extends {
        Insert: infer I
      }
      ? I
      : never
    : never
export type TablesUpdate<
  PublicTableNameOrOptions extends
    | keyof PublicSchema["Tables"]
    | { schema: keyof Database },
  TableName extends PublicTableNameOrOptions extends { schema: keyof Database }
    ? keyof Database[PublicTableNameOrOptions["schema"]]["Tables"]
    : never = never,
> = PublicTableNameOrOptions extends { schema: keyof Database }
  ? Database[PublicTableNameOrOptions["schema"]]["Tables"][TableName] extends {
      Update: infer U
    }
    ? U
    : never
  : PublicTableNameOrOptions extends keyof PublicSchema["Tables"]
    ? PublicSchema["Tables"][PublicTableNameOrOptions] extends {
        Update: infer U
      }
      ? U
      : never
    : never
export type Enums<
  PublicEnumNameOrOptions extends
    | keyof PublicSchema["Enums"]
    | { schema: keyof Database },
  EnumName extends PublicEnumNameOrOptions extends { schema: keyof Database }
    ? keyof Database[PublicEnumNameOrOptions["schema"]]["Enums"]
    : never = never,
> = PublicEnumNameOrOptions extends { schema: keyof Database }
  ? Database[PublicEnumNameOrOptions["schema"]]["Enums"][EnumName]
  : PublicEnumNameOrOptions extends keyof PublicSchema["Enums"]
    ? PublicSchema["Enums"][PublicEnumNameOrOptions]
    : never
export type CompositeTypes<
  PublicCompositeTypeNameOrOptions extends
    | keyof PublicSchema["CompositeTypes"]
    | { schema: keyof Database },
  CompositeTypeName extends PublicCompositeTypeNameOrOptions extends {
    schema: keyof Database
  }
    ? keyof Database[PublicCompositeTypeNameOrOptions["schema"]]["CompositeTypes"]
    : never = never,
> = PublicCompositeTypeNameOrOptions extends { schema: keyof Database }
  ? Database[PublicCompositeTypeNameOrOptions["schema"]]["CompositeTypes"][CompositeTypeName]
  : PublicCompositeTypeNameOrOptions extends keyof PublicSchema["CompositeTypes"]
    ? PublicSchema["CompositeTypes"][PublicCompositeTypeNameOrOptions]
    : never
</file>

<file path="src/lib/__tests__/ai-core.test.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: AI Core Tests                                                                          ║
 * ║ Description: Test suite for AI core functionality                                              ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { describe, it, expect, vi, beforeEach, afterEach } from 'vitest'
import { AICore, AIConfig } from '../ai-core'
import { createClient } from '@supabase/supabase-js'
import { getConfig } from '../config'
// Mock external dependencies
vi.mock('@supabase/supabase-js', () => ({
  createClient: vi.fn(() => ({
    from: vi.fn(() => ({
      insert: vi.fn().mockResolvedValue({ data: null, error: null }),
      select: vi.fn().mockResolvedValue({ data: [], error: null })
    }))
  }))
}))
vi.mock('../config', () => ({
  getConfig: vi.fn(() => ({
    openaiApiKey: 'test-openai-key',
    googleAiApiKey: 'test-gemini-key',
    aiModel: 'gemini-pro',
    aiTemperature: 0.7,
    aiMaxTokens: 2048,
    supabaseUrl: 'https://test.supabase.co',
    supabaseAnonKey: 'test-anon-key',
    enableRealTimeAnalysis: true,
    enableVoiceCommands: true,
    enableHapticFeedback: true,
    enablePerformanceTracking: true,
    enableErrorTracking: true,
    analyticsSampleRate: 100
  }))
}))
describe('AICore', () => {
  let ai: AICore
  let supabase: ReturnType<typeof createClient>
  let config: AIConfig
  let streamCallback: (text: string) => void
  beforeEach(() => {
    // Reset mocks
    vi.clearAllMocks()
    // Initialize test dependencies
    streamCallback = vi.fn()
    supabase = createClient('https://test.supabase.co', 'test-anon-key')
    config = {
      model: 'gemini-pro',
      temperature: 0.7,
      maxTokens: 2048,
      streamCallback
    }
    // Initialize AI core
    ai = new AICore(supabase, config)
  })
  afterEach(() => {
    vi.resetAllMocks()
  })
  describe('generateLessonContent', () => {
    it('should generate lesson content with Gemini', async () => {
      const mockContent = 'Practice typing: The quick brown fox jumps over the lazy dog'
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockResolvedValue({
          response: { text: () => mockContent }
        })
      })
      const content = await ai.generateLessonContent(1, [], 'standard')
      expect(content).toBe(mockContent)
    })
    it('should handle streaming responses', async () => {
      const mockChunks = ['Practice ', 'typing: ', 'Hello ', 'World']
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContentStream: vi.fn().mockResolvedValue({
          stream: {
            async *[Symbol.asyncIterator]() {
              for (const chunk of mockChunks) {
                yield { text: () => chunk }
              }
            }
          }
        })
      })
      const content = await ai.generateLessonContent(1, [], 'standard')
      expect(content).toBe(mockChunks.join(''))
      expect(streamCallback).toHaveBeenCalledTimes(mockChunks.length)
    })
    it('should fall back to basic content on error', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockRejectedValue(new Error('API Error'))
      })
      const content = await ai.generateLessonContent(1, [], 'standard')
      expect(content).toContain('Practice typing home row keys')
    })
  })
  describe('provideTutoring', () => {
    const mockResponse = {
      feedback: 'Good progress!',
      nextLesson: 'Try capital letters next',
      adaptiveHints: ['Focus on finger placement'],
      confidenceScore: 0.8
    }
    it('should provide tutoring feedback', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockResolvedValue({
          response: { text: () => JSON.stringify(mockResponse) }
        })
      })
      const response = await ai.provideTutoring('hello', 'hello world', [], 1)
      expect(response).toEqual(mockResponse)
    })
    it('should handle streaming tutoring responses', async () => {
      const mockChunks = [
        '{"feedback": "Good',
        ' progress!",',
        '"confidenceScore": 0.8}'
      ]
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContentStream: vi.fn().mockResolvedValue({
          stream: {
            async *[Symbol.asyncIterator]() {
              for (const chunk of mockChunks) {
                yield { text: () => chunk }
              }
            }
          }
        })
      })
      const response = await ai.provideTutoring('hello', 'hello world', [], 1)
      expect(response).toHaveProperty('feedback', 'Good progress!')
      expect(response).toHaveProperty('confidenceScore', 0.8)
      expect(streamCallback).toHaveBeenCalledTimes(mockChunks.length)
    })
    it('should handle errors gracefully', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockRejectedValue(new Error('API Error'))
      })
      const response = await ai.provideTutoring('hello', 'hello world', [], 1)
      expect(response).toHaveProperty('feedback')
      expect(response.confidenceScore).toBe(0.5)
    })
  })
  describe('analyzeTypingPatterns', () => {
    const mockAnalysis = {
      errorPatterns: [
        {
          pattern: 'th -> ht',
          frequency: 3,
          suggestion: 'Practice "th" digraph'
        }
      ],
      speedTrends: [
        {
          timestamp: '2024-01-01T00:00:00Z',
          wpm: 45
        }
      ],
      recommendedFocus: ['Accuracy', 'Speed']
    }
    it('should analyze typing patterns', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockResolvedValue({
          response: { text: () => JSON.stringify(mockAnalysis) }
        })
      })
      const analysis = await ai.analyzeTypingPatterns([], [45])
      expect(analysis).toEqual(mockAnalysis)
    })
    it('should store analysis in Supabase when tracking is enabled', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockResolvedValue({
          response: { text: () => JSON.stringify(mockAnalysis) }
        })
      })
      await ai.analyzeTypingPatterns([], [45])
      expect(supabase.from).toHaveBeenCalledWith('typing_analysis')
      expect(supabase.from('typing_analysis').insert).toHaveBeenCalled()
    })
    it('should handle analysis errors gracefully', async () => {
      vi.spyOn(ai as any, 'geminiModel').mockImplementation({
        generateContent: vi.fn().mockRejectedValue(new Error('API Error'))
      })
      const analysis = await ai.analyzeTypingPatterns([], [45])
      expect(analysis.errorPatterns).toHaveLength(0)
      expect(analysis.recommendedFocus).toHaveLength(1)
    })
  })
})
</file>

<file path="src/lib/typing/error-analysis.worker.ts">
/*
 * ███████╗██████╗ ██████╗  ██████╗ ██████╗     ██╗    ██╗ ██████╗ ██████╗ ██╗  ██╗███████╗██████╗
 * ██╔════╝██╔══██╗██╔══██╗██╔═══██╗██╔══██╗    ██║    ██║██╔═══██╗██╔══██╗██║ ██╔╝██╔════╝██╔══██╗
 * █████╗  ██████╔╝██████╔╝██║   ██║██████╔╝    ██║ █╗ ██║██║   ██║██████╔╝█████╔╝ █████╗  ██████╔╝
 * ██╔══╝  ██╔══██╗██╔══██╗██║   ██║██╔══██╗    ██║███╗██║██║   ██║██╔══██╗██╔═██╗ ██╔══╝  ██╔══██╗
 * ███████╗██║  ██║██║  ██║╚██████╔╝██║  ██║    ╚███╔███╔╝╚██████╔╝██║  ██║██║  ██╗███████╗██║  ██║
 * ╚══════╝╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝ ╚═╝  ╚═╝     ╚══╝╚══╝  ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Worker: Error Analysis                                                                         ║
 * ║ Description: Web worker for analyzing typing errors and patterns                              ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { TypingSessionData, Analysis } from '../../types/typing';
/**
 * Error pattern categories for classification
 */
const ERROR_PATTERNS = {
  TRANSPOSITION: 'transposition',
  SUBSTITUTION: 'substitution',
  INSERTION: 'insertion',
  OMISSION: 'omission',
  REPETITION: 'repetition',
  CAPITALIZATION: 'capitalization',
  TIMING: 'timing'
} as const;
/**
 * Analyzes typing errors to identify patterns and provide insights
 */
class ErrorAnalysisWorker {
  private sessionData: TypingSessionData;
  private analysis: Analysis;
  constructor() {
    this.initializeAnalysis();
    self.addEventListener('message', this.handleMessage.bind(this));
  }
  /**
   * Initialize analysis object with default values
   */
  private initializeAnalysis(): void {
    this.analysis = {
      wpm: 0,
      accuracy: 0,
      errorPatterns: [],
      speedTrends: [],
      recommendedFocus: [],
      aiRecommendations: []
    };
  }
  /**
   * Handle incoming messages from the main thread
   */
  private handleMessage(event: MessageEvent): void {
    console.log('Worker received message:', { type: event.data.type });
    switch (event.data.type) {
      case 'ANALYZE_SESSION':
        this.sessionData = event.data.payload;
        this.analyzeSession();
        break;
      default:
        console.error('Unknown message type:', event.data.type);
    }
  }
  /**
   * Main analysis pipeline for typing session data
   */
  private analyzeSession(): void {
    console.log('Starting session analysis...');
    try {
      this.calculateBaseMetrics();
      this.identifyErrorPatterns();
      this.analyzeSpeedTrends();
      this.generateRecommendations();
      self.postMessage({
        type: 'ANALYSIS_COMPLETE',
        payload: this.analysis
      });
    } catch (error) {
      console.error('Error during analysis:', error);
      self.postMessage({
        type: 'ANALYSIS_ERROR',
        error: error.message
      });
    }
  }
  /**
   * Calculate basic typing metrics (WPM, accuracy)
   */
  private calculateBaseMetrics(): void {
    console.log('Calculating base metrics...');
    const { metrics } = this.sessionData;
    const totalChars = this.sessionData.context.targetText.length;
    const errorCount = this.sessionData.errors.length;
    this.analysis.wpm = metrics.wpm;
    this.analysis.accuracy = ((totalChars - errorCount) / totalChars) * 100;
  }
  /**
   * Identify patterns in typing errors
   */
  private identifyErrorPatterns(): void {
    console.log('Identifying error patterns...');
    const patterns = new Map<string, number>();
    this.sessionData.errors.forEach(error => {
      const pattern = this.classifyError(error.expected, error.actual);
      patterns.set(pattern, (patterns.get(pattern) || 0) + 1);
    });
    this.analysis.errorPatterns = Array.from(patterns.entries())
      .map(([pattern, frequency]) => ({
        pattern,
        frequency,
        suggestion: this.getSuggestionForPattern(pattern)
      }))
      .sort((a, b) => b.frequency - a.frequency);
  }
  /**
   * Classify type of error based on expected and actual input
   */
  private classifyError(expected: string, actual: string): string {
    if (actual === expected.toLowerCase() || actual === expected.toUpperCase()) {
      return ERROR_PATTERNS.CAPITALIZATION;
    }
    if (actual.length > expected.length) {
      return ERROR_PATTERNS.INSERTION;
    }
    if (actual.length < expected.length) {
      return ERROR_PATTERNS.OMISSION;
    }
    if (actual === expected[0].repeat(expected.length)) {
      return ERROR_PATTERNS.REPETITION;
    }
    if (actual === expected.split('').reverse().join('')) {
      return ERROR_PATTERNS.TRANSPOSITION;
    }
    return ERROR_PATTERNS.SUBSTITUTION;
  }
  /**
   * Analyze typing speed trends over time
   */
  private analyzeSpeedTrends(): void {
    console.log('Analyzing speed trends...');
    const timeWindows = this.splitIntoTimeWindows(60000); // 1-minute windows
    this.analysis.speedTrends = timeWindows.map(window => ({
      timestamp: new Date(window.startTime).toISOString(),
      wpm: this.calculateWPMForWindow(window)
    }));
  }
  /**
   * Split session data into time windows for trend analysis
   */
  private splitIntoTimeWindows(windowSize: number): Array<{
    startTime: number;
    keyPresses: typeof this.sessionData.keyPresses;
  }> {
    const windows: Array<{
      startTime: number;
      keyPresses: typeof this.sessionData.keyPresses;
    }> = [];
    const { keyPresses } = this.sessionData;
    if (keyPresses.length === 0) return windows;
    const startTime = keyPresses[0].timestamp;
    const endTime = keyPresses[keyPresses.length - 1].timestamp;
    for (let time = startTime; time < endTime; time += windowSize) {
      windows.push({
        startTime: time,
        keyPresses: keyPresses.filter(
          press => press.timestamp >= time && press.timestamp < time + windowSize
        )
      });
    }
    return windows;
  }
  /**
   * Calculate WPM for a specific time window
   */
  private calculateWPMForWindow(window: {
    startTime: number;
    keyPresses: typeof this.sessionData.keyPresses;
  }): number {
    const chars = window.keyPresses.length;
    const minutes = 1; // Fixed 1-minute windows
    return Math.round((chars / 5) / minutes);
  }
  /**
   * Generate personalized recommendations based on analysis
   */
  private generateRecommendations(): void {
    console.log('Generating recommendations...');
    const recommendations: string[] = [];
    const focusAreas: string[] = [];
    // Analyze error patterns
    if (this.analysis.errorPatterns.length > 0) {
      const mostCommonError = this.analysis.errorPatterns[0];
      recommendations.push(
        `Focus on reducing ${mostCommonError.pattern} errors by ${mostCommonError.suggestion}`
      );
      focusAreas.push(mostCommonError.pattern);
    }
    // Analyze speed consistency
    const speedVariance = this.calculateSpeedVariance();
    if (speedVariance > 10) {
      recommendations.push(
        'Work on maintaining consistent typing speed throughout practice sessions'
      );
      focusAreas.push('speed_consistency');
    }
    // Check accuracy threshold
    if (this.analysis.accuracy < 95) {
      recommendations.push(
        'Prioritize accuracy over speed - slow down and focus on correct finger placement'
      );
      focusAreas.push('accuracy');
    }
    this.analysis.aiRecommendations = recommendations;
    this.analysis.recommendedFocus = focusAreas;
  }
  /**
   * Calculate variance in typing speed
   */
  private calculateSpeedVariance(): number {
    const speeds = this.analysis.speedTrends.map(trend => trend.wpm);
    const mean = speeds.reduce((sum, speed) => sum + speed, 0) / speeds.length;
    const squaredDiffs = speeds.map(speed => Math.pow(speed - mean, 2));
    const variance = squaredDiffs.reduce((sum, diff) => sum + diff, 0) / speeds.length;
    return Math.sqrt(variance);
  }
  /**
   * Get improvement suggestion for specific error pattern
   */
  private getSuggestionForPattern(pattern: string): string {
    const suggestions: Record<string, string> = {
      [ERROR_PATTERNS.TRANSPOSITION]: 'practice slower, deliberate key strikes',
      [ERROR_PATTERNS.SUBSTITUTION]: 'focus on finger placement and key mapping',
      [ERROR_PATTERNS.INSERTION]: 'work on key release timing',
      [ERROR_PATTERNS.OMISSION]: 'improve finger strength and key activation',
      [ERROR_PATTERNS.REPETITION]: 'practice key release timing',
      [ERROR_PATTERNS.CAPITALIZATION]: 'focus on shift key coordination',
      [ERROR_PATTERNS.TIMING]: 'maintain steady rhythm while typing'
    };
    return suggestions[pattern] || 'focus on overall accuracy';
  }
}
// Initialize worker
new ErrorAnalysisWorker();
// Export for type checking
export type { ErrorAnalysisWorker };
</file>

<file path="src/lib/accessibility-core.ts">
/*
 * ████████╗██╗   ██╗██████╗ ██╗███╗   ██╗ ██████╗      █████╗ ██╗
 * ╚══██╔══╝╚██╗ ██╔╝██╔══██╗██║████╗  ██║██╔════╝     ██╔══██╗██║
 *    ██║    ╚████╔╝ ██████╔╝██║██╔██╗ ██║██║  ███╗    ███████║██║
 *    ██║     ╚██╔╝  ██╔═══╝ ██║██║╚██╗██║██║   ██║    ██╔══██║██║
 *    ██║      ██║   ██║     ██║██║ ╚████║╚██████╔╝    ██║  ██║██║
 *    ╚═╝      ╚═╝   ╚═╝     ╚═╝╚═╝  ╚═══╝ ╚═════╝     ╚═╝  ╚═╝╚═╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: AccessibilityCore                                                                      ║
 * ║ Description: Core accessibility module providing voice commands, haptic feedback, and          ║
 * ║              advanced error handling for an enhanced typing experience.                        ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
// Type declarations for Web Speech API
declare global {
  interface Window {
    SpeechSynthesis: typeof SpeechSynthesis;
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
    speechSynthesis: SpeechSynthesis;
  }
}
export interface AccessibilityConfig {
  speechRate: number;
  voiceURI: string;
  volume: number;
  pitch: number;
  hapticFeedback: boolean;
  autoCorrect: boolean;
  errorTolerance: number;
  commandDelay: number;
}
export interface VoiceCommand {
  command: string;
  action: () => void;
  description: string;
  examples: string[];
}
export interface FeedbackOptions {
  visual?: boolean;
  audio?: boolean;
  haptic?: boolean;
  priority?: boolean;
}
export class AccessibilityCore {
  private synthesis: SpeechSynthesis;
  private recognition: SpeechRecognition;
  private commands: Map<string, VoiceCommand>;
  private lastCommand: string = '';
  private lastCommandTime: number = 0;
  private errorCount: number = 0;
  private isListening: boolean = false;
  constructor(private config: AccessibilityConfig) {
    // Initialize speech synthesis and recognition
    console.info('Initializing AccessibilityCore with config:', config);
    this.synthesis = window.speechSynthesis;
    this.recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    this.commands = new Map();
    // Initialize with default configuration
    this.config = {
      speechRate: 1,
      voiceURI: '',
      volume: 1,
      pitch: 1,
      hapticFeedback: true,
      autoCorrect: true,
      errorTolerance: 3,
      commandDelay: 1000,
      ...config
    };
    this.initializeVoiceCommands();
    this.setupErrorHandling();
  }
  private initializeVoiceCommands() {
    console.info('Setting up voice recognition system...');
    this.recognition.continuous = true;
    this.recognition.interimResults = true;
    this.recognition.maxAlternatives = 3;
    // Register default commands
    this.registerCommand({
      command: 'start lesson',
      action: () => this.provideFeedback('Starting new lesson...', { audio: true, visual: true }),
      description: 'Starts a new typing lesson',
      examples: ['start lesson', 'begin lesson', 'new lesson']
    });
    this.registerCommand({
      command: 'pause',
      action: () => this.provideFeedback('Lesson paused. Say "resume" to continue.', { audio: true }),
      description: 'Pauses the current lesson',
      examples: ['pause', 'stop', 'wait']
    });
    this.registerCommand({
      command: 'resume',
      action: () => this.provideFeedback('Resuming lesson...', { audio: true }),
      description: 'Resumes the paused lesson',
      examples: ['resume', 'continue', 'go on']
    });
    this.registerCommand({
      command: 'repeat',
      action: () => this.provideFeedback('Repeating last instruction...', { audio: true }),
      description: 'Repeats the last instruction',
      examples: ['repeat', 'say again', 'what was that']
    });
    // Set up recognition handlers
    this.recognition.onstart = () => {
      console.info('Voice recognition started');
      this.isListening = true;
      this.provideFeedback('Voice control activated', { visual: true });
    };
    this.recognition.onend = () => {
      console.info('Voice recognition ended');
      this.isListening = false;
      if (this.errorCount < this.config.errorTolerance) {
        this.recognition.start();
      }
    };
    this.recognition.onerror = (event) => {
      console.error('Voice recognition error:', event.error);
      this.handleError(event);
    };
    this.recognition.onresult = (event) => {
      const result = event.results[event.results.length - 1];
      const transcript = result[0].transcript.toLowerCase().trim();
      console.info('Voice command received:', transcript);
      this.processCommand(transcript, result[0].confidence);
    };
  }
  private processCommand(transcript: string, confidence: number) {
    // Prevent command spam
    const now = Date.now();
    if (now - this.lastCommandTime < this.config.commandDelay) {
      return;
    }
    // Find best matching command
    let bestMatch: VoiceCommand | undefined;
    let bestConfidence = 0;
    this.commands.forEach((command, key) => {
      const similarity = this.calculateSimilarity(transcript, key);
      if (similarity > bestConfidence) {
        bestConfidence = similarity;
        bestMatch = command;
      }
    });
    if (bestMatch && bestConfidence > 0.8) {
      console.info('Executing command:', bestMatch.description);
      bestMatch.action();
      this.lastCommand = transcript;
      this.lastCommandTime = now;
      this.errorCount = 0;
    } else {
      this.provideFeedback('Command not recognized. Please try again.', { audio: true });
      this.errorCount++;
    }
  }
  private calculateSimilarity(str1: string, str2: string): number {
    // Implement Levenshtein distance or similar algorithm
    // This is a simplified version
    const s1 = str1.toLowerCase();
    const s2 = str2.toLowerCase();
    return s1.includes(s2) || s2.includes(s1) ? 1 : 0;
  }
  private setupErrorHandling() {
    window.addEventListener('error', (event) => {
      console.error('Global error caught:', event.error);
      this.handleError(event);
    });
    window.addEventListener('unhandledrejection', (event) => {
      console.error('Unhandled promise rejection:', event.reason);
      this.handleError(event);
    });
  }
  private handleError(error: any) {
    this.errorCount++;
    console.error('Error in AccessibilityCore:', error);
    if (this.errorCount >= this.config.errorTolerance) {
      this.provideFeedback(
        'Voice control is having trouble. Switching to keyboard mode.',
        { audio: true, visual: true, priority: true }
      );
      this.recognition.stop();
    }
  }
  public registerCommand(command: VoiceCommand) {
    this.commands.set(command.command, command);
    console.info('Registered new command:', command.description);
  }
  public provideFeedback(message: string, options: FeedbackOptions = {}) {
    console.info('Providing feedback:', { message, options });
    // Visual feedback
    if (options.visual) {
      // Dispatch custom event for UI updates
      window.dispatchEvent(new CustomEvent('accessibility-feedback', {
        detail: { message, type: 'visual' }
      }));
    }
    // Audio feedback
    if (options.audio !== false) {
      const utterance = new SpeechSynthesisUtterance(message);
      utterance.rate = this.config.speechRate;
      utterance.volume = this.config.volume;
      utterance.pitch = this.config.pitch;
      if (options.priority) {
        this.synthesis.cancel();
      }
      this.synthesis.speak(utterance);
    }
    // Haptic feedback
    if (options.haptic && this.config.hapticFeedback && navigator.vibrate) {
      navigator.vibrate(200);
    }
  }
  public startListening() {
    if (!this.isListening) {
      try {
        this.recognition.start();
        console.info('Voice recognition started');
      } catch (error) {
        console.error('Error starting voice recognition:', error);
        this.handleError(error);
      }
    }
  }
  public stopListening() {
    if (this.isListening) {
      try {
        this.recognition.stop();
        console.info('Voice recognition stopped');
      } catch (error) {
        console.error('Error stopping voice recognition:', error);
        this.handleError(error);
      }
    }
  }
  public updateConfig(newConfig: Partial<AccessibilityConfig>) {
    this.config = { ...this.config, ...newConfig };
    console.info('Updated accessibility configuration:', this.config);
  }
}
</file>

<file path="src/lib/achievements.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Achievements                                                                           ║
 * ║ Description: Achievement system for tracking and unlocking user accomplishments                ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { SupabaseClient } from '@supabase/supabase-js';
import { useToast } from '@/components/ui/use-toast';
import { LucideIcon, Trophy, Zap, Target, Clock, Star, Award, Crown, Medal } from 'lucide-react';
import { createElement } from 'react';
import AchievementNotification from '@/components/achievements/AchievementNotification';
export interface Achievement {
  id: string;
  name: string;
  description: string;
  icon: keyof typeof ACHIEVEMENT_ICONS;
  criteria: AchievementCriteria;
  rarity: 'common' | 'rare' | 'epic' | 'legendary';
}
export interface AchievementCriteria {
  type: 'wpm' | 'accuracy' | 'lessons' | 'streak' | 'practice_time' | 'perfect_score';
  threshold: number;
  comparison: 'gte' | 'eq' | 'lte';
}
export interface AchievementProgress {
  achievementId: string;
  progress: number;
  isUnlocked: boolean;
  unlockedAt?: string;
}
export const ACHIEVEMENT_ICONS = {
  Zap,
  Target,
  Trophy,
  Clock,
  Star,
  Award,
  Crown,
  Medal
} as const;
// Define achievements with their criteria
export const ACHIEVEMENTS: Achievement[] = [
  {
    id: 'speed_demon',
    name: 'Speed Demon',
    description: 'Reach 100 WPM in a typing session',
    icon: 'Zap',
    criteria: {
      type: 'wpm',
      threshold: 100,
      comparison: 'gte'
    },
    rarity: 'epic'
  },
  {
    id: 'perfect_accuracy',
    name: 'Perfectionist',
    description: 'Complete a lesson with 100% accuracy',
    icon: 'Target',
    criteria: {
      type: 'accuracy',
      threshold: 100,
      comparison: 'eq'
    },
    rarity: 'rare'
  },
  {
    id: 'practice_master',
    name: 'Practice Master',
    description: 'Complete 50 typing lessons',
    icon: 'Trophy',
    criteria: {
      type: 'lessons',
      threshold: 50,
      comparison: 'gte'
    },
    rarity: 'common'
  },
  {
    id: 'consistency_king',
    name: 'Consistency King',
    description: 'Maintain a 10-day practice streak',
    icon: 'Crown',
    criteria: {
      type: 'streak',
      threshold: 10,
      comparison: 'gte'
    },
    rarity: 'rare'
  },
  {
    id: 'dedication',
    name: 'Dedication',
    description: 'Practice for a total of 24 hours',
    icon: 'Clock',
    criteria: {
      type: 'practice_time',
      threshold: 24,
      comparison: 'gte'
    },
    rarity: 'epic'
  },
  {
    id: 'flawless_victory',
    name: 'Flawless Victory',
    description: 'Complete an advanced lesson with 100% accuracy and 100+ WPM',
    icon: 'Star',
    criteria: {
      type: 'perfect_score',
      threshold: 1,
      comparison: 'gte'
    },
    rarity: 'legendary'
  }
];
export class AchievementManager {
  private supabase: SupabaseClient;
  private userId: string;
  private toast: ReturnType<typeof useToast>['toast'];
  constructor(supabase: SupabaseClient, userId: string, toast: ReturnType<typeof useToast>['toast']) {
    this.supabase = supabase;
    this.userId = userId;
    this.toast = toast;
  }
  private getIconComponent(iconName: keyof typeof ACHIEVEMENT_ICONS): LucideIcon {
    return ACHIEVEMENT_ICONS[iconName];
  }
  private async checkAchievementCriteria(
    criteria: AchievementCriteria,
    stats: {
      wpm: number;
      accuracy: number;
      lessons: number;
      streak: number;
      practiceTime: number;
      isPerfectScore: boolean;
    }
  ): Promise<boolean> {
    const value = criteria.type === 'perfect_score'
      ? stats.isPerfectScore
      : stats[criteria.type === 'practice_time' ? 'practiceTime' : criteria.type];
    if (typeof value === 'boolean') {
      return value;
    }
    switch (criteria.comparison) {
      case 'gte':
        return value >= criteria.threshold;
      case 'eq':
        return value === criteria.threshold;
      case 'lte':
        return value <= criteria.threshold;
      default:
        return false;
    }
  }
  public async checkAndUnlockAchievements(stats: {
    wpm: number;
    accuracy: number;
    lessons: number;
    streak: number;
    practiceTime: number;
    isPerfectScore: boolean;
  }): Promise<void> {
    try {
      // Get already unlocked achievements
      const { data: unlockedAchievements } = await this.supabase
        .from('user_achievements')
        .select('achievement_id')
        .eq('user_id', this.userId);
      const unlockedIds = new Set(unlockedAchievements?.map(ua => ua.achievement_id) || []);
      // Check each achievement
      for (const achievement of ACHIEVEMENTS) {
        if (unlockedIds.has(achievement.id)) continue;
        const isUnlocked = await this.checkAchievementCriteria(achievement.criteria, stats);
        if (isUnlocked) {
          // Unlock the achievement
          const { error } = await this.supabase
            .from('user_achievements')
            .insert({
              user_id: this.userId,
              achievement_id: achievement.id,
              unlocked_at: new Date().toISOString()
            });
          if (error) throw error;
          // Show notification
          const IconComponent = this.getIconComponent(achievement.icon);
          this.toast({
            title: '🏆 Achievement Unlocked!',
            description: createElement(AchievementNotification, {
              name: achievement.name,
              description: achievement.description,
              icon: IconComponent,
              rarity: achievement.rarity
            }),
            duration: 5000,
          });
        }
      }
    } catch (error) {
      console.error('Error checking achievements:', error);
      this.toast({
        title: 'Error',
        description: 'Failed to check achievements',
        variant: 'destructive',
      });
    }
  }
  public async getProgress(): Promise<AchievementProgress[]> {
    try {
      // Get user stats
      const { data: stats, error: statsError } = await this.supabase
        .from('profiles')
        .select('average_wpm, average_accuracy, total_lessons_completed, total_practice_time')
        .eq('id', this.userId)
        .single();
      if (statsError) throw statsError;
      // Get unlocked achievements
      const { data: unlockedAchievements, error: achievementsError } = await this.supabase
        .from('user_achievements')
        .select('achievement_id, unlocked_at')
        .eq('user_id', this.userId);
      if (achievementsError) throw achievementsError;
      const unlockedMap = new Map(
        unlockedAchievements?.map(ua => [ua.achievement_id, ua.unlocked_at]) || []
      );
      // Calculate progress for each achievement
      return ACHIEVEMENTS.map(achievement => {
        const unlockedAt = unlockedMap.get(achievement.id);
        let progress = 0;
        switch (achievement.criteria.type) {
          case 'wpm':
            progress = (stats.average_wpm / achievement.criteria.threshold) * 100;
            break;
          case 'accuracy':
            progress = (stats.average_accuracy / achievement.criteria.threshold) * 100;
            break;
          case 'lessons':
            progress = (stats.total_lessons_completed / achievement.criteria.threshold) * 100;
            break;
          case 'practice_time':
            progress = ((stats.total_practice_time / 60) / achievement.criteria.threshold) * 100;
            break;
          default:
            progress = unlockedAt ? 100 : 0;
        }
        return {
          achievementId: achievement.id,
          progress: Math.min(Math.round(progress), 100),
          isUnlocked: !!unlockedAt,
          unlockedAt
        };
      });
    } catch (error) {
      console.error('Error getting achievement progress:', error);
      throw error;
    }
  }
}
// Export a function to create the achievement manager
export function createAchievementManager(
  supabase: SupabaseClient,
  userId: string,
  toast: ReturnType<typeof useToast>['toast']
): AchievementManager {
  return new AchievementManager(supabase, userId, toast);
}
</file>

<file path="src/lib/ai-core.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: AI Core                                                                                ║
 * ║ Description: Core AI module providing real-time tutoring capabilities using Gemini 2.0 Flash   ║
 * ║              and OpenAI, with streaming responses and adaptive learning features.              ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import OpenAI from 'openai';
import { SupabaseClient } from '@supabase/supabase-js';
import {
  GoogleGenerativeAI,
  GenerativeModel,
  GenerateContentResult,
  GenerateContentStreamResult
} from '@google/generative-ai';
import { getConfig, Config } from './config';
export interface AIConfig {
  model: 'gpt-4' | 'gpt-3.5-turbo' | 'gemini-pro' | 'gemini-2.0-flash';
  temperature: number;
  maxTokens: number;
  streamCallback?: (text: string) => void;
}
export interface TutorResponse {
  feedback: string;
  nextLesson?: string;
  adaptiveHints?: string[];
  confidenceScore?: number;
}
export interface TypingAnalysis {
  errorPatterns: Array<{
    pattern: string;
    frequency: number;
    suggestion: string;
  }>;
  speedTrends: Array<{
    timestamp: string;
    wpm: number;
  }>;
  recommendedFocus: string[];
}
export class AICore {
  private openai: OpenAI;
  private genAI: GoogleGenerativeAI;
  private geminiModel: GenerativeModel;
  private supabase: SupabaseClient;
  private config: Config;
  private aiConfig: AIConfig;
  constructor(supabase: SupabaseClient, aiConfig: AIConfig) {
    this.config = getConfig();
    this.aiConfig = aiConfig;
    this.openai = new OpenAI({ apiKey: this.config.openaiApiKey });
    this.genAI = new GoogleGenerativeAI(this.config.googleAiApiKey);
    this.geminiModel = this.genAI.getGenerativeModel({
      model: aiConfig.model.startsWith('gemini') ? aiConfig.model : 'gemini-pro'
    });
    this.supabase = supabase;
    // Log initialization for debugging
    console.info('Initializing AI Core with config:', {
      model: aiConfig.model,
      temperature: aiConfig.temperature,
      maxTokens: aiConfig.maxTokens,
      hasStreamCallback: !!aiConfig.streamCallback,
      enableRealTimeAnalysis: this.config.enableRealTimeAnalysis,
      enableVoiceCommands: this.config.enableVoiceCommands
    });
  }
  async generateLessonContent(
    userLevel: number,
    previousMistakes: string[],
    learningStyle: string
  ): Promise<string> {
    if (!this.config.enableRealTimeAnalysis) {
      console.warn('Real-time analysis is disabled. Using fallback content generation.');
      return this.generateFallbackContent(userLevel);
    }
    console.info('Generating adaptive lesson content:', {
      level: userLevel,
      mistakeCount: previousMistakes.length,
      style: learningStyle
    });
    try {
      if (this.config.aiModel.startsWith('gemini')) {
        const prompt = `Generate a typing lesson for a user at level ${userLevel}.
                       Previous mistakes: ${previousMistakes.join(', ')}.
                       Learning style: ${learningStyle}`;
        if (this.aiConfig.streamCallback) {
          const streamResult = await this.geminiModel.generateContentStream(prompt);
          let fullContent = '';
          for await (const chunk of streamResult.stream) {
            const chunkText = chunk.text();
            fullContent += chunkText;
            this.aiConfig.streamCallback(chunkText);
          }
          return fullContent;
        } else {
          const result = await this.geminiModel.generateContent(prompt);
          return result.response.text();
        }
      } else {
        const response = await this.openai.chat.completions.create({
          model: this.config.aiModel,
          messages: [
            {
              role: 'system',
              content: 'You are an expert typing tutor focused on accessibility.'
            },
            {
              role: 'user',
              content: `Generate a typing lesson for a user at level ${userLevel}.
                       Previous mistakes: ${previousMistakes.join(', ')}.
                       Learning style: ${learningStyle}`
            }
          ],
          temperature: this.config.aiTemperature,
          max_tokens: this.config.aiMaxTokens,
          stream: !!this.aiConfig.streamCallback
        });
        if (this.aiConfig.streamCallback && response.hasOwnProperty('on')) {
          const stream = response as any;
          let fullContent = '';
          await new Promise((resolve, reject) => {
            stream.on('data', (chunk: any) => {
              const content = chunk.choices[0]?.delta?.content || '';
              fullContent += content;
              this.aiConfig.streamCallback?.(content);
            });
            stream.on('end', resolve);
            stream.on('error', reject);
          });
          return fullContent;
        } else {
          return (response as OpenAI.Chat.Completions.ChatCompletion).choices[0]?.message?.content || '';
        }
      }
    } catch (error) {
      console.error('Error generating lesson content:', error);
      return this.generateFallbackContent(userLevel);
    }
  }
  private generateFallbackContent(userLevel: number): string {
    const basicLessons = [
      'Practice typing home row keys: asdf jkl;',
      'Type common words: the, and, that, have',
      'Practice numbers and symbols: 1234567890',
      'Focus on capital letters and punctuation',
      'Type complete sentences with proper form'
    ];
    return basicLessons[Math.min(userLevel - 1, basicLessons.length - 1)];
  }
  async provideTutoring(
    currentText: string,
    targetText: string,
    recentMistakes: Array<{ actual: string; expected: string }>,
    userLevel: number
  ): Promise<TutorResponse> {
    if (!this.config.enableRealTimeAnalysis) {
      return {
        feedback: 'Real-time analysis is currently disabled.',
        confidenceScore: 0.5
      };
    }
    console.info('Providing real-time tutoring feedback:', {
      textLength: currentText.length,
      targetLength: targetText.length,
      mistakeCount: recentMistakes.length,
      level: userLevel
    });
    try {
      const prompt = `As a typing tutor, analyze:
                     Current text: "${currentText}"
                     Target text: "${targetText}"
                     Recent mistakes: ${JSON.stringify(recentMistakes)}
                     User level: ${userLevel}
                     Provide feedback in JSON format with:
                     - feedback: Main feedback message
                     - nextLesson: Suggested next lesson content
                     - adaptiveHints: Array of specific hints
                     - confidenceScore: Number between 0-1`;
      if (this.aiConfig.streamCallback) {
        const result = await this.geminiModel.generateContentStream(prompt);
        let fullResponse = '';
        for await (const chunk of result.stream) {
          const chunkText = chunk.text();
          fullResponse += chunkText;
          this.aiConfig.streamCallback(chunkText);
        }
        return JSON.parse(fullResponse);
      } else {
        const result = await this.geminiModel.generateContent(prompt);
        return JSON.parse(result.response.text());
      }
    } catch (error) {
      console.error('Error providing tutoring:', error);
      return {
        feedback: "I'm having trouble analyzing your typing. Let's continue with the current lesson.",
        confidenceScore: 0.5
      };
    }
  }
  async analyzeTypingPatterns(
    recentMistakes: Array<{ actual: string; expected: string }>,
    speedHistory: number[]
  ): Promise<TypingAnalysis> {
    if (!this.config.enableRealTimeAnalysis) {
      return {
        errorPatterns: [],
        speedTrends: [],
        recommendedFocus: ['Real-time analysis is currently disabled.']
      };
    }
    console.info('Analyzing typing patterns:', {
      mistakeCount: recentMistakes.length,
      speedDataPoints: speedHistory.length
    });
    try {
      const prompt = `Analyze typing patterns:
                     Mistakes: ${JSON.stringify(recentMistakes)}
                     Speed history (WPM): ${speedHistory.join(', ')}
                     Provide analysis in JSON format with:
                     - errorPatterns: Array of {pattern, frequency, suggestion}
                     - speedTrends: Array of {timestamp, wpm}
                     - recommendedFocus: Array of focus areas`;
      const result = await this.geminiModel.generateContent(prompt);
      const analysis: TypingAnalysis = JSON.parse(result.response.text());
      // Store analysis in Supabase for long-term pattern recognition
      if (this.config.enablePerformanceTracking) {
        await this.supabase
          .from('typing_analysis')
          .insert([{
            error_patterns: analysis.errorPatterns,
            speed_trends: analysis.speedTrends,
            recommended_focus: analysis.recommendedFocus,
            created_at: new Date().toISOString()
          }]);
      }
      return analysis;
    } catch (error) {
      console.error('Error analyzing typing patterns:', error);
      return {
        errorPatterns: [],
        speedTrends: [],
        recommendedFocus: ['Focus on accuracy and consistent speed']
      };
    }
  }
  // Utility method to update AI configuration
  updateConfig(newConfig: Partial<AIConfig>): void {
    this.config = { ...this.config, ...newConfig };
    console.info('Updated AI configuration:', {
      aiModel: this.config.aiModel,
      aiTemperature: this.config.aiTemperature,
      aiMaxTokens: this.config.aiMaxTokens,
      enableRealTimeAnalysis: this.config.enableRealTimeAnalysis
    });
  }
}
</file>

<file path="src/lib/ai-utils.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: AI Utilities                                                                           ║
 * ║ Description: Utility functions for initializing and managing AI components                     ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { SupabaseClient } from '@supabase/supabase-js';
import { AICore, AIConfig, TutorResponse } from './ai-core';
import { getConfig } from './config';
// Singleton instance of AICore
let aiCoreInstance: AICore | null = null;
/**
 * Initialize the AI Core with proper configuration
 * @param supabase - Supabase client instance
 * @param streamCallback - Optional callback for streaming responses
 * @returns Initialized AICore instance
 */
export function initializeAICore(
  supabase: SupabaseClient,
  streamCallback?: (text: string) => void
): AICore {
  if (aiCoreInstance) {
    console.info('Reusing existing AI Core instance');
    return aiCoreInstance;
  }
  const config = getConfig();
  const aiConfig: AIConfig = {
    model: config.aiModel,
    temperature: config.aiTemperature,
    maxTokens: config.aiMaxTokens,
    streamCallback
  };
  console.info('Initializing new AI Core with config:', {
    model: aiConfig.model,
    temperature: aiConfig.temperature,
    maxTokens: aiConfig.maxTokens,
    hasStreamCallback: !!streamCallback
  });
  aiCoreInstance = new AICore(supabase, aiConfig);
  return aiCoreInstance;
}
/**
 * Reset the AI Core instance (useful for testing or reconfiguration)
 */
export function resetAICore(): void {
  aiCoreInstance = null;
  console.info('AI Core instance has been reset');
}
/**
 * Get the current AI Core instance, initializing if necessary
 * @param supabase - Supabase client instance
 * @param streamCallback - Optional callback for streaming responses
 * @returns AICore instance
 */
export function getAICore(
  supabase: SupabaseClient,
  streamCallback?: (text: string) => void
): AICore {
  return aiCoreInstance || initializeAICore(supabase, streamCallback);
}
/**
 * Format tutor response for display
 * @param response - Raw tutor response
 * @returns Formatted response object
 */
export function formatTutorResponse(response: TutorResponse): {
  message: string;
  hints: string[];
  confidence: number;
} {
  return {
    message: response.feedback,
    hints: response.adaptiveHints || [],
    confidence: response.confidenceScore || 0.5
  };
}
/**
 * Calculate typing accuracy percentage
 * @param mistakes - Array of typing mistakes
 * @param totalCharacters - Total characters typed
 * @returns Accuracy percentage
 */
export function calculateAccuracy(
  mistakes: Array<{ actual: string; expected: string }>,
  totalCharacters: number
): number {
  if (totalCharacters === 0) return 100;
  const errorCount = mistakes.length;
  return Math.round(((totalCharacters - errorCount) / totalCharacters) * 100);
}
/**
 * Calculate words per minute (WPM)
 * @param characterCount - Number of characters typed
 * @param timeInSeconds - Time taken in seconds
 * @returns WPM calculation
 */
export function calculateWPM(characterCount: number, timeInSeconds: number): number {
  if (timeInSeconds === 0) return 0;
  // Standard calculation: 5 characters = 1 word
  const wordsTyped = characterCount / 5;
  const minutes = timeInSeconds / 60;
  return Math.round(wordsTyped / minutes);
}
/**
 * Generate a typing challenge appropriate for the user's level
 * @param userLevel - Current user level (1-10)
 * @param previousMistakes - Array of previous mistakes
 * @returns Challenge text and metadata
 */
export async function generateChallenge(
  userLevel: number,
  previousMistakes: Array<{ actual: string; expected: string }> = []
): Promise<{
  text: string;
  difficulty: number;
  focusAreas: string[];
}> {
  // Fallback content if AI generation fails
  const fallbackChallenges = [
    { text: 'The quick brown fox jumps over the lazy dog.', difficulty: 1 },
    { text: 'Pack my box with five dozen liquor jugs.', difficulty: 2 },
    { text: 'How vexingly quick daft zebras jump!', difficulty: 3 },
    { text: 'The five boxing wizards jump quickly.', difficulty: 4 },
    { text: 'Sphinx of black quartz, judge my vow.', difficulty: 5 }
  ];
  try {
    const ai = aiCoreInstance;
    if (!ai) {
      throw new Error('AI Core not initialized');
    }
    const lessonContent = await ai.generateLessonContent(
      userLevel,
      previousMistakes.map(m => `${m.actual} -> ${m.expected}`),
      'adaptive'
    );
    return {
      text: lessonContent,
      difficulty: userLevel,
      focusAreas: previousMistakes.map(m => m.expected)
    };
  } catch (error) {
    console.error('Failed to generate challenge:', error);
    const fallback = fallbackChallenges[Math.min(userLevel - 1, fallbackChallenges.length - 1)];
    return {
      text: fallback.text,
      difficulty: fallback.difficulty,
      focusAreas: ['accuracy', 'speed']
    };
  }
}
</file>

<file path="src/lib/config.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Configuration                                                                          ║
 * ║ Description: Environment configuration loader and validator for DeepType                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { z } from 'zod';
// Configuration schema for type-safe environment variables
const configSchema = z.object({
  // AI Configuration
  openaiApiKey: z.string().min(1, 'OpenAI API key is required'),
  googleAiApiKey: z.string().min(1, 'Google AI API key is required'),
  aiModel: z.enum(['gpt-4', 'gpt-3.5-turbo', 'gemini-pro', 'gemini-2.0-flash']),
  aiTemperature: z.number().min(0).max(1),
  aiMaxTokens: z.number().positive(),
  // Supabase Configuration
  supabaseUrl: z.string().url('Invalid Supabase URL'),
  supabaseAnonKey: z.string().min(1, 'Supabase anonymous key is required'),
  // Feature Flags
  enableVoiceCommands: z.boolean(),
  enableHapticFeedback: z.boolean(),
  enableRealTimeAnalysis: z.boolean(),
  // Analytics Configuration
  enablePerformanceTracking: z.boolean(),
  enableErrorTracking: z.boolean(),
  analyticsSampleRate: z.number().min(0).max(100),
});
// Type inference from schema
export type Config = z.infer<typeof configSchema>;
// Configuration loader with validation
export function loadConfig(): Config {
  try {
    // Load and validate configuration
    const config = configSchema.parse({
      openaiApiKey: process.env.OPENAI_API_KEY,
      googleAiApiKey: process.env.GOOGLE_AI_API_KEY,
      aiModel: process.env.AI_MODEL || 'gemini-pro',
      aiTemperature: Number(process.env.AI_TEMPERATURE || 0.7),
      aiMaxTokens: Number(process.env.AI_MAX_TOKENS || 2048),
      supabaseUrl: process.env.NEXT_PUBLIC_SUPABASE_URL,
      supabaseAnonKey: process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY,
      enableVoiceCommands: process.env.ENABLE_VOICE_COMMANDS === 'true',
      enableHapticFeedback: process.env.ENABLE_HAPTIC_FEEDBACK === 'true',
      enableRealTimeAnalysis: process.env.ENABLE_REAL_TIME_ANALYSIS === 'true',
      enablePerformanceTracking: process.env.ENABLE_PERFORMANCE_TRACKING === 'true',
      enableErrorTracking: process.env.ENABLE_ERROR_TRACKING === 'true',
      analyticsSampleRate: Number(process.env.ANALYTICS_SAMPLE_RATE || 100),
    });
    // Log configuration for debugging (excluding sensitive values)
    console.info('Configuration loaded:', {
      aiModel: config.aiModel,
      aiTemperature: config.aiTemperature,
      aiMaxTokens: config.aiMaxTokens,
      enableVoiceCommands: config.enableVoiceCommands,
      enableHapticFeedback: config.enableHapticFeedback,
      enableRealTimeAnalysis: config.enableRealTimeAnalysis,
      enablePerformanceTracking: config.enablePerformanceTracking,
      enableErrorTracking: config.enableErrorTracking,
      analyticsSampleRate: config.analyticsSampleRate,
    });
    return config;
  } catch (error) {
    console.error('Configuration validation failed:', error);
    throw new Error('Failed to load configuration. Check your environment variables.');
  }
}
// Singleton instance for configuration
let config: Config | null = null;
// Get configuration, initializing if necessary
export function getConfig(): Config {
  if (!config) {
    config = loadConfig();
  }
  return config;
}
// Reset configuration (useful for testing)
export function resetConfig(): void {
  config = null;
}
// Validate specific configuration subset
export function validateConfig<T extends Partial<Config>>(
  partialConfig: T,
  schema: z.ZodType<T>
): T {
  return schema.parse(partialConfig);
}
</file>

<file path="src/lib/gemini-tutor.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Gemini Tutor                                                                           ║
 * ║ Description: Real-time AI tutoring using Gemini 2.0 Flash with multimodal capabilities        ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { GoogleGenerativeAI, GenerativeModel } from '@google/generative-ai';
import { getConfig } from './config';
interface TutorConfig {
  enableStreaming?: boolean;
  enableVoice?: boolean;
  enableScreenShare?: boolean;
  onStreamUpdate?: (text: string) => void;
  onError?: (error: Error) => void;
}
interface TutorFeedback {
  feedback: string;
  corrections: Array<{
    mistake: string;
    suggestion: string;
    explanation: string;
  }>;
  nextSteps: string[];
  confidenceScore: number;
}
interface PerformanceMetrics {
  wpm: number;
  accuracy: number;
  consistency: number;
  problemAreas: string[];
  recentMistakes: Array<{ actual: string; expected: string }>;
}
// Add ImageCapture type definition
declare global {
  class ImageCapture {
    constructor(track: MediaStreamTrack);
    grabFrame(): Promise<ImageBitmap>;
  }
}
export class GeminiTutor {
  private genAI: GoogleGenerativeAI;
  private model: GenerativeModel;
  private config: TutorConfig;
  private screenCapture: MediaStream | null = null;
  private mediaRecorder: MediaRecorder | null = null;
  private recordedChunks: Blob[] = [];
  constructor(config: TutorConfig = {}) {
    const { googleAiApiKey } = getConfig();
    this.genAI = new GoogleGenerativeAI(googleAiApiKey);
    this.model = this.genAI.getGenerativeModel({
      model: "gemini-2.0-flash",
      generationConfig: {
        temperature: 0.7,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 2048,
      },
    });
    this.config = config;
    // Initialize screen capture if enabled
    if (config.enableScreenShare) {
      this.initializeScreenCapture();
    }
  }
  private async initializeScreenCapture() {
    try {
      this.screenCapture = await navigator.mediaDevices.getDisplayMedia({
        video: {
          displaySurface: "monitor",
        },
        audio: false,
      });
      this.mediaRecorder = new MediaRecorder(this.screenCapture, {
        mimeType: 'video/webm;codecs=vp9',
      });
      this.mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          this.recordedChunks.push(event.data);
        }
      };
      this.mediaRecorder.start(1000); // Capture every second
    } catch (error) {
      console.error('Failed to initialize screen capture:', error);
      this.config.onError?.(error as Error);
    }
  }
  private async getScreenshot(): Promise<string | null> {
    if (!this.screenCapture) return null;
    const videoTrack = this.screenCapture.getVideoTracks()[0];
    const imageCapture = new ImageCapture(videoTrack);
    try {
      const bitmap = await imageCapture.grabFrame();
      const canvas = document.createElement('canvas');
      canvas.width = bitmap.width;
      canvas.height = bitmap.height;
      const context = canvas.getContext('2d');
      if (!context) return null;
      context.drawImage(bitmap, 0, 0);
      return canvas.toDataURL('image/jpeg', 0.8);
    } catch (error) {
      console.error('Failed to capture screenshot:', error);
      return null;
    }
  }
  public async provideFeedback(
    currentText: string,
    targetText: string,
    metrics: PerformanceMetrics
  ): Promise<TutorFeedback> {
    try {
      const screenshot = this.config.enableScreenShare ? await this.getScreenshot() : null;
      const prompt = `
        As a typing tutor, analyze:
        Current text: "${currentText}"
        Target text: "${targetText}"
        Performance Metrics:
        - WPM: ${metrics.wpm}
        - Accuracy: ${metrics.accuracy}%
        - Consistency: ${metrics.consistency}%
        - Recent Mistakes: ${JSON.stringify(metrics.recentMistakes)}
        - Problem Areas: ${metrics.problemAreas.join(', ')}
        ${screenshot ? 'I am also providing a screenshot of the user\'s typing interface for context.' : ''}
        Please provide:
        1. Specific feedback on typing technique and errors
        2. Detailed corrections for recent mistakes
        3. Actionable next steps for improvement
        4. A confidence score for this assessment
        Format the response as a JSON object with the following structure:
        {
          "feedback": "main feedback message",
          "corrections": [{"mistake": "...", "suggestion": "...", "explanation": "..."}],
          "nextSteps": ["step1", "step2", "..."],
          "confidenceScore": 0.95
        }
      `;
      const result = await this.model.generateContent({
        contents: [
          {
            role: "user",
            parts: [
              { text: prompt },
              ...(screenshot ? [{ inlineData: { data: screenshot, mimeType: "image/jpeg" } }] : []),
            ],
          },
        ],
      });
      const response = await result.response;
      const text = response.text();
      // Stream the response if streaming is enabled
      if (this.config.enableStreaming && this.config.onStreamUpdate) {
        this.config.onStreamUpdate(text);
      }
      try {
        const parsedResponse = JSON.parse(text) as TutorFeedback;
        return parsedResponse;
      } catch (error) {
        console.error('Failed to parse Gemini response:', error);
        return {
          feedback: text,
          corrections: [],
          nextSteps: [],
          confidenceScore: 0.5,
        };
      }
    } catch (error) {
      console.error('Error getting AI feedback:', error);
      this.config.onError?.(error as Error);
      return {
        feedback: "I'm having trouble analyzing your typing at the moment. Please continue practicing.",
        corrections: [],
        nextSteps: ["Focus on accuracy", "Maintain a steady rhythm"],
        confidenceScore: 0,
      };
    }
  }
  public async generateLesson(
    level: number,
    recentMistakes: Array<{ actual: string; expected: string }>,
    performanceHistory: Array<{ wpm: number; accuracy: number }>
  ): Promise<string> {
    try {
      const prompt = `
        Generate a typing lesson appropriate for:
        - Skill Level: ${level}/10
        - Recent Mistakes: ${JSON.stringify(recentMistakes)}
        - Performance History: ${JSON.stringify(performanceHistory)}
        Create a lesson that:
        1. Focuses on problem areas
        2. Gradually increases difficulty
        3. Includes common programming terms
        4. Maintains engagement
        The lesson should be 2-3 sentences long and include a mix of:
        - Common words
        - Problem characters
        - Programming concepts
      `;
      const result = await this.model.generateContent(prompt);
      const response = await result.response;
      return response.text();
    } catch (error) {
      console.error('Error generating lesson:', error);
      this.config.onError?.(error as Error);
      return "The quick brown fox jumps over the lazy dog. Practice makes perfect.";
    }
  }
  public cleanup() {
    if (this.screenCapture) {
      this.screenCapture.getTracks().forEach(track => track.stop());
      this.screenCapture = null;
    }
    if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
      this.mediaRecorder.stop();
      this.mediaRecorder = null;
    }
    this.recordedChunks = [];
  }
}
// Export a singleton instance for global use
export const geminiTutor = new GeminiTutor({
  enableStreaming: true,
  enableScreenShare: true,
  enableVoice: true,
  onStreamUpdate: (text) => console.info('AI Stream Update:', text),
  onError: (error) => console.error('AI Error:', error),
});
</file>

<file path="src/lib/utils.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Utilities                                                                              ║
 * ║ Description: Common utility functions for class name merging and type manipulation             ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"
/**
 * Combines multiple class names and merges Tailwind CSS classes efficiently
 * @param inputs - Class names to combine
 * @returns Merged class string
 */
export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}
/**
 * Type-safe wrapper for localStorage
 * @param key - Storage key
 * @param value - Value to store
 */
export function setLocalStorage<T>(key: string, value: T): void {
  try {
    localStorage.setItem(key, JSON.stringify(value))
  } catch (error) {
    console.error(`Error saving to localStorage key "${key}":`, error)
  }
}
/**
 * Type-safe getter for localStorage
 * @param key - Storage key
 * @param fallback - Default value if key doesn't exist
 * @returns Stored value or fallback
 */
export function getLocalStorage<T>(key: string, fallback: T): T {
  try {
    const item = localStorage.getItem(key)
    return item ? (JSON.parse(item) as T) : fallback
  } catch (error) {
    console.error(`Error reading localStorage key "${key}":`, error)
    return fallback
  }
}
/**
 * Formats a date string into a human-readable format
 * @param date - Date string or Date object
 * @param locale - Locale string (default: 'en-US')
 * @returns Formatted date string
 */
export function formatDate(date: string | Date, locale = 'en-US'): string {
  try {
    const dateObject = typeof date === 'string' ? new Date(date) : date
    return new Intl.DateTimeFormat(locale, {
      month: 'long',
      day: 'numeric',
      year: 'numeric',
    }).format(dateObject)
  } catch (error) {
    console.error('Error formatting date:', error)
    return String(date)
  }
}
/**
 * Format a number to a human-readable string with units
 * @param value - Number to format
 * @param unit - Unit to append
 * @param decimals - Number of decimal places
 * @returns Formatted number string
 */
export function formatNumber(
  value: number,
  unit: string = "",
  decimals: number = 0
): string {
  const formatter = new Intl.NumberFormat("en-US", {
    minimumFractionDigits: decimals,
    maximumFractionDigits: decimals,
  })
  return `${formatter.format(value)}${unit ? ` ${unit}` : ""}`
}
/**
 * Debounces a function call
 * @param fn - Function to debounce
 * @param delay - Delay in milliseconds
 * @returns Debounced function
 */
export function debounce<T extends (...args: any[]) => any>(
  fn: T,
  delay: number
): (...args: Parameters<T>) => void {
  let timeoutId: NodeJS.Timeout
  return (...args: Parameters<T>) => {
    clearTimeout(timeoutId)
    timeoutId = setTimeout(() => fn(...args), delay)
  }
}
/**
 * Throttle a function
 * @param fn - Function to throttle
 * @param limit - Time limit in milliseconds
 * @returns Throttled function
 */
export function throttle<T extends (...args: any[]) => any>(
  fn: T,
  limit: number
): (...args: Parameters<T>) => void {
  let inThrottle: boolean
  let lastResult: ReturnType<T>
  return function (...args: Parameters<T>): void {
    if (!inThrottle) {
      inThrottle = true
      lastResult = fn(...args)
      setTimeout(() => (inThrottle = false), limit)
    }
  }
}
/**
 * Creates a URL-friendly slug from a string
 * @param str - String to convert
 * @returns URL-friendly slug
 */
export function slugify(str: string): string {
  return str
    .toLowerCase()
    .trim()
    .replace(/[^\w\s-]/g, '')
    .replace(/[\s_-]+/g, '-')
    .replace(/^-+|-+$/g, '')
}
/**
 * Truncates a string to a specified length
 * @param str - String to truncate
 * @param length - Maximum length
 * @param ending - String to append at truncation point
 * @returns Truncated string
 */
export function truncate(str: string, length: number, ending = '...'): string {
  if (str.length > length) {
    return str.substring(0, length - ending.length) + ending
  }
  return str
}
/**
 * Generates a random string of specified length
 * @param length - Length of the string
 * @returns Random string
 */
export function generateId(length: number = 8): string {
  return Array.from(
    { length },
    () => Math.random().toString(36)[2]
  ).join('')
}
/**
 * Type guard to check if a value is not null or undefined
 * @param value - Value to check
 * @returns Boolean indicating if value is defined
 */
export function isDefined<T>(value: T | null | undefined): value is T {
  return value !== null && value !== undefined
}
/**
 * Safely access nested object properties
 * @param obj - Object to access
 * @param path - Path to property
 * @param fallback - Fallback value
 * @returns Property value or fallback
 */
export function get<T>(
  obj: any,
  path: string,
  fallback: T
): T {
  try {
    return path.split('.').reduce((acc, key) => acc[key], obj) ?? fallback
  } catch {
    return fallback
  }
}
/**
 * Deep clone an object
 * @param obj - Object to clone
 * @returns Cloned object
 */
export function deepClone<T>(obj: T): T {
  if (obj === null || typeof obj !== "object") return obj
  if (obj instanceof Date) return new Date(obj) as T
  if (obj instanceof Array) return obj.map(deepClone) as T
  if (obj instanceof Object) {
    const copy = {} as Record<string, any>
    for (const key in obj) {
      if (Object.prototype.hasOwnProperty.call(obj, key)) {
        copy[key] = deepClone(obj[key as keyof T])
      }
    }
    return copy as T
  }
  throw new Error(`Unable to copy obj! Its type isn't supported.`)
}
/**
 * Check if two objects are deeply equal
 * @param obj1 - First object
 * @param obj2 - Second object
 * @returns Whether the objects are equal
 */
export function deepEqual(obj1: any, obj2: any): boolean {
  if (obj1 === obj2) return true
  if (
    typeof obj1 !== "object" ||
    typeof obj2 !== "object" ||
    obj1 === null ||
    obj2 === null
  )
    return false
  const keys1 = Object.keys(obj1)
  const keys2 = Object.keys(obj2)
  if (keys1.length !== keys2.length) return false
  for (const key of keys1) {
    if (!keys2.includes(key)) return false
    if (!deepEqual(obj1[key], obj2[key])) return false
  }
  return true
}
/**
 * Convert a string to title case
 * @param str - String to convert
 * @returns Title case string
 */
export function toTitleCase(str: string): string {
  return str.replace(
    /\w\S*/g,
    (txt) => txt.charAt(0).toUpperCase() + txt.slice(1).toLowerCase()
  )
}
/**
 * Convert a string to kebab case
 * @param str - String to convert
 * @returns Kebab case string
 */
export function toKebabCase(str: string): string {
  return str
    .replace(/([a-z])([A-Z])/g, "$1-$2")
    .replace(/[\s_]+/g, "-")
    .toLowerCase()
}
/**
 * Convert a string to camel case
 * @param str - String to convert
 * @returns Camel case string
 */
export function toCamelCase(str: string): string {
  return str
    .replace(/(?:^\w|[A-Z]|\b\w)/g, (word, index) =>
      index === 0 ? word.toLowerCase() : word.toUpperCase()
    )
    .replace(/\s+/g, "")
}
</file>

<file path="src/routes/Auth.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Auth Page                                                                           ║
 * ║ Description: Authentication page with login, signup, and password reset functionality          ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import { useSupabase } from '@/hooks/useSupabase';
import AuthForm from '@/components/auth/AuthForm';
const Auth: React.FC = () => {
  const navigate = useNavigate();
  const { supabase } = useSupabase();
  useEffect(() => {
    // Check if user is already authenticated
    const checkAuth = async () => {
      const { data: { session } } = await supabase.auth.getSession();
      if (session) {
        navigate('/');
      }
    };
    checkAuth();
    // Listen for auth state changes
    const { data: { subscription } } = supabase.auth.onAuthStateChange((event, session) => {
      if (event === 'SIGNED_IN' && session) {
        navigate('/');
      }
    });
    return () => {
      subscription.unsubscribe();
    };
  }, [navigate, supabase.auth]);
  return (
    <div className="min-h-screen flex items-center justify-center p-4 bg-gradient-to-b from-background to-background/80">
      <div className="w-full max-w-md">
        <div className="text-center mb-8">
          <h1 className="text-4xl font-bold tracking-tight mb-2">DeepType</h1>
          <p className="text-muted-foreground">
            Master touch typing with AI-powered tutoring
          </p>
        </div>
        <AuthForm />
      </div>
    </div>
  );
};
export default Auth;
</file>

<file path="src/routes/Index.tsx">
import { useState, useEffect } from "react";
import { useNavigate } from "react-router-dom";
import AccessibleTypingTutor from "../components/AccessibleTypingTutor";
import { supabase } from "@/integrations/supabase/client";
import { User } from "@supabase/supabase-js";
const Index = () => {
  const navigate = useNavigate();
  const [user, setUser] = useState<User | null>(null);
  const [isLoading, setIsLoading] = useState(true);
  useEffect(() => {
    // Get initial session
    supabase.auth.getSession().then(({ data: { session } }) => {
      setUser(session?.user ?? null);
      setIsLoading(false);
      if (!session?.user) {
        navigate('/auth');
      }
    });
    // Listen for auth changes
    const {
      data: { subscription },
    } = supabase.auth.onAuthStateChange((_event, session) => {
      setUser(session?.user ?? null);
      // If we're on the index page and there's no session, redirect to auth
      if (!session?.user) {
        console.log("No session detected, redirecting to auth");
        navigate('/auth');
      }
    });
    return () => subscription.unsubscribe();
  }, [navigate]);
  if (isLoading) {
    return (
      <div className="min-h-screen bg-background flex items-center justify-center">
        <div className="animate-pulse text-primary">Loading...</div>
      </div>
    );
  }
  return (
    <div className="min-h-screen bg-background">
      <nav className="p-4 flex justify-between items-center">
        <h1 className="text-2xl font-bold">DeepType AI</h1>
        <div className="flex gap-4 items-center">
          {user ? (
            <>
              <span className="text-sm text-secondary">
                {user.email}
              </span>
              <button
                onClick={async () => {
                  await supabase.auth.signOut();
                  navigate('/auth');
                }}
                className="px-4 py-2 rounded-lg border border-primary/50 hover:bg-primary/10 transition-all duration-200"
                aria-label="Sign out"
              >
                Sign Out
              </button>
            </>
          ) : (
            <button
              onClick={() => navigate("/auth")}
              className="px-4 py-2 rounded-lg border border-primary/50 hover:bg-primary/10 transition-all duration-200"
              aria-label="Sign in to track your progress"
            >
              Sign In
            </button>
          )}
        </div>
      </nav>
      {user && <AccessibleTypingTutor />}
    </div>
  );
};
export default Index;
</file>

<file path="src/routes/Leaderboard.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Leaderboard Page                                                                    ║
 * ║ Description: Global leaderboard with filters and time periods                                  ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useState } from 'react';
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import LeaderboardTable from '@/components/leaderboard/LeaderboardTable';
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from '@/components/ui/select';
import { useLeaderboard, type TimePeriod, type SortBy, formatRank } from '@/hooks/useLeaderboard';
import { useToast } from '@/components/ui/use-toast';
import { Button } from '@/components/ui/button';
import { RefreshCcw } from 'lucide-react';
const Leaderboard: React.FC = () => {
  const [timePeriod, setTimePeriod] = useState<TimePeriod>('all');
  const [sortBy, setSortBy] = useState<SortBy>('wpm');
  const { toast } = useToast();
  const {
    currentUserRank,
    refetch,
  } = useLeaderboard({
    limit: 50,
    timePeriod,
    sortBy,
    onError: (error) => {
      toast({
        title: 'Error',
        description: error.message,
        variant: 'destructive',
      });
    },
  });
  const handleRefresh = async () => {
    try {
      await refetch();
      toast({
        title: 'Refreshed',
        description: 'Leaderboard data has been updated.',
      });
    } catch (error) {
      // Error is handled by the hook
    }
  };
  return (
    <div className="container mx-auto px-4 py-8">
      <Card>
        <CardHeader>
          <div className="flex items-center justify-between">
            <div>
              <CardTitle>Global Leaderboard</CardTitle>
              <CardDescription>
                {currentUserRank ? (
                  <>You are ranked {formatRank(currentUserRank)} globally</>
                ) : (
                  'See how you stack up against other typists'
                )}
              </CardDescription>
            </div>
            <div className="flex items-center gap-4">
              <Select
                value={timePeriod}
                onValueChange={(value) => setTimePeriod(value as TimePeriod)}
              >
                <SelectTrigger className="w-[140px]">
                  <SelectValue placeholder="Time Period" />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="all">All Time</SelectItem>
                  <SelectItem value="today">Today</SelectItem>
                  <SelectItem value="week">This Week</SelectItem>
                  <SelectItem value="month">This Month</SelectItem>
                </SelectContent>
              </Select>
              <Select
                value={sortBy}
                onValueChange={(value) => setSortBy(value as SortBy)}
              >
                <SelectTrigger className="w-[140px]">
                  <SelectValue placeholder="Sort By" />
                </SelectTrigger>
                <SelectContent>
                  <SelectItem value="wpm">WPM</SelectItem>
                  <SelectItem value="accuracy">Accuracy</SelectItem>
                  <SelectItem value="lessons">Lessons</SelectItem>
                  <SelectItem value="achievements">Achievements</SelectItem>
                </SelectContent>
              </Select>
              <Button
                variant="outline"
                size="icon"
                onClick={handleRefresh}
              >
                <RefreshCcw className="h-4 w-4" />
              </Button>
            </div>
          </div>
        </CardHeader>
        <CardContent>
          <Tabs defaultValue="global" className="space-y-4">
            <TabsList>
              <TabsTrigger value="global">Global</TabsTrigger>
              <TabsTrigger value="friends">Friends</TabsTrigger>
              <TabsTrigger value="local">Your Region</TabsTrigger>
            </TabsList>
            <TabsContent value="global" className="space-y-4">
              <LeaderboardTable
                limit={50}
                timePeriod={timePeriod}
                sortBy={sortBy}
                className="animate-in fade-in-50 slide-in-from-bottom-5"
              />
            </TabsContent>
            <TabsContent value="friends" className="space-y-4">
              <div className="flex flex-col items-center justify-center h-[400px] text-muted-foreground">
                <p className="text-lg font-medium mb-2">Coming Soon</p>
                <p className="text-sm">
                  Challenge your friends and track your progress together
                </p>
              </div>
            </TabsContent>
            <TabsContent value="local" className="space-y-4">
              <div className="flex flex-col items-center justify-center h-[400px] text-muted-foreground">
                <p className="text-lg font-medium mb-2">Coming Soon</p>
                <p className="text-sm">
                  Compete with typists in your region
                </p>
              </div>
            </TabsContent>
          </Tabs>
        </CardContent>
      </Card>
    </div>
  );
};
export default Leaderboard;
</file>

<file path="src/routes/NotFound.tsx">
import { useLocation } from "react-router-dom";
import { useEffect } from "react";
const NotFound = () => {
  const location = useLocation();
  useEffect(() => {
    console.error(
      "404 Error: User attempted to access non-existent route:",
      location.pathname
    );
  }, [location.pathname]);
  return (
    <div className="min-h-screen flex items-center justify-center bg-gray-100">
      <div className="text-center">
        <h1 className="text-4xl font-bold mb-4">404</h1>
        <p className="text-xl text-gray-600 mb-4">Oops! Page not found</p>
        <a href="/" className="text-blue-500 hover:text-blue-700 underline">
          Return to Home
        </a>
      </div>
    </div>
  );
};
export default NotFound;
</file>

<file path="src/routes/Profile.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: Profile Page                                                                        ║
 * ║ Description: User profile page displaying stats, achievements, and typing progress             ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import { useQuery } from '@tanstack/react-query';
import { useSupabase } from '@/hooks/useSupabase';
import { Card, CardContent, CardHeader, CardTitle, CardDescription } from '@/components/ui/card';
import { Progress } from '@/components/ui/progress';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Spinner } from '@/components/ui/spinner';
import { Badge } from '@/components/ui/badge';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import { useAchievements } from '@/hooks/useAchievements';
import AchievementGrid from '@/components/achievements/AchievementGrid';
interface Profile {
  id: string;
  username: string;
  full_name: string | null;
  avatar_url: string | null;
  level: number;
  total_lessons_completed: number;
  total_practice_time: number;
  average_wpm: number;
  average_accuracy: number;
}
interface Achievement {
  id: string;
  name: string;
  description: string;
  icon: string;
}
interface SupabaseUserAchievement {
  achievements: Achievement;
  unlocked_at: string;
}
interface AchievementWithUnlockDate extends Achievement {
  unlocked_at: string;
}
const Profile: React.FC = () => {
  const { supabase } = useSupabase();
  const { achievements, isLoading: isAchievementsLoading } = useAchievements();
  // Fetch user profile data
  const { data: profile, isLoading: isProfileLoading } = useQuery({
    queryKey: ['profile'],
    queryFn: async () => {
      const { data: { user } } = await supabase.auth.getUser();
      if (!user) throw new Error('No user found');
      const { data, error } = await supabase
        .from('profiles')
        .select('*')
        .eq('id', user.id)
        .single();
      if (error) throw error;
      return data as Profile;
    },
  });
  if (isProfileLoading || isAchievementsLoading) {
    return (
      <div className="min-h-screen flex items-center justify-center">
        <Spinner size="lg" />
      </div>
    );
  }
  if (!profile) {
    return (
      <div className="min-h-screen flex items-center justify-center">
        <p className="text-muted-foreground">Profile not found</p>
      </div>
    );
  }
  return (
    <div className="container mx-auto px-4 py-8">
      {/* Profile Header */}
      <div className="flex items-center gap-6 mb-8">
        <Avatar className="w-24 h-24">
          <AvatarImage src={profile.avatar_url || undefined} />
          <AvatarFallback>
            {profile.username?.slice(0, 2).toUpperCase() || 'U'}
          </AvatarFallback>
        </Avatar>
        <div>
          <h1 className="text-3xl font-bold">{profile.full_name || profile.username}</h1>
          <p className="text-muted-foreground">Level {profile.level} Typist</p>
        </div>
      </div>
      {/* Stats Grid */}
      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4 mb-8">
        <Card>
          <CardHeader className="pb-2">
            <CardTitle className="text-sm font-medium">Average WPM</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{profile.average_wpm}</div>
            <Progress value={profile.average_wpm} max={100} className="mt-2" />
          </CardContent>
        </Card>
        <Card>
          <CardHeader className="pb-2">
            <CardTitle className="text-sm font-medium">Accuracy</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{profile.average_accuracy}%</div>
            <Progress value={profile.average_accuracy} max={100} className="mt-2" />
          </CardContent>
        </Card>
        <Card>
          <CardHeader className="pb-2">
            <CardTitle className="text-sm font-medium">Lessons Completed</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">{profile.total_lessons_completed}</div>
          </CardContent>
        </Card>
        <Card>
          <CardHeader className="pb-2">
            <CardTitle className="text-sm font-medium">Practice Time</CardTitle>
          </CardHeader>
          <CardContent>
            <div className="text-2xl font-bold">
              {Math.round(profile.total_practice_time / 60)}h
            </div>
          </CardContent>
        </Card>
      </div>
      {/* Achievements */}
      <Card className="mb-8">
        <CardHeader>
          <CardTitle>Achievements</CardTitle>
          <CardDescription>
            Track your progress and unlock achievements as you improve
          </CardDescription>
        </CardHeader>
        <CardContent>
          <AchievementGrid progress={achievements} />
        </CardContent>
      </Card>
    </div>
  );
};
export default Profile;
</file>

<file path="src/routes/TypingTutor.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: TypingTutor Page                                                                    ║
 * ║ Description: Main typing tutor page with real-time feedback and AI-powered tutoring           ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React, { useState, useEffect, useCallback, useRef } from 'react';
import { useAI } from '@/hooks/useAI';
import { useSupabase } from '@/hooks/useSupabase';
import { AccessibilityCore } from '@/lib/accessibility-core';
import VisualKeyboard from '@/components/VisualKeyboard';
import { useToast } from '@/components/ui/use-toast';
import { calculateWPM, calculateAccuracy } from '@/lib/ai-utils';
import {
  Card,
  CardContent,
  CardDescription,
  CardFooter,
  CardHeader,
  CardTitle,
} from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Progress } from '@/components/ui/progress';
import { Badge } from '@/components/ui/badge';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { LineChart, Line, XAxis, YAxis, Tooltip, ResponsiveContainer } from 'recharts';
import { Spinner } from '@/components/ui/spinner';
import { geminiTutor } from '@/lib/gemini-tutor';
interface TypingStats {
  wpm: number;
  accuracy: number;
  mistakes: Array<{ actual: string; expected: string }>;
  timestamp: string;
}
const TypingTutor: React.FC = () => {
  // State management
  const [currentText, setCurrentText] = useState('');
  const [targetText, setTargetText] = useState('');
  const [level, setLevel] = useState(1);
  const [isStarted, setIsStarted] = useState(false);
  const [pressedKey, setPressedKey] = useState<string | null>(null);
  const [nextKey, setNextKey] = useState<string | null>(null);
  const [stats, setStats] = useState<TypingStats[]>([]);
  const [mistakes, setMistakes] = useState<Array<{ actual: string; expected: string }>>([]);
  const [startTime, setStartTime] = useState<number | null>(null);
  const [feedback, setFeedback] = useState<{
    message: string;
    corrections: Array<{ mistake: string; suggestion: string; explanation: string }>;
    nextSteps: string[];
  } | null>(null);
  const [problemAreas, setProblemAreas] = useState<string[]>([]);
  const [consistency, setConsistency] = useState(100);
  // Refs
  const inputRef = useRef<HTMLInputElement>(null);
  const accessibilityRef = useRef<AccessibilityCore | null>(null);
  // Hooks
  const { supabase } = useSupabase();
  const { toast } = useToast();
  const { generateLesson, getTutoring, analyzePatterns, isLoading } = useAI({
    enableStreaming: true,
    onStreamUpdate: (text) => {
      console.info('AI Stream Update:', text);
    },
  });
  // Initialize accessibility features
  useEffect(() => {
    if (!accessibilityRef.current) {
      accessibilityRef.current = new AccessibilityCore({
        speechRate: 1,
        voiceURI: '',
        volume: 1,
        pitch: 1,
        hapticFeedback: true,
        autoCorrect: true,
        errorTolerance: 3,
        commandDelay: 1000,
      });
    }
    return () => {
      if (accessibilityRef.current) {
        accessibilityRef.current.stopListening();
      }
    };
  }, []);
  // Load initial lesson
  useEffect(() => {
    const loadLesson = async () => {
      try {
        const lesson = await geminiTutor.generateLesson(
          level,
          mistakes,
          stats.map(s => ({ wpm: s.wpm, accuracy: s.accuracy }))
        );
        setTargetText(lesson);
        setNextKey(lesson[0]?.toLowerCase() || null);
      } catch (error) {
        console.error('Failed to load lesson:', error);
        toast({
          title: 'Error',
          description: 'Failed to load lesson. Please try again.',
          variant: 'destructive',
        });
      }
    };
    loadLesson();
  }, [level, mistakes, stats, toast]);
  // Calculate current statistics
  const currentStats = useCallback(() => {
    if (!startTime) return { wpm: 0, accuracy: 100 };
    const timeElapsed = (Date.now() - startTime) / 1000;
    const wpm = calculateWPM(currentText.length, timeElapsed);
    const accuracy = calculateAccuracy(mistakes, currentText.length);
    return { wpm, accuracy };
  }, [startTime, currentText.length, mistakes]);
  // Handle keyboard input
  const handleKeyDown = useCallback(async (e: KeyboardEvent) => {
    if (!isStarted) {
      setIsStarted(true);
      setStartTime(Date.now());
    }
    setPressedKey(e.key);
    if (!startTime) {
      setStartTime(Date.now());
    }
    const expectedChar = targetText[currentText.length];
    if (e.key === expectedChar) {
      setCurrentText(prev => prev + e.key);
      setNextKey(targetText[currentText.length + 1]?.toLowerCase() || null);
      // Get real-time feedback every 5 characters
      if ((currentText.length + 1) % 5 === 0) {
        try {
          const { wpm, accuracy } = currentStats();
          const tutorFeedback = await geminiTutor.provideFeedback(
            currentText + e.key,
            targetText,
            {
              wpm,
              accuracy,
              consistency,
              problemAreas,
              recentMistakes: mistakes
            }
          );
          setFeedback({
            message: tutorFeedback.feedback,
            corrections: tutorFeedback.corrections,
            nextSteps: tutorFeedback.nextSteps
          });
          // Update problem areas based on AI analysis
          if (tutorFeedback.corrections.length > 0) {
            setProblemAreas(prev => [
              ...new Set([
                ...prev,
                ...tutorFeedback.corrections.map(c => c.mistake)
              ])
            ]);
          }
        } catch (error) {
          console.error('Error getting real-time feedback:', error);
        }
      }
    } else {
      setMistakes(prev => [...prev, { actual: e.key, expected: expectedChar }]);
      accessibilityRef.current?.provideFeedback('Incorrect key', { haptic: true });
      // Update consistency score
      setConsistency(prev => Math.max(0, prev - 2));
    }
  }, [isStarted, startTime, targetText, currentText, currentStats, mistakes, consistency, problemAreas]);
  const handleKeyUp = useCallback(() => {
    setPressedKey(null);
  }, []);
  // Add keyboard event listeners
  useEffect(() => {
    window.addEventListener('keydown', handleKeyDown);
    window.addEventListener('keyup', handleKeyUp);
    return () => {
      window.removeEventListener('keydown', handleKeyDown);
      window.removeEventListener('keyup', handleKeyUp);
    };
  }, [handleKeyDown, handleKeyUp]);
  // Update stats when lesson is completed
  useEffect(() => {
    if (currentText.length === targetText.length && targetText.length > 0) {
      const { wpm, accuracy } = currentStats();
      setStats(prev => [...prev, {
        wpm,
        accuracy,
        mistakes,
        timestamp: new Date().toISOString()
      }]);
      // Save progress to Supabase
      const saveProgress = async () => {
        try {
          const { error } = await supabase
            .from('typing_progress')
            .insert([{
              wpm,
              accuracy,
              mistakes: JSON.stringify(mistakes),
              level,
              completed_at: new Date().toISOString()
            }]);
          if (error) throw error;
          // Get comprehensive feedback from Gemini
          const tutorFeedback = await geminiTutor.provideFeedback(
            currentText,
            targetText,
            {
              wpm,
              accuracy,
              consistency,
              problemAreas,
              recentMistakes: mistakes
            }
          );
          setFeedback({
            message: tutorFeedback.feedback,
            corrections: tutorFeedback.corrections,
            nextSteps: tutorFeedback.nextSteps
          });
          toast({
            title: 'Lesson Complete!',
            description: `WPM: ${wpm} | Accuracy: ${accuracy}%\n${tutorFeedback.feedback}`,
          });
          // Advance level if performance is good
          if (wpm >= 30 && accuracy >= 90 && tutorFeedback.confidenceScore > 0.8) {
            setLevel(prev => prev + 1);
          }
        } catch (error) {
          console.error('Failed to save progress:', error);
        }
      };
      saveProgress();
    }
  }, [currentText, targetText, currentStats, mistakes, level, stats, supabase, consistency, problemAreas]);
  // Cleanup Gemini tutor on unmount
  useEffect(() => {
    return () => {
      geminiTutor.cleanup();
    };
  }, []);
  return (
    <div className="container mx-auto px-4 py-8 min-h-screen bg-gradient-to-b from-background to-background/80">
      <Card className="w-full max-w-4xl mx-auto">
        <CardHeader>
          <CardTitle className="text-3xl font-bold tracking-tight">DeepType Tutor</CardTitle>
          <CardDescription>Level {level} - AI-Powered Typing Practice</CardDescription>
          <div className="flex gap-2 mt-2">
            <Badge variant="outline">WPM: {currentStats().wpm}</Badge>
            <Badge variant="outline">Accuracy: {currentStats().accuracy}%</Badge>
          </div>
        </CardHeader>
        <CardContent>
          <div className="space-y-8">
            {isLoading ? (
              <div className="flex items-center justify-center h-32">
                <Spinner size="lg" label="Loading lesson..." />
              </div>
            ) : (
              <>
                {/* Target Text Display */}
                <div className="relative rounded-lg bg-muted p-4 font-mono text-lg">
                  {targetText.split('').map((char, index) => (
                    <span
                      key={index}
                      className={`${
                        index < currentText.length
                          ? currentText[index] === targetText[index]
                            ? 'text-green-500'
                            : 'text-red-500'
                          : index === currentText.length
                          ? 'bg-primary/20 animate-pulse'
                          : 'text-muted-foreground'
                      }`}
                    >
                      {char}
                    </span>
                  ))}
                </div>
                {/* Progress Bar */}
                <Progress
                  value={(currentText.length / targetText.length) * 100}
                  className="h-2"
                />
                {/* Visual Keyboard */}
                <VisualKeyboard
                  pressedKey={pressedKey}
                  nextKey={nextKey}
                />
                {/* AI Feedback Panel */}
                {feedback && (
                  <div className="mt-4 p-4 rounded-lg bg-primary/5 border border-primary/20">
                    <h3 className="text-lg font-semibold mb-2">AI Tutor Feedback</h3>
                    <p className="text-primary mb-4">{feedback.message}</p>
                    {feedback.corrections.length > 0 && (
                      <div className="mb-4">
                        <h4 className="font-medium mb-2">Corrections:</h4>
                        <ul className="space-y-2">
                          {feedback.corrections.map((correction, index) => (
                            <li key={index} className="text-sm">
                              <span className="text-destructive">{correction.mistake}</span>
                              {' → '}
                              <span className="text-green-500">{correction.suggestion}</span>
                              <p className="text-muted-foreground mt-1">{correction.explanation}</p>
                            </li>
                          ))}
                        </ul>
                      </div>
                    )}
                    {feedback.nextSteps.length > 0 && (
                      <div>
                        <h4 className="font-medium mb-2">Next Steps:</h4>
                        <ul className="list-disc list-inside space-y-1">
                          {feedback.nextSteps.map((step, index) => (
                            <li key={index} className="text-sm text-muted-foreground">
                              {step}
                            </li>
                          ))}
                        </ul>
                      </div>
                    )}
                  </div>
                )}
              </>
            )}
          </div>
        </CardContent>
        <CardFooter className="flex justify-between">
          <Button
            variant="outline"
            onClick={() => {
              setCurrentText('');
              setStartTime(null);
              setMistakes([]);
              inputRef.current?.focus();
            }}
          >
            Reset Lesson
          </Button>
          <Button
            variant="default"
            onClick={() => accessibilityRef.current?.startListening()}
          >
            Enable Voice Commands
          </Button>
        </CardFooter>
      </Card>
      {/* Performance Charts */}
      <Card className="w-full max-w-4xl mx-auto mt-8">
        <CardHeader>
          <CardTitle>Performance Analytics</CardTitle>
        </CardHeader>
        <CardContent>
          <Tabs defaultValue="wpm">
            <TabsList>
              <TabsTrigger value="wpm">WPM Over Time</TabsTrigger>
              <TabsTrigger value="accuracy">Accuracy Trends</TabsTrigger>
            </TabsList>
            <TabsContent value="wpm">
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={stats}>
                  <XAxis
                    dataKey="timestamp"
                    tickFormatter={(value) => new Date(value).toLocaleTimeString()}
                  />
                  <YAxis />
                  <Tooltip
                    labelFormatter={(value) => new Date(value).toLocaleString()}
                  />
                  <Line
                    type="monotone"
                    dataKey="wpm"
                    stroke="#10b981"
                    strokeWidth={2}
                  />
                </LineChart>
              </ResponsiveContainer>
            </TabsContent>
            <TabsContent value="accuracy">
              <ResponsiveContainer width="100%" height={300}>
                <LineChart data={stats}>
                  <XAxis
                    dataKey="timestamp"
                    tickFormatter={(value) => new Date(value).toLocaleTimeString()}
                  />
                  <YAxis domain={[0, 100]} />
                  <Tooltip
                    labelFormatter={(value) => new Date(value).toLocaleString()}
                  />
                  <Line
                    type="monotone"
                    dataKey="accuracy"
                    stroke="#3b82f6"
                    strokeWidth={2}
                  />
                </LineChart>
              </ResponsiveContainer>
            </TabsContent>
          </Tabs>
        </CardContent>
      </Card>
    </div>
  );
};
export default TypingTutor;
</file>

<file path="src/services/ai-tutor.service.ts">
/*
 *  █████╗ ██╗    ████████╗██╗   ██╗████████╗ ██████╗ ██████╗
 * ██╔══██╗██║    ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗
 * ███████║██║       ██║   ██║   ██║   ██║   ██║   ██║██████╔╝
 * ██╔══██║██║       ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗
 * ██║  ██║██║       ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║
 * ╚═╝  ╚═╝╚═╝       ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Service: AI Typing Tutor                                                                       ║
 * ║ Description: Core service for AI-powered typing instruction and feedback                       ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { IAIAdapter } from '../adapters/base.adapter';
import {
  Lesson,
  Analysis,
  TutorFeedback,
  FeedbackRequest,
  TypingSessionData,
  UserProfile
} from '../types/typing';
import {
  DEFAULT_AI_CONFIG,
  LESSON_CONFIG,
  ANALYSIS_CONFIG,
  FEEDBACK_CONFIG
} from '../config/ai.config';
/**
 * Service interface for AI-powered typing tutor
 */
export interface IAITutorService {
  generateLesson(userProfile: UserProfile): Promise<Lesson>;
  analyzePerformance(sessionData: TypingSessionData): Promise<Analysis>;
  provideFeedback(request: FeedbackRequest): Promise<TutorFeedback>;
}
/**
 * Implementation of AI-powered typing tutor service
 */
export class AITutorService implements IAITutorService {
  private adapter: IAIAdapter;
  private worker: Worker;
  constructor(adapter: IAIAdapter) {
    this.adapter = adapter;
    this.initializeService();
  }
  /**
   * Initialize the service and its dependencies
   */
  private async initializeService(): Promise<void> {
    console.log('Initializing AI Tutor Service...');
    try {
      // Initialize AI adapter
      await this.adapter.initialize(DEFAULT_AI_CONFIG);
      // Initialize error analysis worker
      this.worker = new Worker(
        new URL('../lib/typing/error-analysis.worker.ts', import.meta.url),
        { type: 'module' }
      );
      this.worker.onerror = (error) => {
        console.error('Error in analysis worker:', error);
      };
      console.log('AI Tutor Service initialized successfully');
    } catch (error) {
      console.error('Failed to initialize AI Tutor Service:', error);
      throw new Error('Service initialization failed');
    }
  }
  /**
   * Generate a personalized typing lesson
   */
  public async generateLesson(userProfile: UserProfile): Promise<Lesson> {
    console.log('Generating lesson for user:', userProfile.id);
    try {
      // Determine appropriate difficulty level
      const difficulty = this.calculateDifficulty(userProfile);
      // Get focus areas based on user's weak points
      const focusAreas = this.determineFocusAreas(userProfile);
      // Generate lesson using AI adapter
      const lesson = await this.adapter.generateLesson(
        userProfile.level,
        focusAreas,
        {
          difficulty,
          contentType: this.selectContentType(userProfile),
          duration: this.calculateLessonDuration(userProfile),
          includeExercises: true,
          adaptivePacing: true,
          thematicContent: this.getThematicContent(userProfile)
        }
      );
      console.log('Lesson generated successfully:', { lessonId: lesson.id });
      return lesson;
    } catch (error) {
      console.error('Failed to generate lesson:', error);
      return this.getFallbackLesson(userProfile);
    }
  }
  /**
   * Analyze typing performance using web worker
   */
  public async analyzePerformance(sessionData: TypingSessionData): Promise<Analysis> {
    console.log('Analyzing performance for session:', sessionData.context.lessonId);
    return new Promise((resolve, reject) => {
      try {
        // Set up worker message handling
        this.worker.onmessage = (event) => {
          if (event.data.type === 'ANALYSIS_COMPLETE') {
            console.log('Analysis completed successfully');
            resolve(event.data.payload);
          } else if (event.data.type === 'ANALYSIS_ERROR') {
            console.error('Analysis failed:', event.data.error);
            reject(new Error(event.data.error));
          }
        };
        // Send session data to worker for analysis
        this.worker.postMessage({
          type: 'ANALYZE_SESSION',
          payload: sessionData
        });
      } catch (error) {
        console.error('Error initiating analysis:', error);
        reject(error);
      }
    });
  }
  /**
   * Provide real-time feedback during typing practice
   */
  public async provideFeedback(request: FeedbackRequest): Promise<TutorFeedback> {
    console.log('Generating feedback for user level:', request.userLevel);
    try {
      // Get AI-powered feedback
      const feedback = await this.adapter.provideFeedback(request);
      // Enhance feedback with additional context
      return this.enhanceFeedback(feedback, request);
    } catch (error) {
      console.error('Failed to generate feedback:', error);
      return this.getFallbackFeedback(request);
    }
  }
  /**
   * Calculate appropriate difficulty level based on user profile
   */
  private calculateDifficulty(profile: UserProfile): 'beginner' | 'intermediate' | 'advanced' {
    const { level, accuracy, wpm } = profile;
    if (level < 30 || accuracy < ANALYSIS_CONFIG.accuracy.needsImprovement) {
      return 'beginner';
    } else if (level < 70 || wpm < ANALYSIS_CONFIG.wpm.intermediate.target) {
      return 'intermediate';
    } else {
      return 'advanced';
    }
  }
  /**
   * Determine focus areas based on user's performance history
   */
  private determineFocusAreas(profile: UserProfile): string[] {
    const focusAreas: string[] = [];
    // Add areas based on weak keys
    if (profile.weakKeys.length > 0) {
      focusAreas.push('key_accuracy');
    }
    // Add areas based on recent mistakes
    if (profile.recentMistakes.length > ANALYSIS_CONFIG.errorPatterns.significantThreshold) {
      focusAreas.push('error_patterns');
    }
    // Add speed-related focus if WPM is below target
    if (profile.wpm < ANALYSIS_CONFIG.wpm[this.calculateDifficulty(profile)].target) {
      focusAreas.push('speed');
    }
    return focusAreas.length > 0 ? focusAreas : ['general_improvement'];
  }
  /**
   * Select appropriate content type based on user profile
   */
  private selectContentType(profile: UserProfile): 'code' | 'text' | 'mixed' {
    // Default to text for beginners
    if (profile.level < 30) {
      return 'text';
    }
    // Use code content for advanced users
    if (profile.level > 70) {
      return 'code';
    }
    // Mix content types for intermediate users
    return 'mixed';
  }
  /**
   * Calculate appropriate lesson duration based on user level
   */
  private calculateLessonDuration(profile: UserProfile): number {
    const baseDuration = 300; // 5 minutes
    const levelFactor = Math.min(2, 1 + (profile.level / 100));
    return Math.round(baseDuration * levelFactor);
  }
  /**
   * Get thematic content based on user preferences
   */
  private getThematicContent(profile: UserProfile): string[] {
    return profile.focusAreas.length > 0
      ? profile.focusAreas
      : LESSON_CONFIG.contentTypes.text.categories;
  }
  /**
   * Enhance AI feedback with additional context and suggestions
   */
  private enhanceFeedback(
    feedback: TutorFeedback,
    request: FeedbackRequest
  ): TutorFeedback {
    return {
      ...feedback,
      suggestions: [
        ...feedback.suggestions,
        ...this.generateAdditionalSuggestions(request)
      ],
      metadata: {
        ...feedback.metadata,
        confidence: this.calculateConfidence(feedback, request),
        focusAreas: this.refineFocusAreas(feedback, request)
      }
    };
  }
  /**
   * Generate additional suggestions based on request context
   */
  private generateAdditionalSuggestions(request: FeedbackRequest): string[] {
    const suggestions: string[] = [];
    if (request.recentMistakes.length > 0) {
      suggestions.push(
        'Practice these characters slowly: ' +
        Array.from(new Set(request.recentMistakes.map(m => m.expected))).join(', ')
      );
    }
    if (request.context?.previousAttempts && request.context.previousAttempts > 2) {
      suggestions.push(
        'Consider taking a short break to refresh your focus'
      );
    }
    return suggestions;
  }
  /**
   * Calculate confidence score for feedback
   */
  private calculateConfidence(
    feedback: TutorFeedback,
    request: FeedbackRequest
  ): number {
    let confidence = 0.8; // Base confidence
    // Adjust based on error pattern recognition
    if (feedback.corrections.length > 0) {
      confidence += 0.1;
    }
    // Adjust based on context availability
    if (request.context) {
      confidence += 0.1;
    }
    return Math.min(1, confidence);
  }
  /**
   * Refine focus areas based on feedback and request context
   */
  private refineFocusAreas(
    feedback: TutorFeedback,
    request: FeedbackRequest
  ): string[] {
    const areas = new Set<string>();
    // Add areas from feedback
    feedback.metadata?.focusAreas?.forEach(area => areas.add(area));
    // Add areas based on recent mistakes
    if (request.recentMistakes.length > ANALYSIS_CONFIG.errorPatterns.significantThreshold) {
      areas.add('error_patterns');
    }
    return Array.from(areas);
  }
  /**
   * Get fallback lesson when AI generation fails
   */
  private getFallbackLesson(profile: UserProfile): Lesson {
    console.log('Using fallback lesson for user:', profile.id);
    return {
      id: `fallback-${Date.now()}`,
      content: 'The quick brown fox jumps over the lazy dog. ' +
               'Pack my box with five dozen liquor jugs. ' +
               'How vexingly quick daft zebras jump!',
      level: profile.level,
      focusKeys: [],
      estimatedDuration: 300,
      metadata: {
        category: 'general',
        difficulty: 'intermediate',
        tags: ['fallback', 'pangram']
      }
    };
  }
  /**
   * Get fallback feedback when AI generation fails
   */
  private getFallbackFeedback(request: FeedbackRequest): TutorFeedback {
    console.log('Using fallback feedback for user level:', request.userLevel);
    return {
      message: 'Keep practicing to improve your typing skills!',
      corrections: request.recentMistakes.map(mistake => ({
        actual: mistake.actual,
        expected: mistake.expected,
        suggestion: 'Focus on accuracy over speed'
      })),
      suggestions: [
        'Take your time and focus on accuracy',
        'Practice regularly to build muscle memory',
        'Pay attention to proper finger placement'
      ],
      metadata: {
        confidence: 0.5,
        focusAreas: ['accuracy'],
      }
    };
  }
}
</file>

<file path="src/styles/base.css">
@tailwind base;
@layer base {
  :root {
    --background: 0 0% 3.9%;
    --foreground: 0 0% 98%;
    --card: 0 0% 3.9%;
    --card-foreground: 0 0% 98%;
    --popover: 0 0% 3.9%;
    --popover-foreground: 0 0% 98%;
    --primary: 0 0% 98%;
    --primary-foreground: 0 0% 9%;
    --secondary: 0 0% 14.9%;
    --secondary-foreground: 0 0% 98%;
    --muted: 0 0% 14.9%;
    --muted-foreground: 0 0% 63.9%;
    --accent: 0 0% 14.9%;
    --accent-foreground: 0 0% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 0 0% 98%;
    --border: 0 0% 14.9%;
    --input: 0 0% 14.9%;
    --ring: 0 0% 83.1%;
    --radius: 0.5rem;
  }
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
    font-feature-settings: "rlig" 1, "calt" 1;
  }
  /* Custom Scrollbar */
  ::-webkit-scrollbar {
    @apply w-2;
  }
  ::-webkit-scrollbar-track {
    @apply bg-muted rounded-full;
  }
  ::-webkit-scrollbar-thumb {
    @apply bg-muted-foreground/50 rounded-full hover:bg-muted-foreground/70;
  }
}
</file>

<file path="src/styles/components.css">
@tailwind components;
@layer components {
  /* Performance Chart Styles */
  .recharts-tooltip-wrapper {
    @apply !bg-background/95 backdrop-blur-sm border border-border rounded-lg shadow-lg;
  }
  .recharts-tooltip-label {
    @apply text-sm font-medium text-foreground p-2;
  }
  .recharts-tooltip-item {
    @apply text-xs text-muted-foreground px-2 pb-2;
  }
  /* Keyboard Styles */
  .key-highlight {
    @apply relative overflow-hidden;
  }
  .key-highlight::after {
    content: '';
    @apply absolute inset-0 bg-primary/10 transform scale-0 transition-transform duration-200;
  }
  .key-highlight:hover::after {
    @apply scale-100;
  }
  /* Loading States */
  .loading {
    @apply relative pointer-events-none opacity-60;
  }
  .loading::after {
    content: '';
    @apply absolute inset-0 bg-background/50 backdrop-blur-sm;
  }
  /* Accessibility Styles */
  .sr-only {
    @apply absolute w-px h-px p-0 -m-px overflow-hidden whitespace-nowrap border-0;
  }
  .focus-ring {
    @apply focus:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2;
  }
}
</file>

<file path="src/styles/globals.css">
@import url("https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;800;900&display=swap");
@import url("https://api.fontshare.com/v2/css?f[]=satoshi@1,900,700,500,300,400&display=swap");
@import url("https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap");
@tailwind base;
@tailwind components;
@tailwind utilities;
@layer base {
  :root {
    /* Base Colors */
    --background: 0 0% 100%;
    --foreground: 240 10% 3.9%;
    /* Component Colors */
    --card: 0 0% 100%;
    --card-foreground: 240 10% 3.9%;
    --popover: 0 0% 100%;
    --popover-foreground: 240 10% 3.9%;
    --primary: 240 5.9% 10%;
    --primary-foreground: 0 0% 98%;
    --secondary: 240 4.8% 95.9%;
    --secondary-foreground: 240 5.9% 10%;
    --muted: 240 4.8% 95.9%;
    --muted-foreground: 240 3.8% 46.1%;
    --accent: 240 4.8% 95.9%;
    --accent-foreground: 240 5.9% 10%;
    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 0 0% 98%;
    /* UI Elements */
    --border: 240 5.9% 90%;
    --input: 240 5.9% 90%;
    --ring: 240 5.9% 10%;
    --radius: 0.5rem;
  }
  /* Fallback for legacy browsers that do not support modern color functions */
  @supports not (color: hsl(0, 0%, 0%)) {
    :root {
      --muted: #f1f5f9; /* Fallback fallback color */
      --muted-foreground: #495057;
    }
  }
  .dark {
    --background: 240 10% 3.9%;
    --foreground: 0 0% 98%;
    --card: 240 10% 3.9%;
    --card-foreground: 0 0% 98%;
    --popover: 240 10% 3.9%;
    --popover-foreground: 0 0% 98%;
    --primary: 0 0% 98%;
    --primary-foreground: 240 5.9% 10%;
    --secondary: 240 3.7% 15.9%;
    --secondary-foreground: 0 0% 98%;
    --muted: 240 3.7% 15.9%;
    --muted-foreground: 240 5% 64.9%;
    --accent: 240 3.7% 15.9%;
    --accent-foreground: 0 0% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 0 0% 98%;
    --border: 240 3.7% 15.9%;
    --input: 240 3.7% 15.9%;
    --ring: 240 4.9% 83.9%;
  }
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground antialiased;
    font-feature-settings:
      "ss01",
      "ss02",
      "cv01",
      "cv02",
      "cv03",
      "cv04",
      "salt",
      "liga" 1;
  }
  /* Improved Custom Scrollbar */
  ::-webkit-scrollbar {
    width: 8px;
    height: 8px;
  }
  ::-webkit-scrollbar-track {
    background: hsl(var(--background));
  }
  ::-webkit-scrollbar-thumb {
    background: hsl(var(--muted));
    border-radius: 4px;
  }
  ::-webkit-scrollbar-thumb:hover {
    background: hsl(var(--muted-foreground));
  }
  /* Focus styles for better ADHD support */
  :focus {
    outline: 2px solid hsl(var(--ring));
    outline-offset: 2px;
  }
  /* Smooth transitions for better visual experience */
  * {
    transition:
      background-color 0.2s ease,
      border-color 0.2s ease,
      opacity 0.2s ease;
  }
}
@layer components {
  /* Performance Chart Styles */
  .recharts-tooltip-wrapper {
    @apply !bg-background/95 backdrop-blur-sm border border-border rounded-lg shadow-lg;
  }
  .recharts-tooltip-label {
    @apply text-sm font-medium text-foreground p-2;
  }
  .recharts-tooltip-item {
    @apply text-xs text-muted-foreground px-2 pb-2;
  }
  /* Interactive Elements */
  .key-highlight {
    @apply relative overflow-hidden;
  }
  .key-highlight::after {
    content: "";
    @apply absolute inset-0 bg-primary/10 transform scale-0 transition-transform duration-200;
  }
  .key-highlight:hover::after {
    @apply scale-100;
  }
  /* Loading States */
  .loading {
    @apply relative pointer-events-none opacity-60;
  }
  .loading::after {
    content: "";
    @apply absolute inset-0 bg-background/50 backdrop-blur-sm;
  }
  /* Accessibility */
  .sr-only {
    @apply absolute w-px h-px p-0 -m-px overflow-hidden whitespace-nowrap border-0;
  }
  .focus-ring {
    @apply focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 focus-visible:outline-none focus-visible:ring-offset-background;
  }
}
@layer utilities {
  /* Semantic Color Utilities */
  .text-muted-foreground {
    color: hsl(var(--muted-foreground));
  }
  .bg-muted {
    background-color: hsl(var(--muted));
  }
  .bg-muted-foreground {
    background-color: hsl(var(--muted-foreground));
  }
  /* Reduced Motion */
  @media (prefers-reduced-motion: reduce) {
    * {
      @apply !transition-none !animate-none;
    }
  }
  /* Responsive Container */
  @media (max-width: 640px) {
    .container {
      @apply px-4;
    }
  }
}
/* Focus animation */
.pulse-focus {
  animation: pulse 1s cubic-bezier(0.4, 0, 0.6, 1);
}
@keyframes pulse {
  0%,
  100% {
    opacity: 1;
  }
  50% {
    opacity: 0.7;
  }
}
</file>

<file path="src/styles/utilities.css">
@tailwind utilities;
@layer utilities {
  /* Color Utilities */
  .text-muted-foreground {
    color: hsl(var(--muted-foreground));
  }
  .bg-muted {
    background-color: hsl(var(--muted));
  }
  .bg-muted-foreground {
    background-color: hsl(var(--muted-foreground));
  }
  /* Animation Utilities */
  @keyframes cursor-blink {
    0%, 100% { opacity: 1; }
    50% { opacity: 0; }
  }
  @keyframes scale-in {
    from { transform: scale(0.95); opacity: 0; }
    to { transform: scale(1); opacity: 1; }
  }
  @keyframes slide-up {
    from { transform: translateY(10px); opacity: 0; }
    to { transform: translateY(0); opacity: 1; }
  }
  @keyframes fade-in {
    from { opacity: 0; }
    to { opacity: 1; }
  }
  .animate-cursor-blink {
    animation: cursor-blink 1s step-end infinite;
  }
  .animate-scale-in {
    animation: scale-in 0.2s ease-out;
  }
  .animate-slide-up {
    animation: slide-up 0.3s ease-out;
  }
  .animate-fade-in {
    animation: fade-in 0.3s ease-out;
  }
  /* Typography */
  .font-mono {
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  }
  .font-sans {
    font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif;
  }
  /* Responsive Design */
  @media (max-width: 640px) {
    .container {
      @apply px-4;
    }
  }
  @media (prefers-reduced-motion) {
    * {
      @apply !transition-none !animate-none;
    }
  }
}
</file>

<file path="src/test/setup.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Test Setup                                                                             ║
 * ║ Description: Setup file for configuring the test environment                                   ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import '@testing-library/jest-dom'
import { expect, afterEach } from 'vitest'
import { cleanup } from '@testing-library/react'
import matchers from '@testing-library/jest-dom/matchers'
// Extend Vitest's expect method with methods from react-testing-library
expect.extend(matchers)
// Cleanup after each test case (e.g. clearing jsdom)
afterEach(() => {
  cleanup()
})
// Mock IntersectionObserver
const mockIntersectionObserver = vi.fn()
mockIntersectionObserver.mockReturnValue({
  observe: () => null,
  unobserve: () => null,
  disconnect: () => null
})
window.IntersectionObserver = mockIntersectionObserver
// Mock ResizeObserver
const mockResizeObserver = vi.fn()
mockResizeObserver.mockReturnValue({
  observe: () => null,
  unobserve: () => null,
  disconnect: () => null
})
window.ResizeObserver = mockResizeObserver
// Mock window.matchMedia
Object.defineProperty(window, 'matchMedia', {
  writable: true,
  value: vi.fn().mockImplementation(query => ({
    matches: false,
    media: query,
    onchange: null,
    addListener: vi.fn(),
    removeListener: vi.fn(),
    addEventListener: vi.fn(),
    removeEventListener: vi.fn(),
    dispatchEvent: vi.fn()
  }))
})
// Mock window.scrollTo
window.scrollTo = vi.fn()
// Mock console methods to catch errors
const originalConsoleError = console.error
const originalConsoleWarn = console.warn
console.error = (...args: any[]) => {
  if (
    typeof args[0] === 'string' &&
    args[0].includes('Warning: ReactDOM.render is no longer supported')
  ) {
    return
  }
  originalConsoleError.call(console, ...args)
}
console.warn = (...args: any[]) => {
  if (
    typeof args[0] === 'string' &&
    args[0].includes('Warning: React.createFactory()')
  ) {
    return
  }
  originalConsoleWarn.call(console, ...args)
}
// Mock fetch
global.fetch = vi.fn()
// Mock localStorage
const localStorageMock = {
  getItem: vi.fn(),
  setItem: vi.fn(),
  removeItem: vi.fn(),
  clear: vi.fn()
}
Object.defineProperty(window, 'localStorage', { value: localStorageMock })
// Mock sessionStorage
const sessionStorageMock = {
  getItem: vi.fn(),
  setItem: vi.fn(),
  removeItem: vi.fn(),
  clear: vi.fn()
}
Object.defineProperty(window, 'sessionStorage', { value: sessionStorageMock })
</file>

<file path="src/types/theme.d.ts">
declare global {
  type HexColor = `#${string}`;
  type HSLString = `${number} ${number}% ${number}%`;
  interface ThemeColors {
    background: HSLString;
    foreground: HSLString;
    card: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    popover: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    primary: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    secondary: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    muted: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    accent: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    destructive: {
      DEFAULT: HSLString;
      foreground: HSLString;
    };
    border: HSLString;
    input: HSLString;
    ring: HSLString;
  }
  interface ThemeConfig {
    darkMode: string[];
    content: string[];
    theme: {
      extend: {
        colors: ThemeColors;
        borderRadius: {
          lg: string;
          md: string;
          sm: string;
        };
        keyframes: Record<string, Record<string, Record<string, string>>>;
        animation: Record<string, string>;
      };
    };
    plugins: any[];
  }
}
export {};
</file>

<file path="src/types/typing.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Types: Typing Core                                                                             ║
 * ║ Description: Type definitions for typing tutor core functionality                              ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
/**
 * Finger positions for touch typing
 */
export type FingerPosition =
  | 'left-pinky' | 'left-ring' | 'left-middle' | 'left-index'
  | 'right-index' | 'right-middle' | 'right-ring' | 'right-pinky';
/**
 * Metadata for each key on the keyboard
 */
export interface KeyMetadata {
  character: string;
  finger: FingerPosition;
  frequencyScore: number;
  soundProfile: string;
  hapticPattern: HapticPattern;
}
/**
 * User profile with typing preferences and history
 */
export interface UserProfile {
  id: string;
  level: number;
  accuracy: number;
  wpm: number;
  weakKeys: string[];
  learningStyle: 'visual' | 'auditory' | 'kinesthetic';
  recentMistakes: Array<{ actual: string; expected: string }>;
  focusAreas: string[];
  preferences: {
    soundEnabled: boolean;
    hapticEnabled: boolean;
    visualGuides: boolean;
    keyboardLayout: 'qwerty' | 'dvorak' | 'colemak';
  };
}
/**
 * A typing lesson with content and metadata
 */
export interface Lesson {
  id: string;
  content: string;
  level: number;
  focusKeys: string[];
  estimatedDuration: number;
  metadata?: {
    category?: string;
    tags?: string[];
    source?: string;
    difficulty?: 'beginner' | 'intermediate' | 'advanced';
  };
}
/**
 * A typing session with performance metrics
 */
export interface TypingSession {
  id: string;
  userId: string;
  startTime: Date;
  endTime: Date;
  wpm: number;
  accuracy: number;
  errors: Array<{
    actual: string;
    expected: string;
    timestamp: number;
    position: number;
  }>;
  keyPressTimings: Record<string, number[]>;
  pauseDurations: number[];
  completedText: string;
  targetText: string;
}
/**
 * Analysis of typing performance
 */
export interface Analysis {
  wpm: number;
  accuracy: number;
  errorPatterns: Array<{
    pattern: string;
    frequency: number;
    suggestion: string;
  }>;
  speedTrends: Array<{
    timestamp: string;
    wpm: number;
  }>;
  recommendedFocus: string[];
  aiRecommendations: string[];
}
/**
 * Request for AI tutor feedback
 */
export interface FeedbackRequest {
  currentText: string;
  targetText: string;
  recentMistakes: Array<{ actual: string; expected: string }>;
  userLevel: number;
  learningStyle: string;
  context?: {
    lessonType?: string;
    previousAttempts?: number;
    timeSpent?: number;
  };
}
/**
 * AI tutor feedback response
 */
export interface TutorFeedback {
  message: string;
  corrections: Array<{
    actual: string;
    expected: string;
    suggestion: string;
  }>;
  suggestions: string[];
  metadata?: {
    confidence: number;
    focusAreas?: string[];
    nextLessonRecommendation?: string;
  };
}
/**
 * Haptic feedback pattern
 */
export interface HapticPattern {
  intensity: number;
  duration: number;
  pattern: number[];
  description: string;
}
/**
 * Performance metrics for analytics
 */
export interface TypingMetrics {
  userId: string;
  sessionId: string;
  timestamp: number;
  metrics: {
    wpm: number;
    accuracy: number;
    errorRate: number;
    backspaceCount: number;
    pauseCount: number;
    totalTime: number;
    activeTime: number;
  };
  heatmap: Record<string, number>;
  progression: {
    levelProgress: number;
    achievementsUnlocked: string[];
    skillsImproved: string[];
  };
}
/**
 * Raw typing session data for analysis
 */
export interface TypingSessionData {
  keyPresses: Array<{
    key: string;
    timestamp: number;
    duration: number;
    pressure?: number;
  }>;
  errors: Array<{
    expected: string;
    actual: string;
    position: number;
    timestamp: number;
  }>;
  metrics: {
    wpm: number;
    accuracy: number;
    duration: number;
    pauseCount: number;
  };
  context: {
    lessonId: string;
    difficulty: string;
    targetText: string;
    completedText: string;
  };
}
</file>

<file path="src/utils/typingAnalytics.ts">
export interface MistakePattern {
  character: string;
  count: number;
  expectedChar: string;
}
export interface TypingAnalysis {
  commonMistakes: MistakePattern[];
  problemKeys: string[];
  averageWPM: number;
  consistencyScore: number;
}
export const analyzeTypingPatterns = (mistakes: Array<{ actual: string; expected: string }>, recentWPMs: number[]): TypingAnalysis => {
  const mistakeMap = new Map<string, MistakePattern>();
  mistakes.forEach(({ actual, expected }) => {
    const key = `${actual}-${expected}`;
    if (!mistakeMap.has(key)) {
      mistakeMap.set(key, { character: actual, count: 0, expectedChar: expected });
    }
    const pattern = mistakeMap.get(key)!;
    pattern.count++;
  });
  const commonMistakes = Array.from(mistakeMap.values())
    .sort((a, b) => b.count - a.count)
    .slice(0, 5);
  const problemKeys = commonMistakes.map(m => m.expectedChar);
  const averageWPM = recentWPMs.length > 0 
    ? recentWPMs.reduce((a, b) => a + b, 0) / recentWPMs.length 
    : 0;
  const consistencyScore = calculateConsistencyScore(recentWPMs);
  return {
    commonMistakes,
    problemKeys,
    averageWPM,
    consistencyScore
  };
};
const calculateConsistencyScore = (wpmHistory: number[]): number => {
  if (wpmHistory.length < 2) return 100;
  const variations = wpmHistory.slice(1).map((wpm, i) => 
    Math.abs(wpm - wpmHistory[i]) / wpmHistory[i]
  );
  const averageVariation = variations.reduce((a, b) => a + b, 0) / variations.length;
  return Math.round((1 - averageVariation) * 100);
};
export const generateAdaptiveLessons = (analysis: TypingAnalysis, currentLevel: string): string[] => {
  const baseText = "the quick brown fox jumps over the lazy dog";
  const problemKeys = analysis.problemKeys;
  const lessonTemplates = {
    beginner: [
      baseText,
      ...problemKeys.map(key => `${key.repeat(3)} ${baseText}`),
      baseText.split('').sort(() => Math.random() - 0.5).join('')
    ],
    intermediate: [
      ...problemKeys.map(key => `The ${key} appears frequently in this ${key}sentence about ${key}typing.`),
      baseText.split(' ').sort(() => Math.random() - 0.5).join(' ')
    ],
    advanced: [
      ...problemKeys.map(key => `Programming requires ${key} usage in ${key}various ${key}contexts.`),
      "function handleTyping(event) { console.log(event.key); }"
    ]
  };
  return lessonTemplates[currentLevel as keyof typeof lessonTemplates] || lessonTemplates.beginner;
};
</file>

<file path="src/App.css">
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}
.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}
@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}
@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}
.card {
  padding: 2em;
}
.read-the-docs {
  color: #888;
}
</file>

<file path="src/App.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Component: App                                                                                 ║
 * ║ Description: Main application component with routing and global providers                      ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { BrowserRouter as Router, Routes, Route, Navigate } from 'react-router-dom';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { Toaster } from '@/components/ui/toaster';
import TypingTutor from '@/routes/TypingTutor';
import Auth from '@/routes/Auth';
import Profile from '@/routes/Profile';
import Navigation from '@/components/layout/Navigation';
import ProtectedRoute from '@/components/auth/ProtectedRoute';
import Leaderboard from '@/routes/Leaderboard';
import ErrorBoundary from '@/components/ErrorBoundary';
import { ThemeProvider } from './contexts/theme-context';
const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 60 * 1000, // 1 minute
      gcTime: 5 * 60 * 1000, // 5 minutes
      refetchOnWindowFocus: false,
      retry: 1,
    },
  },
});
function App() {
  return (
    <ThemeProvider>
      <ErrorBoundary>
        <QueryClientProvider client={queryClient}>
          <Router>
            <div className="min-h-screen bg-background">
              <Routes>
                {/* Public Routes */}
                <Route
                  path="/auth"
                  element={
                    <ErrorBoundary>
                      <Auth />
                    </ErrorBoundary>
                  }
                />
                {/* Protected Routes */}
                <Route
                  path="/"
                  element={
                    <ProtectedRoute>
                      <ErrorBoundary>
                        <>
                          <Navigation />
                          <TypingTutor />
                        </>
                      </ErrorBoundary>
                    </ProtectedRoute>
                  }
                />
                <Route
                  path="/profile"
                  element={
                    <ProtectedRoute>
                      <ErrorBoundary>
                        <>
                          <Navigation />
                          <Profile />
                        </>
                      </ErrorBoundary>
                    </ProtectedRoute>
                  }
                />
                <Route
                  path="/leaderboard"
                  element={
                    <ProtectedRoute>
                      <ErrorBoundary>
                        <>
                          <Navigation />
                          <Leaderboard />
                        </>
                      </ErrorBoundary>
                    </ProtectedRoute>
                  }
                />
                {/* Catch-all route */}
                <Route path="*" element={<Navigate to="/" replace />} />
              </Routes>
            </div>
          </Router>
          <Toaster />
        </QueryClientProvider>
      </ErrorBoundary>
    </ThemeProvider>
  );
}
export default App;
</file>

<file path="src/index.css">
@tailwind base;
@tailwind components;
@tailwind utilities;
@layer base {
  :root {
    --background: 0 0% 100%;
    --foreground: 222.2 84% 4.9%;
    --card: 0 0% 100%;
    --card-foreground: 222.2 84% 4.9%;
    --popover: 0 0% 100%;
    --popover-foreground: 222.2 84% 4.9%;
    --primary: 222.2 47.4% 11.2%;
    --primary-foreground: 210 40% 98%;
    --secondary: 210 40% 96.1%;
    --secondary-foreground: 222.2 47.4% 11.2%;
    --muted: 210 40% 96.1%;
    --muted-foreground: 215.4 16.3% 46.9%;
    --accent: 210 40% 96.1%;
    --accent-foreground: 222.2 47.4% 11.2%;
    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 210 40% 98%;
    --border: 214.3 31.8% 91.4%;
    --input: 214.3 31.8% 91.4%;
    --ring: 222.2 84% 4.9%;
    --radius: 0.5rem;
    --sidebar-background: 0 0% 98%;
    --sidebar-foreground: 240 5.3% 26.1%;
    --sidebar-primary: 240 5.9% 10%;
    --sidebar-primary-foreground: 0 0% 98%;
    --sidebar-accent: 240 4.8% 95.9%;
    --sidebar-accent-foreground: 240 5.9% 10%;
    --sidebar-border: 220 13% 91%;
    --sidebar-ring: 217.2 91.2% 59.8%;
  }
  .dark {
    --background: 222.2 84% 4.9%;
    --foreground: 210 40% 98%;
    --card: 222.2 84% 4.9%;
    --card-foreground: 210 40% 98%;
    --popover: 222.2 84% 4.9%;
    --popover-foreground: 210 40% 98%;
    --primary: 210 40% 98%;
    --primary-foreground: 222.2 47.4% 11.2%;
    --secondary: 217.2 32.6% 17.5%;
    --secondary-foreground: 210 40% 98%;
    --muted: 217.2 32.6% 17.5%;
    --muted-foreground: 215 20.2% 65.1%;
    --accent: 217.2 32.6% 17.5%;
    --accent-foreground: 210 40% 98%;
    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 210 40% 98%;
    --border: 217.2 32.6% 17.5%;
    --input: 217.2 32.6% 17.5%;
    --ring: 212.7 26.8% 83.9%;
    --sidebar-background: 240 5.9% 10%;
    --sidebar-foreground: 240 4.8% 95.9%;
    --sidebar-primary: 224.3 76.3% 48%;
    --sidebar-primary-foreground: 0 0% 100%;
    --sidebar-accent: 240 3.7% 15.9%;
    --sidebar-accent-foreground: 240 4.8% 95.9%;
    --sidebar-border: 240 3.7% 15.9%;
    --sidebar-ring: 217.2 91.2% 59.8%;
  }
}
@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
    font-feature-settings: "rlig" 1, "calt" 1;
  }
}
@layer components {
  .recharts-tooltip-wrapper {
    @apply !bg-background/95 backdrop-blur-sm border border-border rounded-lg shadow-lg;
  }
  .recharts-tooltip-label {
    @apply text-sm font-medium text-foreground p-2;
  }
  .recharts-tooltip-item {
    @apply text-xs text-muted-foreground px-2 pb-2;
  }
}
</file>

<file path="src/main.tsx">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Main Entry Point                                                                       ║
 * ║ Description: Application entry point with global providers and styles                          ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import React from 'react';
import ReactDOM from 'react-dom/client';
import App from './App.tsx';
import './styles/globals.css';
// Add error boundary for better debugging
const renderApp = () => {
  try {
    console.debug('Initializing application...');
    const rootElement = document.getElementById('root');
    if (!rootElement) {
      throw new Error('Root element not found! Check if your index.html has a div with id="root"');
    }
    ReactDOM.createRoot(rootElement).render(
      <React.StrictMode>
        <App />
      </React.StrictMode>
    );
    console.debug('Application successfully mounted');
  } catch (error) {
    console.error('Failed to initialize application:', error);
    throw error;
  }
};
renderApp();
</file>

<file path="src/PRD.md">
# DeepType by Empathy Labs: Comprehensive Project Dossier

## 1. Executive Summary & Branding

**Vision Statement:** *Empower every individual – regardless of vision or ability – to master typing and digital communication through empathetic AI.* DeepType’s mission is to be an **AI-powered typing tutor** that enables blind and visually-impaired users to achieve keyboard proficiency, opening doors to education, employment, and independence. With a focus on **“accessible by design”**, DeepType bridges the digital divide by turning the keyboard into an inclusive gateway for all.

**Naming & Brand Identity:** **DeepType** is the product name, reflecting the use of deep learning (“Deep”) to revolutionize touch typing (“Type”). It exists under the **DeepHustle.ai** project umbrella – an innovation initiative by **Empathy Labs**. Empathy Labs is the organization’s brand, emphasizing user-centric design and emotional intelligence in tech. The naming strategy ensures clarity: Empathy Labs conveys trust and compassion, DeepHustle.ai signals cutting-edge AI innovation, and DeepType itself clearly describes the product’s function. Together, they present a unified brand with a heart (Empathy) and a brain (Deep AI technology).

**Market Positioning:** DeepType is positioned at the intersection of **assistive technology** and **ed-tech**. It targets a significant underserved market: the millions of people with visual impairments who require effective tools to learn keyboard skills. Globally, an estimated **43 million people are blind and 295 million have moderate-to-severe visual impairment** ([Bridging the Digital Disability Divide: Determinants of Internet Use among Visually Impaired Individuals in Thailand](https://www.mdpi.com/2673-7272/4/3/43#:~:text=difficulty%20seeing%20well%20at%20a,that%20the%20prevalence%20of%20visual)). Many of these individuals rely on screen readers (like JAWS or NVDA) to use computers, yet accessible typing training tools are scarce and outdated. DeepType aims to be the **premier digital solution for accessible typing education**, much like a “Duolingo for typing” tailored to blind and low-vision users. By leveraging AI and modern UX practices, it stands out from legacy offerings. The brand promise is empowerment: *DeepType gives users the confidence to navigate the digital world through touch typing.* This resonates strongly in a market where independence and employment are tightly linked to technological skills – especially when **over 70% of blind and visually-impaired adults face unemployment, partly due to limited access to training** ([Employment Barriers for the Blind and Visually Impaired — World Services for the Blind](https://www.wsblind.org/blog/2021/6/16/employment-barriers-for-the-blind-and-visually-impaired#:~:text=variety%20of%20reasons%20from%20lack,impaired%20and%20how%20to%20overcome)). DeepType will be marketed as an **inclusion-driven innovation**: not just another typing app, but a social impact tool that **combines empathy with technology** to transform lives. 

## 2. Ultimate Pitch & Pitch Deck

### High-Impact Introduction (The Elevator Pitch)
Imagine **pressing a key** and unlocking a world of opportunity. DeepType is an AI-powered tutor that speaks, listens, and *understands*, turning the once tedious task of learning to type into an empowering journey for those who need it most. **DeepType by Empathy Labs** is *the first accessibility-first typing coach*, designed for the blind and visually impaired, but delightful for everyone. It’s like having a personal coach who never gets tired, never judges, and is available 24/7 – **“the Swiss Army Knife of typing tutors”** for all abilities.

> **“Because everyone deserves a voice and a keyboard.”**

### Key Differentiators & Competitive Edge
- **🎯 Built for Accessibility from Day One:** Unlike generic typing programs, DeepType is built *ground-up for blind and low-vision users*. Every feature – from audio guidance to high-contrast visuals – follows accessibility best practices (WCAG 2.1, ARIA roles, etc.). This isn’t a retrofit; it’s a revolution. We don’t just meet compliance, we embrace **“accessibility-first”** design as our core ethos.
- **🗣 Voice-First and Hands-On:** DeepType is a **voice-interactive tutor**. It speaks instructions and encouragement in natural language, and listens for commands. Users can navigate the entire learning experience with speech or a single button. This **voice-first interaction** means eyes-free, hassle-free learning – perfect for blind users and also convenient for anyone (think learning while keeping your eyes on another task).
- **🤖 Adaptive AI Coach:** At the heart of DeepType is cutting-edge AI (including large language models) that adapts in real-time to the learner. Make a mistake? DeepType’s AI detects the pattern and offers **personalized feedback and exercises**. Struggling with certain letters? The AI dynamically adjusts the lesson to give extra practice where needed. No static lessons or one-size-fits-all curriculum – **DeepType learns the learner**.
- **🌐 Cross-Platform Convenience:** Available on **web, desktop, and mobile** without separate development silos. Our tech stack enables writing code once and deploying everywhere, ensuring a consistent experience whether the user is on a Windows PC with a screen reader, a Mac, or a mobile device. DeepType even works offline for desktop (via an app) so it’s reliable in classrooms and areas with limited internet.
- **💡 Competitive Edge over Legacy Tools:** Traditional solutions like *Talking Typing Teacher* rely on pre-recorded voices and fixed sc ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))L146】. DeepType, however, uses AI voices and natural language generation to provide **conversational, context-aware guidance**. Unlike screen readers (JAWS/NVDA) that simply echo keys, DeepType teaches with a curriculum, tracks progress, and gamifies the experience. And unlike costly enterprise software, DeepType aims to be affordable or free for end-users (with a sustainable business model backing it – see below).

### Problem & Opportunity
For a visually impaired person, learning to type is not just a skill – it’s a lifeline to the digital world. Yet current options are bleak: decades-old software with robotic feedback, expensive licenses, or relying on general screen readers that **don’t teach**. The result? Many blind individuals struggle with slow typing or never learn, limiting their potential in school and the workplace. **Unmet need:** an engaging, effective, and affordable way to learn keyboarding without sight.

**Opportunity:** DeepType sits at the convergence of two rising tides – the advancement of AI in education, and the push for digital inclusion. Recent breakthroughs show the promise of AI in assistive tech (e.g., *Be My Eyes* integrating OpenAI’s GPT-4 to describe images for blind  ([Introducing Be My AI (formerly Virtual Volunteer) for People who are Blind or Have Low Vision, Powered by OpenAI’s GPT-4](https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer#:~:text=a%20dynamic%20new%20image,a%20wide%20variety%20of%20tasks))1-L4】). Yet, no one has applied such AI prowess to *typing education* for the blind. We have the first-mover advantage to capture this niche and expand it to a broader audience (sighted users also benefit from voice-assisted, hands-free learning – think driving, or dyslexic learners using multi-sensory feedback). 

DeepType can become the **gold standard** for accessible skills training, starting with typing and potentially expanding to other digital literacy skills. By solving a deeply specific problem with excellence, we build trust and brand loyalty in the assistive tech community.

### Business Model & Monetization Strategy
DeepType’s business model balances social impact with sustainability:

- **Freemium Core:** The base product (core typing lessons and accessibility features) is **free for individual users**, ensuring that cost is never a barrier for those who need it most. This echoes the strategy of NVDA, the free screen reader that rapidly gained global adoption – now used by ~65% of screen reader  ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=NVDA%20is%20again%20the%20most,of%20respondents))1-L4】, surpassing its expensive rival JAWS.
- **B2B and Institutional Licensing:** Revenue is generated by offering premium plans to schools, rehabilitation centers, and enterprises. For example:
  - **Education Edition:** Schools for the blind or K-12 special education programs can subscribe to a managed DeepType Classroom package, which includes teacher dashboards, student progress analytics, and custom lesson creation. These institutions often have funding or grants for assistive technology and will pay for a solution that demonstrably improves student outcomes.
  - **Corporate Training & CSR:** Companies aiming to hire and upskill visually-impaired employees (or retrain workers who lost vision) can license DeepType for professional use. Additionally, corporations could sponsor DeepType deployments as part of their Corporate Social Responsibility programs, effectively *sponsoring free licenses for users* in developing regions (a model similar to how some companies sponsor NVDA development via donations).
- **Premium Features for Power Users:** While the basic tutor is free, advanced features could be behind a modest subscription. Examples: an **AI “Tutor Plus”** that users can converse with to get career advice or advanced typing drills, cloud sync of personal progress across devices, or the ability to **generate custom practice content** (e.g., “I want to practice typing this specific book or code snippet” – the AI prepares a lesson). These power features cater to enthusiasts or professionals and can justify a monthly fee.
- **Grants and Partnerships:** Given its mission-driven nature, DeepType will aggressively seek partnerships with nonprofits (e.g., American Foundation for the Blind) and technology grants. These partnerships not only provide funding but also endorsements and user base. For instance, a foundation might fund the development of a new Braille-display integration feature, or a government agency might deploy DeepType in digital literacy initiatives.
- **Open-Source Community Edition:** A portion of DeepType’s code (especially accessibility utilities) could be open-sourced to encourage community contributions and transparency. This fosters goodwill and potentially free improvements, while the core AI tutoring logic and premium services remain proprietary for monetization. It’s similar to how some companies open-source their SDKs but sell hosted services.

**Monetization with Empathy:** At all times, our strategy ensures that **the end-user who most needs DeepType (a blind learner)** is never left out due to cost. Revenue streams target those who *can* pay (schools, orgs, sponsors) to subsidize those who cannot. This aligns with our brand values (Empathy Labs) and creates a positive feedback loop: more users -> more data -> better AI -> more compelling product -> more paying partners.

### Microcopy & Persuasive Messaging
DeepType’s voice and tone are **motivational, friendly, and inclusive**. We use microcopy (short pieces of guiding text or audio) to keep users engaged and encouraged:

- Onboarding greeting: *“Welcome to DeepType – where your fingers learn to sing on the keyboard. Let’s unlock your potential, one key at a time!”*
- When the user makes a mistake: *“Whoops, that didn’t match. No worries – try again, I’m right here with you.”* (No scolding, always supportive.)
- Success message: *“Great job! You nailed that. Ready for the next challenge?”*
- Idle encouragement (if user is inactive): *“I’m still here. Whenever you’re ready, press the space bar and we’ll continue your journey.”*
- Interface labels use empowering language: The “Start” button might say **“Begin Your Journey”**, the help section: **“Need a hand? (Press H)”** spoken as *“You can press H anytime for help – I’ve got tips for you.”*

Persuasive messaging also highlights outcomes: After a progress milestone, DeepType might say, *“You’ve improved your speed by 20%! Imagine writing emails or coding with this speed. You’re on fire!”* This connects the practice to real-life benefits, sustaining motivation.

Throughout our copy, we emphasize **independence, confidence, and fun**. Typing is framed not as a tedious skill, but as *liberation*. *“Your voice in the digital world”* is a recurring theme – since typing enables one to communicate just like sighted peers. Our branding tagline could be: **“DeepType: Touch the keys, touch the world.”** This resonates emotionally and sticks in memory.

### Accessibility-First Product Positioning
DeepType proudly wears the “accessibility-first” badge. In pitches and materials, we make it clear this is **not an afterthought or add-on**. For example, the product website and pitch deck prominently state: **“Designed for accessibility from scratch – not retrofitted.”** We highlight features like:
- **Voice Guided Learning:** All lessons are delivered through clear speech, so a user with zero vision can participate without any setup. *“If you can hear, you can learn with DeepType.”*
- **One-Button Navigation:** The entire UI can be driven with a single key or switch device. This means even users with motor challenges or cognitive overload can navigate step-by-step. (For instance, an on-screen highlight moves through options and the user hits the one button to select – a common approach for switch accessibility.)
- **High Contrast & Large Print:** For low-vision users, DeepType offers bold, large text and high-contrast color themes out of the box. Screenshots in the pitch deck demonstrate a stark black-and-white interface with >AAA contrast, showing our dedication to usable design for low vision.
- **Screen-Reader Friendly:** DeepType works harmoniously with screen readers. However, it often won’t need a screen reader’s assistance because it *is* the screen reader for its own interface. (E.g., the app announces “Menu: Start, Settings, Exit – use arrow keys or say ‘Start’” etc.) This dual approach (integrating with or without external screen reader) means flexibility for user preference.
- **Multimodal Input:** The user can *speak* commands, *type* responses, or even use a *Braille display* in future iterations. DeepType positions itself as *device-agnostic*. If you can press any button or utter a word, you can control it. Such flexibility is rare and a strong selling point in assistive tech.

By positioning accessibility at the forefront, we also capture the interest of allies: educators, parents, occupational therapists, and diversity officers who seek tools that champion inclusive design. DeepType isn’t just a tool; it’s a statement that technology should serve everyone. Our pitch emphasizes this higher purpose, which not only differentiates us but also often sways decision-makers (e.g., a school district choosing between a generic typing software vs. DeepType will see that only DeepType was *built* for their blind students’ needs).

### Pitch Deck Outline (Markdown Slides)

*(Below is a markdown representation of the Pitch Deck for DeepType. Each slide is described with its title and key bullet points.)*

**Slide 1: Title & Vision**  
**DeepType** by Empathy Labs  
*The AI-Powered Typing Tutor for All*  
- *Vision:* Empower every individual to communicate digitally, regardless of vision or ability.  
- *Tagline:* **Touch the keys, touch the world.**  

**Slide 2: The Problem**  
- 285 million people have visual impairments, 43 million blind worl ([Bridging the Digital Disability Divide: Determinants of Internet Use among Visually Impaired Individuals in Thailand](https://www.mdpi.com/2673-7272/4/3/43#:~:text=difficulty%20seeing%20well%20at%20a,that%20the%20prevalence%20of%20visual))1-L4】.  
- Typing = essential skill for education & jobs, yet **no effective way to learn if you can’t see the keyboard.**  
- Legacy solutions are outdated, hard to access, or extremely expensive. (E.g., 70% unemployment among blind partly due to lack of tech s ([Employment Barriers for the Blind and Visually Impaired — World Services for the Blind](https://www.wsblind.org/blog/2021/6/16/employment-barriers-for-the-blind-and-visually-impaired#:~:text=variety%20of%20reasons%20from%20lack,impaired%20and%20how%20to%20overcome))-L77】.)  
- *Opportunity:* Huge gap in assistive education – **time to innovate.**

**Slide 3: The Solution**  
- **DeepType** – an AI tutor that **speaks, listens, and adapts** to the user.  
- Learn to type through interactive audio lessons, real-time feedback, and personalized practice.  
- Use it on any device: phone, tablet, computer – no sight required.  
- Outcome: Blind/low-vision users gain digital independence (sending emails, coding, writing) by mastering typing.

**Slide 4: Why Now? (Market Timing)**  
- **AI & Accessibility Renaissance:** AI is enabling new assistive tech (e.g., GPT-4 Vision in Be My ([Introducing Be My AI (formerly Virtual Volunteer) for People who are Blind or Have Low Vision, Powered by OpenAI’s GPT-4](https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer#:~:text=a%20dynamic%20new%20image,a%20wide%20variety%20of%20tasks))1-L4】, Seeing AI by Microsoft combining visio ([Seeing AI: New Technology Research to Support the Blind and Visually Impaired Community - Microsoft Accessibility Blog](https://blogs.microsoft.com/accessibility/seeing-ai/#:~:text=Seeing%20AI%20will%20use%20computer,identify%20emotions%20on%20people%E2%80%99s%20faces))L137】). It’s *proven* that AI can make tech more inclusive.  
- **Tech Convergence:** Speech recognition, text-to-speech, and language models are all advanced *and affordable* in 2025 – enabling our solution.  
- **Remote Learning Boom:** Post-2020, digital learning tools are mainstream. Accessibility in e-learning is a highlighted need. Institutions seek solutions like DeepType to include all learners.

**Slide 5: Key Features (What makes DeepType special)**  
- 🗣 **Voice-Guided Lessons:** Friendly voice instructions and feedback. *Hands-free learning.*  
- 🤖 **Adaptive Learning AI:** Adjusts difficulty in real-time, generates custom exercises on the fly. *No one gets left behind or bored.*  
- 🎮 **Gamified & Motivating:** Progress badges, fun sound cues, and challenges keep learners engaged. *It’s fun!*  
- 💻 **Cross-Platform:** Use it via web browser, dedicated desktop app, or on mobile. Consistent experience, cloud-synced progress.  
- ♿ **Accessibility at Core:** High-contrast UI, ARIA labels, one-switch mode, works with screen readers – *built to WCAG standards.*  

**Slide 6: Competitive Landscape**  
- **Talking Typing Teacher (Legacy software):** Audio-based lessons with recorded voice. *Cons:* static content, no AI, Windows-only, n ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))137-L146】.  
- **TypeAbility (JAWS add-on):** Teaches typing within JAWS screen reader. *Cons:* requires expensive JAWS, dated curriculum.  
- **NVDA/JAWS Screen Readers:** Not actually teaching tools – they identify keys but don’t provide structured lessons (only a “keyboard help” mode). Also, JAWS costs  ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】 or $1200 perpetual, pricing many out.  
- **Mainstream Typing Apps (e.g., Mavis Beacon):** Visual-centric, unusable for blind users; no voice guidance.  
- **Others (Seeing AI, Be My Eyes):** Solve *different* problems (environment perception, not typing).  
**DeepType’s Edge:** No one offers an **AI-driven, accessibility-first typing tutor.** We combine the strengths of these (voice output, structured curriculum) and add modern AI adaptivity and multi-platform support. We’re in a league of our own – a blue ocean in assistive ed-tech.

**Slide 7: Business Model**  
- **Free for Users:** Core app free for end-users (removing adoption barriers, like NVDA did with screen readers).  
- **B2B/Institutional Sales:** Revenue from schools, rehab centers, libraries – annual licenses including admin dashboards & priority support.  
- **Sponsored Programs:** Partner with nonprofits/corporates to sponsor deployments (CSR funding covers costs for communities in need).  
- **Premium Add-ons:** Optional subscription for advanced personal features (e.g., conversational practice buddy, specialty courses like “coding keyboard shortcuts” or “Excel navigation”).  
- **Scaling Plan:** Start in assistive tech niche -> expand features for general audience (e.g., sighted people using voice tutor while multitasking) -> position as a universal typing tutor (with accessibility as our differentiator and moral backbone).  

**Slide 8: Roadmap & Milestones**  
- **Q1:** MVP launch (Web app beta) – Core lessons A-Z, basic voice feedback. Collect user feedback.  
- **Q2:** Desktop/Mobile apps release (wrapped PWA). Add voice command navigation, cloud sync via Supabase.  
- **Q3:** AI Adaptive Engine v2 – integrate GPT-based error analysis and *live coaching*, plus initial multi-language support (type in English, Spanish, etc.).  
- **Q4:** Institutional Dashboard – analytics for classrooms, content editor for teachers. Begin pilot programs with 3 blind schools.  
- **Year 2:** Scale to 10K+ users, GPT-4 (or Gemini) powered conversational tutor (“Ask DeepType anything”), Braille display integration, and pursue Series A funding for growth.  

**Slide 9: Team & Empathy Labs**  
- **Founders:** [Your Name] – (Background in AI and personal connection to accessibility), [Other Name] – (EdTech veteran, created learning curricula).  
- **Empathy Labs** – Innovation lab focusing on human-centric AI. DeepHustle.ai is our project incubator blending deep learning with human empathy.  
- Advisors include accessibility experts (e.g., a blind tech lead, special-ed teacher) and AI researchers. Backed by [Mentors/Accelerator if any].  
- *Our superpower:* We combine technical expertise with lived experience insights – building *with* the community, not just for them.

**Slide 10: The Ask & Closing**  
- **Seeking:** [If pitching to investors: $X seed funding] OR [If pitching for partnership: pilot opportunities, introductions, etc.].  
- This will fuel development of advanced features (real-time voice AI, more languages) and allow us to distribute DeepType to those who need it globally.  
- **Impact:** A successful DeepType means thousands of people will gain skills, confidence, and jobs that were previously out of reach. It’s not just an investment in a product, it’s an investment in *digital equality*.  
- *Join us* in typing a new chapter of inclusion.  
- **Thank You.**  
*(Contact: your.email@empathylabs.ai | www.deephustle.ai/deeptype)*

*(End of Pitch Deck)*

*Note: Images and graphics (previously planned) are omitted in this text version, but would include screenshots of the app interface, icons representing voice/AI, and an illustrative user persona to humanize the story.* 

## 3. Comprehensive PRD (Product Requirements Document)

### Core Functionalities & User Stories
DeepType’s product requirements focus on delivering a **fully accessible, intelligent typing tutor**. The core functionalities include:

- **Interactive Audio Lessons:** The system provides step-by-step typing lessons through audio prompts. *User Story:* *“As a blind beginner, I want the tutor to tell me which fingers and keys to use so I can learn touch typing without sight.”*  
  - The lesson content ranges from learning home row keys to complex sentences. Each lesson is articulated by a pleasant human-like voice (text-to-speech).
  - The user can control playback: say “repeat” or press a key to hear instructions again. The lesson flows at the user’s pace, waiting for input and giving feedback.
- **Real-time Feedback and Error Correction:** As the user types, DeepType instantly checks input. If the user presses the correct key, they get positive feedback (a chime sound or voice “Good!”). If wrong, the system signals it (buzz sound or “Oops, try again, that was X, not Y”). *User Story:* *“If I mistype, I want immediate correction so I know what to fix.”*  
  - The system identifies which key was pressed in error and can speak its name (“You pressed K, but the target was F”). This reinforces learning of key positions.
  - Errors are logged to adapt difficulty (e.g., if user consistently struggles with a particular hand or letter, the AI will introduce extra practice for it).
- **Voice Commands & Navigation:** DeepType supports a set of voice commands to navigate the app hands-free. *User Story:* *“As a user who can’t see the screen, I want to be able to speak commands like ‘next lesson’ or ‘main menu’ to control the app.”*  
  - Key commands: “Start lesson one”, “Pause”, “Resume”, “Repeat”, “Menu”, “Help”. Alternatively, a single keyboard key (like the spacebar or a special “DeepType Key”) can cycle through options and select them, accommodating one-switch users.
  - The app includes a voice-controlled onboarding/tutorial where it teaches the user how to use these voice or single-key controls (with practice, e.g., “Press the spacebar to select an option now”).
- **User Progress Tracking:** Each user has a profile with their lesson progress, typing speed (WPM), and accuracy stats. *User Story:* *“I want to track my improvement over time and resume where I left off.”*  
  - Achievements or badges are unlocked for milestones (e.g., 5 lessons completed, first 30 WPM speed, etc.) to encourage progress.
  - Data like last lesson completed, current difficulty level, and custom preferences (voice speed, theme) are saved (likely in a cloud database if logged in, or locally if offline).
- **Adaptive Lesson Planning (AI-driven):** The content adapts to the user’s performance. *User Story:* *“If I’m finding something too easy or too hard, I want the tutor to adjust so I’m always appropriately challenged.”*  
  - If the user breezes through lessons with high accuracy, the AI might skip some redundant practice or suggest a more advanced exercise (e.g., move from letters to words sooner).
  - If the user struggles, the AI can inject an extra practice session focusing on troublesome keys, or slow down the pace of new key introductions.
  - This is powered by a rules engine enhanced with machine learning: initially simple (if accuracy < X, repeat lesson), later more complex using pattern recognition (e.g., “user often swaps S and A – maybe do a drill contrasting those letters”).
- **Multimodal Teaching Aids:** While primarily audio, DeepType can optionally display visual aids for those who have some vision:
  - An on-screen keyboard graphic highlighting the key to press (with high contrast colors).
  - Large-print text of the current exercise (e.g., the word or sentence to type) in case low-vision users want to follow along visually.
  - These aids have proper alt-text or ARIA descriptions so even if a blind user accidentally focuses them, the screen reader will say something like “Visual keyboard illustration, current key: F” rather than leaving them in the dark.
- **Content Variety & Gamification:** To keep engagement, the PRD includes mini-games and varied content:
  - Typing games that are audio-based (for example, an audio version of “falling words”: the voice says a random word and the user must type it before another word “falls” – represented by a ticking timer or rising tone; this becomes a score challenge).
  - Fun exercises like typing the lyrics of a song (with the music playing in the background if they want), or typing to control a simple audio game (e.g., “hit the spaceship by typing the letter that corresponds to its coordinate” – this would be described via audio).
  - These are stretch goals, but mentioned in PRD to ensure extensibility of the content engine to support game modes in addition to linear lessons.
- **Accessibility Compliance & Settings:** DeepType will meet or exceed all relevant accessibility guidelines.
  - **Settings include:** TTS voice selection (if multiple voices available), speech rate adjustment, verbosity level (novice mode might speak very verbosely including each character, while advanced mode might assume more and speak less), high contrast toggle, and an option to integrate with existing screen reader (for those who prefer their JAWS voice for consistency – the app could output via screen reader instead of its own TTS).
  - It will also have an **“Audio-only mode”** which ensures that all necessary information is spoken without requiring any visual element (for pure blind usage with no screen).
  - Conversely a “Visual Assist mode” can provide on-screen hints for sighted or low-vision supporters (like a parent or teacher watching can see what’s going on). This dual-output approach ensures both blind users and sighted helpers get what they need.

These core features are driven by our guiding principle: **teach typing in a way that feels natural and supportive to someone who cannot see.** The PRD ensures that any feature that involves output must have an audible form, and any input must be possible via keyboard or voice (not just mouse/touch). 

### Technical Stack Overview
To implement the above, DeepType leverages a modern and flexible technical stack, stitching together AI services, cross-platform frameworks, and accessible UI libraries:

- **Frontend:** We plan to use **Web technologies (HTML, CSS, TypeScript/JavaScript)**, likely within a framework like **React** for modularity. The UI will be a web app at its core for easy cross-platform reach. We might employ frameworks or libraries that aid in building accessible components (e.g., Reach UI or ARIA toolkit for React, which provide pre-built accessible widgets).
- **Lovable.dev & Bolt.new (AI-assisted development):** To accelerate development, we will experiment with AI-powered coding tools. **Lovable.dev** can scaffold a project quickly – generating a React/TypeScript app with our described ([Lovable.dev - AI Web App Builder | Refine](https://refine.dev/blog/lovable-ai/#:~:text=It%20acts%20as%20your%20AI,it%20all%20with%20remarkable%20efficiency))†L71-L79】. It acts as an “AI co-engineer,” setting up the skeleton (routing, basic pages, API integrations with Supabase, etc.) from natural language specs. **Bolt.new**, an AI-powered development en ([Bolt vs. Cursor: Which AI Coding App Is Better?](https://www.thepromptwarrior.com/p/bolt-vs-cursor-which-ai-coding-app-is-better#:~:text=Bolt%20%28bolt.new%29%20is%20a%20web,powered%20development%20environment))†L62-L70】, will be used to prototype interface components rapidly. For example, by prompting “Create a high-contrast landing page with a ‘Start Lesson’ button and our logo,” we can get boilerplate that we then refine. These tools don’t replace coding but speed up initial setup and repetitive tasks, allowing the team to focus on complex logic. *Rationale:* This fits our lean startup approach – we leverage AI to build AI software faster.
- **Backend:** The backend is relatively lightweight – mostly to handle persistent data and AI integration:
  - **Supabase (PostgreSQL database + Auth):** We choose Supabase as our primary backend platform. It provides an **open-source Firebase alternative** with a Postgres database, restful APIs, real-time subscriptions, and user authentication out-o ([Best backends for FlutterFlow: Firebase vs Supabase vs Xano](https://www.lowcode.agency/blog/best-backends-for-flutterflow#:~:text=Best%20backends%20for%20FlutterFlow%3A%20Firebase,time%20databases%2C%20authentication%2C))†L33-L40】. This means we can store user profiles, progress logs, achievements, etc., with minimal backend code. Supabase’s auth will manage sign-ups (with email/password or OAuth if needed) and we can secure data (each user’s data is private, etc).
  - Supabase’s real-time capabilities might be used to sync live progress (for instance, if a teacher is remotely monitoring a student’s lesson, the keystrokes per minute or errors could stream to a dashboard in real-time).
  - We also plan to use Supabase’s storage (for any audio clips or if we allow users to upload custom text content to practice on).
- **AI Services:** A cornerstone is integration of advanced AI:
  - **OpenAI API:** We will utilize OpenAI’s APIs for a couple of purposes. One is the **GPT-4 (or GPT-3.5) model** for language tasks – e.g., generating dynamic practice sentences or engaging trivia about what the user is typing (imagine as they practice, the AI shares a fun fact: “Did you know the word ‘TYPE’ originated from…” to keep things interesting). Another is potential use of OpenAI’s new multimodal/voice features. For instance, OpenAI’s Whisper model for speech-to-text and their text-to-speech for voice output. The **OpenAI Whisper API** can transcribe audio (like user’s spoken commands) with high accuracy. While it doesn’t yet support true streaming real-time transcription in the API, we can chunk audio every few seconds to simulate  ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%20am%20aware%20that%20currently,intentions%20to%20make%20this%20live)) ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%E2%80%99ve%20heard%20of%20people%20sending,to%20chunk%20smartly))†L23-L31】. We’ll design our voice command system around this limitation (short commands which can be captured in <5s chunks, so the delay is negligible).
  - **Google Gemini 2.0 (Flash API):** As we future-proof, we note Google’s upcoming **Gemini 2.0** from DeepMind, which promises **multimodal support and lightning-fast  ([Google | Gemini 2.0 Flash API - Kaggle](https://www.kaggle.com/models/google/gemini-2.0-flash-api#:~:text=Google%20,arrow_drop_up%2027))5†L5-L13】**. Once available, this could power the adaptive coach in DeepType, possibly providing even quicker or more nuanced feedback than GPT. For example, Gemini might be used to analyze a user’s pattern of mistakes in depth or run a real-time conversation mode with the user about their progress. The “Flash API” suggests real-time or streaming capabilities, which could help in creating an interactive agent that feels live. The PRD includes support to integrate Gemini as an alternative or supplement to OpenAI (keeping our architecture model-agnostic so we can plug in the best AI).
  - **Text-to-Speech (TTS):** We have options here: use the Web Speech API in browsers for on-device TTS (which leverages system voices) *and/or* use cloud TTS (e.g., Google Cloud Text-to-Speech or Amazon Polly) for consistent high-quality voices across platforms. For simplicity and offline support on desktop, we will use on-device TTS where available. On mobile, we can use the platform’s native screen reader voice via an API or our own integrated voice. The PRD requirement is that the voice must be clear and preferably natural (neural voices). We’ll research which approach yields the best combination of latency and quality. It might be viable to ship a pre-trained lightweight TTS model for offline (for example, Coqui TTS or similar open-source) for an offline desktop mode, while using cloud for online mode.
- **Cross-platform Frameworks:** To achieve **“web, desktop, mobile-native without re-coding”**, we outline a cross-platform strategy:
  - The core is a Progressive Web App (PWA) built in React/TypeScript. This runs in any modern browser (fulfilling the web requirement).
  - **Desktop:** We will use **Tauri** or Electron to wrap the web app into a desktop application for Windows/Mac/Linux. Tauri is preferred for its lightweight, security, and ability to create native binaries with a web ([tauri-apps/tauri: Build smaller, faster, and more secure desktop and ...](https://github.com/tauri-apps/tauri#:~:text=tauri,end%20framework%20that%20compiles))1†L5-L13】. With Tauri, our web code becomes a desktop app that can access local resources (e.g., the microphone for voice input) and run offline. Tauri allows building installers and an app experience that integrates with the OS (start menu, etc.), all while reusing 99% of our code.
  - **Mobile:** For mobile native, we have a couple of paths. We can package the PWA as an app using something like **Capacitor (from Ionic)** which allows deploying web code as native iOS/Android apps with access to native APIs (for mic, vibration, etc.). Alternatively, React Native could be used with the same business logic, but that’s a separate codebase unless we unify via something like Expo’s web support. Given our resource constraints, we lean towards using the web PWA directly on mobile (modern iOS and Android browsers support many necessary APIs). We ensure the PWA is installable (Add to Home Screen) and works offline after first load (using Service Workers for caching). This way, a user can “install” DeepType from the browser and use it like a native app. In parallel, if needed for App Store presence, we can wrap the PWA in a minimal native shell (Capacitor) and publish it. This still avoids rewriting logic.
  - **Shared Code:** We will maintain a single codebase for logic. For any platform-specific code (like file access on desktop, or different speech API on web vs mobile), we use conditional wrappers or services. E.g., an `AudioInputService` interface with implementations: one uses the browser `webkitSpeechRecognition` (for Chrome), another uses the Cordova/Capacitor plugin for speech on mobile, another perhaps uses an Electron node module or the OS’s Speech API for desktop. The app at runtime picks the appropriate one. This design is specified in the PRD to ensure we prep for multi-environment deployment.
- **Cursor AI & Vercel v0.dev (Dev Tools):** While not part of the product delivered to users, internally we incorporate tools like **Cursor (an AI-assisted code editor)** for improving our development speed and code quality. Cursor can help write repetitive code or tests by conversing with the codebase. **V0.dev** by Vercel is another tool that can generate UI components from descriptions, particularly for React + Tailwi ([Vercel v0.dev: A hands-on review · Reflections - Ann Catherine Jose](https://annjose.com/post/v0-dev-firsthand/#:~:text=Vercel%20v0.dev%3A%20A%20hands,CSS%2C%20and%20shadcn%20UI%20components))3†L5-L13】. We will use v0.dev for designing accessible UI components; for example, describe “a large high-contrast toggle switch with ARIA roles” and refine the output. This ensures even the development process keeps accessibility in mind (the AI is likely to include ARIA attributes if we specify).
- **Realtime Collaboration & API**: If we allow a teacher or remote volunteer to monitor or assist a session, we might use WebSockets or Supabase’s realtime channels. This is a potential feature for v2, but the architecture includes the ability (maybe via a Node.js server using **Socket.io** or Supabase realtime) to broadcast events (like “user completed Lesson 2”) to a connected dashboard. 
- **Testing and CI:** The PRD requires that we integrate automated accessibility testing in our CI pipeline. Tools like **axe-core** (by Deque Systems) can be used to run accessibility audits on our interface to catch issues (like missing labels) early. Also, we plan to include unit tests for critical logic (especially the adaptive algorithm – we can write tests simulating sequences of mistakes and ensuring the lesson adaptation logic does what we expect).

In summary, the technical stack is a blend of **AI services (OpenAI, possibly DeepMind)**, **cloud backend (Supabase)**, and **web-centric cross-platform frameworks (React, Tauri/Capacitor)**. This allows us to meet the broad requirements: intelligent behavior, data persistence, and deployment on multiple platforms with one codebase. By using AI coding aids (Lovable.dev, Bolt.new, Cursor), we also significantly reduce development time, enabling a small team to achieve results comparable to a much larger team – a critical advantage for a startup.

### Cross-Platform Architecture Plan
To ensure DeepType runs seamlessly on web, desktop, and mobile **without duplicating effort**, the architecture is carefully planned:

**Overall Architecture:**  
At its core, DeepType is a single-page application (SPA) that communicates with cloud services as needed. Think of it as a layered design:
- **UI Layer:** React components (or similar) render the interface and manage interactions. This layer cares about things like showing a virtual keyboard, displaying text, capturing keypress or microphone input.
- **Logic Layer:** This is the heart – lesson logic, state management (could use something like Redux or React Context for state), and the adaptive algorithm. It’s abstracted so it doesn’t directly depend on browser APIs – meaning it can run in any JS environment (browser, Node, React Native).
- **Platform Services Layer:** These are small modules that handle platform-specific functions: e.g., `SpeechInputService`, `SpeechOutputService`, `StorageService`. The app, instead of calling `window.speechSynthesis.speak` directly, will call an interface method `speak(text)`. In the web build, that maps to `window.speechSynthesis`; in a mobile build, it could call a native plugin; in desktop, maybe it calls an OS-level TTS or uses the web one as well. Similarly for listening: on web, use `SpeechRecognition` API if available; on desktop, possibly the OS dictation API or route audio to our server’s Whisper. We design these as swappable modules.
- **Backend API Layer:** When the app needs to fetch or save data (login, pulling down a new set of practice sentences, updating progress), it calls our backend via REST or GraphQL (Supabase provides RESTful endpoints and client libraries). There’s also calls to AI APIs: those might be direct from the frontend (for less sensitive requests and if CORS allows) or via our backend proxy (for secure calls that involve API secrets). For instance, generating a custom exercise using GPT might be done by sending a request to our backend function `generateExercise` which then calls OpenAI and returns result, so we don’t expose the API key on the client.

**Web App:** 
- Runs entirely in the browser. On modern browsers, even voice is possible: Chrome’s implementation of the Web Speech API allows real-time-ish speech recognition (though not standard across all browsers yet). The web app will detect capabilities: if using Chrome, can enable full voice commands locally; if not, it might fall back to sending audio to server or require keyboard control for commands.
- The web app is the primary development target (fast iteration with hot-reload, devtools, etc.). It will be responsive (using CSS flexbox or grid) so it can fit small mobile screens up to large desktop screens with reflow. We’ll likely implement a simplified layout for mobile (bigger buttons, maybe hide visuals) versus desktop (which could show more info at once).
- We ensure PWA compliance: a service worker for offline caching of assets and perhaps an offline mode where a set of lessons are available without internet. The app manifest will allow it to be installed on home screen.

**Desktop App:** 
- Using **Tauri**, we package the same web app. Tauri essentially provides a Rust core that loads our HTML/CSS/JS and presents it in a native window (using the system’s webview). It also allows calling native code via an API bridge. We will use that for deeper integration: for example, file system access if we want to log data locally, or to use system-level TTS if needed. But we’ll minimize divergence – ideally the app behaves the same as in a browser.
- Building for desktop will produce .exe for Windows, .app for Mac, etc. All logic still runs in JS in the webview, but with optional assists from Rust. Tauri’s security means we should explicitly allow any API calls from JS to Rust (limiting risk). According to Tauri docs, you can build small (a few MB) apps since it doesn’t bundle a full Chromium like ([tauri-apps/tauri: Build smaller, faster, and more secure desktop and ...](https://github.com/tauri-apps/tauri#:~:text=tauri,end%20framework%20that%20compiles))1†L5-L13】.
- We’ll include auto-update in the desktop version (Tauri has mechanisms or we can roll our own checking our server for updates).
- For desktop, one goal is working fully offline. With everything packaged and using on-device TTS, a user could install and run DeepType on a computer with no internet (good for secure environments or those without connectivity).
- One challenge: speech recognition offline. If the PC has no internet, Chrome’s engine won’t work (it sends to Google servers). We may implement a setting “offline mode” where voice commands are disabled or limited to what we can do locally. There are local speech recognition projects (Vosk, Coqui STT) that could be integrated in the desktop via the Rust side for offline STT. That’s an advanced feature – PRD marks it as a possibility if we target truly offline voice input.

**Mobile App:** 
- If running as pure PWA: On Android, Chrome will allow install and mic usage easily. On iOS, Safari PWA has gotten better and does allow some offline caching and even speech synthesis via Web Speech. Speech recognition on iOS Safari might not be available (as Apple hasn’t enabled the API as of iOS 16/17), so on iPhone the voice commands might be limited unless we use a hack with an external service. For the best experience on mobile, a native wrapper with Capacitor is ideal:
  - **Capacitor** wraps our web code into a WebView inside a minimal native app. We get access to Cordova/Capacitor plugins for things like SpeechRecognition (which under the hood could use Siri’s transcription or just present a native prompt). We’d use the capacitor-community speech recognition plugin, and text-to-speech plugin for consistency.
  - With that, we can publish to App Store/Play Store. The code remains the same, just including capacitor JS bridge scripts and some config.
- Mobile considerations: touches and gestures. A blind user on mobile might use VoiceOver or TalkBack screen reader. We need to ensure compatibility (the app’s elements must be properly labeled so if VoiceOver is running, it can read the “Start Lesson” button, etc.). Alternatively, the user might rely solely on our in-app voice and not the system screen reader. We must handle both gracefully:
  - Possibly detect if a screen reader is active (some OS allow detection) and then adjust (for example, if VoiceOver is on, we might not use our custom gestures to avoid conflict).
  - The single-switch idea on mobile: a Bluetooth switch or just tapping anywhere on screen as the one button. We can implement a full-screen invisible button for “select”, and have an automated scanning focus (which the user hears). This is complex but doable; however, voice commands largely cover it too.
- Performance: Mobile devices have limited processing for AI. But since heavy AI (like GPT) calls are cloud-based, and TTS can be offloaded, the client just needs to handle audio playback/recording and UI. We’ll test on mid-range devices to ensure smooth audio and no lag in typing feedback.

**Security & Privacy:** Given we’re dealing with potentially personal data (user progress, maybe voice recordings), architecture includes:
- Using secure communications (HTTPS for all API calls).
- Storing minimal personal data (maybe just email and performance metrics). All sensitive AI processing (like speech or adaptive suggestions) can be done on the fly and not stored, or if stored, anonymized.
- If we allow cloud sync of user data, we’ll ensure compliance with privacy laws (GDPR etc.), provide data export/delete options.
- The voice data: if we do send voice to our servers (like for Whisper transcription), we’ll do it via secure WebSocket or HTTPS and not retain the raw audio after transcription (unless user opts in to share for improvement).
- These details would be in the PRD under a “Non-functional Requirements” section: performance (e.g., “The app should have <200ms latency for keystroke feedback”), security (“User data encrypted at rest and in transit”), and accessibility (which we treat as functional requirement actually, given the nature).

The cross-platform plan in summary: **One codebase, modular design, deploy everywhere.** This approach minimizes redundant work and ensures feature parity across platforms. Users can start on one device and continue on another seamlessly. For instance, a student practices on a PC at school, then later on their phone at home – DeepType will sync their progress via Supabase so the experience continues. This ubiquity is a strong point in our PRD because many existing tools are limited to one platform (e.g., legacy typing software on Windows only). We’re essentially making DeepType available wherever the user is, ensuring consistency and convenience.

## 4. SWOT Analysis & Gap Analysis

To assess DeepType’s strategic position, we conduct a **SWOT analysis** (Strengths, Weaknesses, Opportunities, Threats) along with a **Gap Analysis** of the current market solutions versus user needs.

### SWOT Analysis

**Strengths:**  
- **Accessibility Expertise:** DeepType is designed with accessibility at its core. This specialized focus is a strong differentiator – few competitors can claim the same level of built-in support for blind users. We have first-mover advantage in this niche.  
- **AI-Powered Adaptivity:** Our use of AI (GPT-4, adaptive algorithms) provides a personalized learning experience that static software cannot match. This not only improves effectiveness but also gives us a tech prestige (“the most advanced tutor out there”).  
- **Cross-Platform Reach:** Being available on web, desktop, and mobile greatly expands our user base. Schools that use Windows PCs, individuals on Macs, and users in developing countries who primarily have Android phones – all can use DeepType. This ubiquity is a strength.  
- **Backed by Empathy Labs Vision:** The strong branding and vision (Empathy + Deep Tech) builds trust. Stakeholders are more likely to support a mission-driven product. It’s not just software, it’s a cause. That can galvanize community support, volunteer contributions, etc.  
- **Community and Cost Advantage:** If we keep the core free, we align with the NVDA approach which saw massive communi ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=NVDA%20is%20again%20the%20most,of%20respondents))35†L1-L4】. This can turn users into evangelists. Also, open-sourcing parts can harness community development (strengthening the product beyond our internal capacity).

**Weaknesses:**  
- **Limited Initial Content/Scope:** As a new product, we might launch with a limited curriculum or features compared to mature competitors. For example, Talking Typing Teacher has a whole suite of lessons, games, and a word processor; we might not have all that on day one. Users might find content not deep enough if we don’t rapidly expand it.  
- **Reliance on AI/Internet:** Some features (like voice AI or advanced adaptivity) rely on internet connectivity and third-party APIs. In scenarios of no internet, our experience might degrade compared to offline software that’s fully self-contained. Also, API costs (OpenAI etc.) could become a burden if usage scales and isn’t monetized proportionally.  
- **User Adoption Hurdle:** Ironically, reaching the target users can be challenging. Many visually impaired learners depend on instructors or institutions to recommend tools. Convincing these gatekeepers (teachers, rehab specialists) to try a new product is a slow process. We also must support users who are not tech-savvy – the onboarding must be foolproof. Any small usability issue for a blind user could turn them away. So our margin for error is slim, especially with such a discerning audience that has been trained to rely on known solutions.  
- **Small Team & Resources:** Initially, we are likely a small team. Implementing and maintaining multi-platform software with heavy AI might stretch our resources. We’ll need to prioritize carefully. Lack of certain domain expertise (e.g., if none of us are blind, we might misjudge some UX aspects) could be a weakness – that’s why involving beta users and experts early is crucial.  
- **Unproven Effectiveness:** We believe our approach is superior, but we will need data to prove learning outcomes. Until we gather success stories or studies, some educators might be skeptical if AI adaptivity truly yields better results than, say, structured repetition. We might also face scrutiny: “Does this actually teach touch typing effectively, or is it too fancy?” Overcoming that initial doubt is a challenge.

**Opportunities:**  
- **Market Void / Blue Ocean:** The niche of *accessible typing tutors* is under-served. The main products are few and dated (TypeAbility, Talking Typer, etc.), leaving a large global user base with needs not fully met. This void is our opportunity to become the de facto solution worldwide, much like how JAWS became synonymous with screen reader in the ’90s, or how **Be My Eyes** became essential on smartphones for visual assistance. We can *define* the category.  
- **Institutional Partnerships:** Schools for the blind, vocational rehab centers, libraries, and disability organizations are actively looking for tools to improve digital literacy. For instance, partnering with national blindness federations or ministries of education could give DeepType an official channel to thousands of users. There are also grants in assistive tech we can tap into (e.g., governments funding tech for inclusive education).  
- **Technology Trends:** Voice assistants and voice UIs are trending. People are increasingly comfortable talking to devices (Alexa, Siri, etc.). DeepType rides this wave as a *voice-first educational app*. We can piggyback on this trend in marketing, and even possibly integrate with those ecosystems (imagine an Alexa skill “typing practice” that is a subset of DeepType’s functionality – a stretch idea, but possible).  
- **Expansion to Other Skills:** Once we establish ourselves in typing, the underlying tech (voice interaction + adaptive learning) could apply to other skills for visually impaired users: using a touchscreen, learning braille through audio drills, coding tutorials (teaching programming with spoken guidance), etc. DeepType could evolve into a suite (“DeepSkills” platform). The opportunity is to leverage our tech platform for multiple products, increasing ROI on our R&D.  
- **Corporate Accessibility Compliance:** Companies are under pressure to be more inclusive. DeepType could be marketed to employers to help upskill blind employees or as a tool for diversity training. It might even be used by sighted employees to practice screen reader mode or keyboard-only usage as empathy training. That’s a tangential use case, but an opportunity for B2B sales beyond the obvious.  
- **Competitive Weaknesses:** Our competitors have known weaknesses we can exploit:
  - *JAWS and TypeAbility:* Expensive and thus not accessible to many (JAWS is $90/year or $1200 ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】; TypeAbility requires JAWS to run, doubling cost). We can undercut by being free/low-cost.
  - *Talking Typing Teacher:* No longer actively supported (as noted, manufacturer offers no tec ([Talking Typing Teacher - Standard](https://www.maxiaids.com/product/talking-typing-teacher-standard#:~:text=,Please%20reference%20instructions))). If it’s abandonware, users will gladly switch to a modern supported product.
  - *General Typing Software:* They ignore blind users entirely – an open goal for us to score. Also, even for sighted users, many find traditional typing tutors boring – our unique voice-interactive approach might attract a subset of mainstream users (like those who prefer auditory learning or have ADHD and enjoy a more interactive style).
- **Global Reach & Localization:** Many developing countries have a growing population of visually impaired individuals with increasing access to tech (cheap Android phones, etc.), but absolutely no local-language typing tutors. We can lead in localization – using our AI to generate lessons in various languages quickly. The opportunity to become the go-to solution in non-English markets is huge (and the competition there is basically zero beyond English). This not only is good mission-wise but also opens avenues for funding from international agencies focusing on literacy and disability.

**Threats:**  
- **Competition from Big Tech:** If our idea proves the market, big players might step in. For instance, Microsoft could enhance their *Seeing AI* app or Windows’ Narrator to include a typing tutor mode. They have resources and existing user base (Seeing AI is free and popul ([Seeing AI - Apps on Google Play](https://play.google.com/store/apps/details?id=com.microsoft.seeingai&hl=en_US#:~:text=Seeing%20AI%20is%20a%20free,blind%20and%20low%20vision%20community))†L11-L18】; JAWS or NVDA could implement a tutorial feature in future versions). If an OS-level feature appears, it’s a serious threat (why install DeepType if Windows teaches typing out-of-the-box?). Our defense is to move quickly and establish brand loyalty and superiority before that happens.  
- **Rapid Tech Changes:** The AI we rely on (OpenAI, etc.) is evolving. There’s a risk of API price increases or policy changes (for example, if OpenAI changes how their educational use licenses work or if a free tier is removed, etc.). Also, new AI models might outshine our approach, requiring continuous integration. If we fail to keep up with the latest (like not adopting Google’s superior model once out), a competitor could and get an edge.  
- **Funding Risks:** As a mission product, if we don’t get the right funding, development could stall. It’s a threat that the project might not sustain purely on revenue early on (because we plan to subsidize users). We mitigate this by pursuing partnerships and proving value to paying customers ASAP.  
- **User Adoption Risks:** The visually impaired community often relies on word-of-mouth and is careful with new tech (due to many products over-promising and under-delivering). If our early version has bugs or inaccessible pieces, word could spread and tarnish our reputation. We could get labeled as “not truly accessible” which is hard to recover from. So quality and community engagement are crucial to avoid the threat of negative perception.  
- **Legal/Compliance:** If we were to collect voice data, there are privacy concerns. Mishandling user data could lead to legal issues or distrust. Also, as an educational tool, we need to be careful with claims – if we operate in the EU, we might need to comply with GDPR, etc. Not doing so could shut us out of key markets. While not an immediate competitor threat, regulatory issues can threaten the project’s reach (for instance, needing to ensure our TTS voices are licensed for our use case, etc.).  
- **Competition from Adjacent Fields:** An indirect threat: a mainstream typing tutor might add an “accessibility mode” cheaply (like just adding voice prompts). Even if it’s not as good as DeepType, if it’s part of a product that’s already deployed widely, it could suck away potential users. For example, if Typing.com (a popular free web tutor) decided to add a screenreader-friendly mode, schools might just use that rather than try something new. We have to stay ahead by truly outperforming any half-measures others might do.

### Gap Analysis (Unmet Needs in Assistive Typing Education)

The gap analysis compares **user needs** (especially those of blind/low-vision learners) against what existing solutions provide, highlighting where those solutions fall short and how DeepType fills the gap:

- **Need:** *Accessible, Non-Visual Guidance.*  
  **Gap in current solutions:** Traditional typing software assumes you can see on-screen instructions or hands. Blind users instead use specialized programs or screen readers, but these often provide minimal guidance (maybe just spoken letters). Tools like Talking Typing Teacher addressed this with recorded  ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))137-L146】, but it’s not interactive beyond fixed lessons. Modern screen readers have a “keyboard learn mode” (press a key and it announces it), but *no structured lessons or progression*. **DeepType’s Fill:** We provide a full curriculum via audio, not just isolated key feedback. It’s interactive and **context-aware voice guidance**, which currently no mainstream or assistive tool fully offers (existing ones either have voice but no AI interactivity, or interactivity but not voice-first). We also allow voice input for control, which none of the old tutors do – they required keyboard navigation through menus, which is cumbersome.
- **Need:** *Adaptive Learning Pace.*  
  **Gap:** Current teaching programs for the blind are one-size-fits-all. For example, TypeAbility has 99 lessons that everyone goes through the same way. If a student already knows some touch typing, they can’t easily skip ahead without manually picking lessons. Conversely, if they struggle, the software doesn’t custom-tailor more practice; the teacher would have to manually repeat lessons. **DeepType’s Fill:** The adaptive AI engine ensures each learner gets a customized experience – essentially a personal tutor adjusting on the fly. This is a major gap we fill; none of the existing products leverage AI or adaptive algorithms. We cite how modern AI can do this, e.g., *“NVDA and JAWS don’t teach, and older tutors don’t adapt – DeepType is the first to personalize typing education in real-time.”*
- **Need:** *Engagement & Motivation.*  
  **Gap:** Learning to type by touch is tedious, more so when done with bland software. Many visually impaired students lose interest in current tutors because they involve repetitive drills with monotonous feedback. Gamification is minimal (Talking Typer might have some games, but audio games were limited due to tech of its time). Also, immediate encouragement is something a human teacher gives – software often just says “incorrect” in a flat tone. **DeepType’s Fill:** We use gamified elements and lively, *human-like encouragement*. The AI voice can vary phrases, crack a mild joke, or reference earlier progress (“You consistently get F right now – fantastic improvement!”). By tracking progress and awarding badges, we introduce a gaming element. There’s currently a gap in turning typing practice into something fun for blind users; DeepType intends to fill that with audio games and challenges. Even small touches like different sound effects for successes vs. mistakes can improve engagement, and our design documents emphasize a rich audio feedback scheme, whereas older software might just beep or say “wrong” in the same tone.
- **Need:** *Modern Platform Support.*  
  **Gap:** A huge practical gap is that many existing solutions run only on certain platforms (mostly Windows). For instance, Talking Typing Teacher is a Windows program from decades ago (does it run on Windows 10/11 easily? Possibly with compatibility mode). Mac users or mobile users have zero options in that category. In today’s world, a learner might want to practice on their phone or tablet – currently not possible with specialized typing tutors. **DeepType’s Fill:** Cross-platform availability. We meet users where they are. No current competitor offers a mobile app for typing for the blind. DeepType will likely be the first to have that. This is a critical gap we fill: *accessibility of the tutor itself* on different devices.
- **Need:** *Support & Community.*  
  **Gap:** As noted, some older products are no longer supported (no updates, no support lines). If a user hits a bug or compatibility issue, they’re stuck. Also, there’s no community around them (perhaps some mailing lists, but nothing active). **DeepType’s Fill:** As a new, mission-driven product, we plan to build a community forum where users can share experiences, ask questions, and where we (developers) actively respond. This community aspect plus active support (even if via email or chat) addresses the frustration gap. Also, if DeepType is offered partly open-source or free, communities of volunteers (like translators, or those making lesson content) can form, which doesn’t exist currently for closed old software.
- **Need:** *Affordability.*  
  **Gap:** The cost of some solutions (JAWS + TypeAbility combo, or even just JAWS’s own training material) is high. Many individuals in low-income settings cannot afford these. There is a clear gap for a *free or low-cost solution*. NVDA filled that gap in screen readers, but for typing tutors, NVDA only goes so far with its basic help mode. **DeepType’s Fill:** Free core offering – we will ensure a student can learn touch typing without paying. This is filling a direct economic gap. By citing JAWS’ cost and NV ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=Which%20of%20the%20following%20desktop%2Flaptop,Dolphin%20SuperNova%2083%205.4)) ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】, we strengthen the argument that free, quality tools see widespread adoption in this space.
- **Need:** *Integrations & Extendability.*  
  **Gap:** Current tools are pretty closed. For example, a teacher can’t easily add custom lessons in Talking Typing Teacher beyond what’s built-in (maybe they have some minor customization, but likely limited). And those don’t integrate with learning management systems or other tools. **DeepType’s Fill:** Because it’s modern and API-driven, we could integrate with other systems. For instance, a teacher could download a report of a student’s progress or we could integrate with a braille display (to output the text being typed in braille for deaf-blind users – a future possibility). Being built on web tech, we also can update content continuously, push new lessons, etc., which older software cannot unless you install an update. So we fill the gap of an *evolving platform* versus a static product.

In summary, the gap analysis reveals that **DeepType addresses multiple unmet needs**: a truly accessible interface, personalized learning, engaging experience, multi-device support, and affordability – none of which are collectively present in any one existing product. This analysis reinforces our value proposition: **DeepType isn’t just an incremental improvement, it’s a generational leap in assistive typing education.** Our strategy is to communicate this clearly to users and stakeholders: we’re solving problems that have lingered for years in this domain.

## 5. Feature Breakdown & Roadmap (Token-Based Deep Research Allocation)

To build DeepType efficiently, we break down the development into features and modules, and allocate “deep research” resources (like focused R&D tasks or use of large context AI analysis, measured in notional tokens) to each. We also consider the complexity (for debugging and planning).

Below is the **feature development order** (roughly in priority) along with an estimation of research and debugging complexity:

- **1. Audio Lesson Engine** – *Priority: Highest (MVP)*  
  *Description:* The core system to play audio instructions, accept keystroke input, and give immediate audio feedback. This includes the basics of lesson scripting (sequences of prompts and expected inputs).  
  *Research Allocation:* **🔎🔎 (2 tokens)** – Requires researching best practices in teaching touch typing (education domain research) and tuning TTS for clarity. The basic approach is known (many tutors do this), so minimal deep AI research needed beyond selecting a pleasant voice and designing effective prompt wording. Some research into phonetics may help (ensuring letters are pronounced distinctly, e.g., “F” vs “S”).  
  *Complexity/Debugging:* **Medium.** Handling keyboard events and timing feedback is straightforward, but we must fine-tune for different typing speeds and ensure no input is missed. Debugging involves making sure fast typists don’t break it and slow typists aren’t rushed. Also must debug how it behaves if user presses wrong keys multiple times, etc.

- **2. Voice Command & Control** – *Priority: High (MVP adjunct)*  
  *Description:* Implementing the ability to navigate menus or trigger actions with speech (and alternatively with a single key). For MVP, focus on a few essential commands like “repeat”, “next”, “pause”.  
  *Research Allocation:* **🔎🔎🔎 (3 tokens)** – We need to do a deep dive on **speech recognition integration**. Specifically, research how to use the Web Speech API or Whisper for near-real-time command recognition. This might involve experimenting with different recognition approaches (browser vs server) and measuring latency. There’s also research on keyword spotting (like maybe always listening for the word “DeepType” or a wake word). We might allocate one token for investigating Web Speech API constraints, one for Whisper API chunk ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%20am%20aware%20that%20currently,intentions%20to%20make%20this%20live)) ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%E2%80%99ve%20heard%20of%20people%20sending,to%20chunk%20smartly))1】, and one for designing a command vocabulary that’s easy to recognize (e.g., avoiding hard-to-distinguish words).  
  *Complexity/Debugging:* **High.** Speech input can be unpredictable (accents, noise). Debugging this involves lots of testing with different voices and ensuring false positives/negatives are minimized. Ensuring that the voice recognition doesn’t interfere with the lesson audio (it might pick up the tutor’s voice – we likely have to mute the recognizer when the tutor is talking, etc.). One-button control also needs careful state management (scanning menus etc., which can be tricky to debug timing).

- **3. Curriculum Content & Progression** – *Priority: High*  
  *Description:* Develop the actual lessons from beginner to advanced. Define each lesson’s content (keys introduced, practice words, etc.), and the progression logic (# of exercises before unlocking next lesson, criteria to pass).  
  *Research Allocation:* **🔎 (1 token)** – Mostly educational design rather than technical. We might use one deep research cycle to review existing touch typing curricula (like what order keys are taught in QWERTY, proven methods such as home row first, etc.). Perhaps consult educational research on typing for blind learners (maybe one exists from APH or Perkins). Use AI to generate a bank of practice sentences that are phonetically diverse and relevant. This is not heavy on algorithm research, more on content quality.  
  *Complexity/Debugging:* **Low/Medium.** Content itself doesn’t “bug out” in the software sense, but ensuring it’s effective is key. We will need user testing to adjust difficulty. The main debugging is ensuring the system properly loads and transitions between lessons according to the curriculum definitions (state machine bugs, etc.). That’s manageable.

- **4. Adaptive Learning Algorithm** – *Priority: Medium-High (after basic version works)*  
  *Description:* The AI that adjusts difficulty and practice based on user performance. Initially maybe rule-based (if error rate > 20%, repeat lesson), eventually ML-driven (pattern recognition).  
  *Research Allocation:* **🔎🔎🔎🔎 (4 tokens)** – This is a core differentiator, so we allocate significant deep research. We’d research:
    - Optimal adaptive learning techniques (perhaps look at how Kahn Academy or language learning apps do it).
    - Possibly use a reinforcement learning or Bayesian model to decide when a user is ready to progress. An AI research token might go into prototyping a model that predicts “mastery” of a key.
    - If using GPT to analyze mistakes, allocate a token to prompt engineering: e.g., feeding it the sequence of user inputs and asking it to suggest which keys to focus on.
    - Research also includes user modeling: how to keep a profile of user strengths/weaknesses. We might dedicate one token to reading academic papers on personalized learning for keyboard or similar.  
  *Complexity/Debugging:* **High.** This feature can get complex, as it introduces a lot of condition branches and data handling. Debugging means verifying the adaptation does what we expect – e.g., does it properly detect patterns? We’ll create simulation tests (feed in a fake user who always messes up certain keys, see if it adapts accordingly). Also need to ensure it doesn’t make the experience erratic (too easy or too hard jumps). The complexity is both in design and in testing the machine learning components. We will likely implement a simpler heuristic first (easier to debug), then gradually hand over to AI suggestions.

- **5. Multi-language & Localization Support** – *Priority: Medium*  
  *Description:* Allow the tutor to teach typing in different languages (different keyboard layouts) and localize the interface and voice prompts to other languages.  
  *Research Allocation:* **🔎🔎 (2 tokens)** – We’d do research on how different keyboard layouts (AZERTY, etc.) should be taught – order of keys might differ. Also research on language-specific considerations (for example, some languages have accented characters, we must handle speaking those). One token for investigating TTS and STT capabilities in target languages (ensuring our chosen APIs have good Spanish, French, etc. voices and recognition). Another token for internationalization framework (how to structure our content so it’s translation-friendly). Possibly use AI to assist in translating content or generating language-specific practice text.  
  *Complexity/Debugging:* **Medium.** Internationalization in code can cause bugs (e.g., text not appearing if locale switch fails, or right-to-left languages messing layout). We’ll need to test each language environment thoroughly. Also if we support switching keyboard layout, the input key codes vs expected letters mapping must be handled – not too hard, but details matter (e.g., on a French keyboard hitting the key labeled ‘A’ actually sends a different scancode). We should plan a robust key mapping module. Debugging that requires physically testing or emulating different keyboard layouts.

- **6. Gamified Modules (Audio Games)** – *Priority: Medium (Enhancement)*  
  *Description:* Develop game-like exercises (e.g., a “typing race” where the user must type words quickly to win, or an audio target practice as described).  
  *Research Allocation:* **🔎🔎 (2 tokens)** – Research audio game design for the blind. There’s a niche community and some existing games (like audio shoot ’em ups). We’d spend tokens on learning what makes audio games fun and how to convey game state via sound. Also maybe research using spatial audio cues (e.g., a sound panning left/right to indicate something). If we incorporate scoring, research psychological aspects of reward systems.  
  *Complexity/Debugging:* **Medium.** Game logic can be complex but contained. The main debugging is making sure the games remain accessible (no unintended need for vision) and that timing loops are correct (e.g., in a fast-paced game, ensure performance on different devices doesn’t lag). There might be interesting edge cases like pausing a game, or what if the user speaks a command during a game unintentionally. We’ll sandbox games so they don’t break the main flow. They are optional modules, so they won’t hold up core launch if issues arise (they can be beta features toggled off if needed).

- **7. User Management & Cloud Sync** – *Priority: Medium*  
  *Description:* Enabling user accounts, saving progress to cloud, and syncing across devices. Also includes the teacher dashboard (for multiple users).  
  *Research Allocation:* **🔎 (1 token)** – Most of this is straightforward use of Supabase (which we already know). Minimal research needed beyond reading Supabase docs thoroughly and perhaps looking into data structures for storing metrics. One token might be spent on security/privacy best practices, ensuring we implement auth securely and consider data encryption. If doing a teacher portal, research compliance with student data regulations (FERPA in US schools, etc.).  
  *Complexity/Debugging:* **Medium.** Integrating auth can introduce edge cases (password resets, offline mode for non-logged in, etc.). Data sync bugs might include duplicate records or conflicts if offline edits are allowed. The teacher dashboard is essentially a separate interface filtering through data – complexity depends on how fancy we get (if just viewing stats, it’s simpler; if real-time monitoring or control, more complex). We will likely phase this: initial just personal accounts, later teacher view. Testing multi-user scenarios and data privacy (ensuring user A can’t see user B’s data) is crucial.

- **8. Accessibility QA & Polishing** – *Priority: Continuous*  
  *Description:* This is not a single feature but an ongoing task: conducting thorough accessibility testing (with screen readers, various assistive tech) and addressing any gaps.  
  *Research Allocation:* **🔎 (1 token)** – We allocate “deep research” for keeping up with accessibility best practices. That means reading updated WCAG guidelines, ARIA techniques, or consulting with accessibility experts. Possibly use an AI to audit our UI code for accessibility issues.  
  *Complexity/Debugging:* **Medium-High.** Debugging accessibility is a bit different: it might be fixing an ARIA label or adjusting focus order – not complex algorithmically, but requires meticulous attention. We will treat any accessibility issue as a high priority bug. For example, if during testing we find that a screen reader announces something incorrectly, that needs fixing. It’s an iterative process with each UI component. Also making sure our one-switch navigation doesn’t trap a user or cause an endless loop is something to carefully test (simulate a user only hitting spacebar and ensure they can do everything, albeit slowly).

- **9. Advanced AI Tutor (“Conversational Mode”)** – *Priority: Low (Future enhancement)*  
  *Description:* A mode where the user can ask the AI questions like “What fingers should I use for this?” or even have a conversational lesson summary (“What did I do wrong today?” – and the AI explains). This leverages the LLM to act like a tutor you can talk to.  
  *Research Allocation:* **🔎🔎🔎 (3 tokens)** – This is a bleeding-edge feature, so heavy research. We’d experiment with prompting GPT-4 to act as a tutor given a transcript of the session. One token on figuring out *how to condense session data for the prompt* (maybe we use a summary), one on *natural language understanding of user questions* (maybe some fine-tuning or prompt library like “if user asks something about typing technique, answer from our knowledge base”), and one on *voice interaction specifics* (maintaining context in a voice conversation, possibly using OpenAI’s conversation mode or multi-turn handling). Also research if any similar conversational tutors exist (for other subjects) to learn from their approach.  
  *Complexity/Debugging:* **High.** This combines many systems: ASR (speech to text) to get the question, the LLM to answer, TTS to speak back. There’s potential for error at each stage (misheard question, or nonsensical AI answer). We need to constrain the AI to be accurate and not give harmful advice. Debugging requires a lot of testing with various questions. We may need to implement a fallback if AI fails (like a static FAQ database as backup). This feature would likely be introduced carefully as a beta with disclaimers.

Each feature/module will be tackled in roughly the above order, though some can be parallel (e.g., content development can happen alongside coding the engine, voice command R&D can parallel basic engine coding). 

The term **“token-based deep research”** implies we will use our access to AI (large context models, web research) in measured chunks to answer key unknowns for each feature. For instance, before implementing the voice command, we might spend one “token” (a dedicated session) with GPT-4 browsing literature on best voice UX practices, and another token to prototype the code using an AI coding assistant. By allocating these tokens, we ensure we do not go in blind on complex features – we first gather insights or even let an AI help structure the solution.

**Debugging Complexity Allocation:** We acknowledge some features will consume more debugging time (e.g., speech-related features, adaptive logic). We allocate our development sprints accordingly:
- The initial Audio Lesson Engine and basic UI gets a large chunk of initial debugging allocation since that must be rock-solid (this is our foundation).
- Voice commands and adaptivity we will release in beta phases, expecting to gather bug reports and iterate.
- We’ll maintain a testing matrix (different OS, browsers, with/without screen readers, etc.) and allocate time to run through it for each major release.

Finally, the roadmap in terms of timeline (as partially outlined in the pitch deck’s slide 8) is:
- **Phase 1 (Months 0-3):** Implement features 1, 2, 3 to deliver a functional MVP for English on web/desktop. Conduct deep research for adaptivity but implement basic logic first. Begin user testing with a small group (maybe from a local blind community or online forum).
- **Phase 2 (Months 4-6):** Implement adaptivity (feature 4) and polish based on feedback. Add more lessons to cover full keyboard. Introduce user accounts (part of 7) if needed for testers. Beta release to broader audience, possibly in partnership with an organization to pilot in a class.
- **Phase 3 (Months 6-9):** Add some gamified exercises (5 and partial 6) to increase engagement. Start mobile packaging. Incorporate localization groundwork (so non-English testers can try Spanish or others by end of this phase).
- **Phase 4 (Months 9-12):** Focus on robust cloud sync and teacher dashboard (rest of 7) for institutional use. Expand language support. Hardening accessibility compliance (8) in preparation for an official 1.0 launch. Maybe start work on the conversational AI tutor (9) in R&D, though that might go into next year’s plan.
- **Beyond:** After core product is stable and widely adopted, roll out the advanced AI tutor as a premium add-on and explore other skill modules.

This feature breakdown and roadmap ensures that we tackle critical needs first (so users get value early), while also setting aside time to delve into complex features with adequate research. By assigning “tokens” of deep research to the hairy problems, we leverage AI and existing knowledge to de-risk our development path. Each completed feature moves us closer to the vision of a comprehensive, intelligent typing tutor that leaves no learner behind.

## 6. Accessibility-First UX/UI Guidelines

DeepType’s UX/UI is governed by a simple rule: **if it’s not accessible, it doesn’t go in the product.** Every interface element and interaction is crafted to be usable by our target users. Here we outline key guidelines and principles that our design and development must follow, ensuring an **accessibility-first** experience:

### Voice-First Interaction Principles
Designing for a voice-first interface means assuming the primary mode of user interaction is through spoken dialogue and audio feedback:
- **Everything is Announced:** The user should never have to guess what’s happening. Whenever the app state changes, a concise announcement is made. For example, when a new lesson starts, it might say, “Lesson 5: Typing words with S and D. Press any key to begin.” When a lesson is completed, it announces the result (“Lesson complete! Accuracy: 90%, Speed: 12 WPM. Great job!”).
- **Conversational Tone:** The voice interactions should feel natural. Instead of robotic commands, we use conversational language. This means using first person (“I will now show you...”) or second person (“You can try that again”) to create a rapport. According to usability research, *voice interfaces should b ([3 Reasons Why It's Time to Talk about Voice UI - Frog Design](https://www.frog.co/designmind/3-reasons-why-its-time-to-talk-about-voice-ui#:~:text=3%20Reasons%20Why%20It%27s%20Time,This))to be effective*. We apply this by giving our TTS prompts some personality (while staying professional and clear).
- **Short Prompts & Confirmations:** Users can’t see a long list of options, so voice prompts should be brief and not overload memory. For example, in a menu, don’t read all options at once if there are many; instead, present them one at a time or in small groups. Use auditory icons if helpful (like a subtle tone indicating more options available). After a voice command is given by the user, the system should confirm it understood (e.g., User: “repeat”, System: “Repeating the instruction: [then repeats].”). This confirmation principle prevents confusion in case the speech recognition misheard something.
- **Tolerance and Recovery:** If the system doesn’t catch a voice command or the user says something unexpected, it should handle it gracefully. Maybe provide a gentle reprompt: “Sorry, I didn’t get that. You can say ‘repeat’ or ‘menu’.” This ensures the user never feels stuck. Designing these flows is critical: every voice prompt in our flowchart has an error handling branch.
- **No Unnecessary Voice Input:** We keep required voice input minimal. While we support voice commands, we don’t force the user to speak at any time if they’re not comfortable. There’s always an alternative like pressing a key or clicking. Voice is offered as a convenience and necessity for some, but *not mandatory*. This principle is inclusive (some blind users are non-verbal or simply shy to speak commands, and some have speech impairments).
- **Environmental Awareness:** Recognize that users might be in noisy environments or around others. We allow the user to use headphones and still operate. For privacy or quiet settings, we might include an option for vibrational feedback or subtle sounds instead of spoken feedback (for example, a user who is deaf-blind might rely on vibrations – although that’s a very niche case, advanced but possible with a Braille display’s vibrate feature or phone vibration on key events). Our UI should consider an option “mute voice output” which then outputs to braille display or just uses beeps for correct/wrong (for those who can’t hear). This is part of being voice-first but not voice-only.

### Single-Button UI Mechanics
We design DeepType such that a user with only one switch or one finger available can still navigate the entire application:
- **Focus Highlight and Scan:** At any given screen (or menu), one element is in focus (virtually). For example, on startup, “Start Lesson” might be the focused item. If the user presses the “Select” button (space bar or a hardware switch), that item activates. If the user does nothing for a moment or presses a different special key (or the same button depending on config), the focus will *move to the next item* and announce it. This is a **sequential scan** mechanism. It’s similar to how many switch-accessibility interfaces work, and also how TV interfaces or old Nokia phones worked. This will be an optional mode if voice commands aren’t used.
- **Timing vs Manual Advance:** Some scanning UIs auto-advance focus after X seconds. We likely prefer manual advance (press to move focus) because timing can be stressful and users might miss the window. Manual gives users full control: e.g., press Tab or a special “next” key to cycle focus, and Space or “select” to activate. We ensure that both the physical keyboard and an on-screen single big button can do these (on touchscreen, perhaps tap = select, long press = next, or vice versa — we have to pick an intuitive mapping).
- **Consistent Layout:** The number of interactive elements at any time is kept minimal to aid scanning. For example, the main menu might have 3 options (Start, Settings, Exit). In a lesson, perhaps only one or two (maybe a “stop” button). By limiting choices, we reduce how much the user must scan through. This is a guideline: keep UI screens simple. If a complex input is needed (like entering an email for signup), we handle it with a special flow that is still single-switch friendly (e.g., an onscreen keyboard that scans through letters group by group, although typing an email might be easier if we allow voice dictation or just let a sighted assistant do that step).
- **Visible Focus Indicator:** For low-vision and sighted support observers, we will have a **high-contrast focus ring or highlight** around the currently focused element. Perhaps a thick yellow outline or a bright glow. This is an ARIA best practice for keyboard navigation – ensure focus is not hidden. We also might enlarge the focused item or put a subtle animation (like pulsing) to catch attention. This helps users who can see a bit to follow along, and it’s also good for any keyboard user.
- **Auditory Focus Indicator:** In addition to the voice reading the focused element (“Settings”), we could have a sound cue when moving focus. For instance, a tick sound each time focus advances, and a slightly different tone when wrapping around back to first item. These auditory cues help users understand the interface structure (like “there were 3 items, I heard 3 ticks and now a wrap-around sound, so I know I’m back at top”). This concept comes from existing screen reader behaviors and auditory UI design.
- **ARIA Roles for Widgets:** We will use ARIA roles to inform assistive tech that our custom scanning UI is a list of menu items. For example, in HTML we might mark the menu container with `role="menubar"` and each option as `role="menuitem"`. This way, if a screen reader is running, it knows to treat them as a menu. Even though we have our own voice, this ensures compatibility. According to MDN, ARIA roles provide semantic meaning so screen reade ([WAI-ARIA Roles - Accessibility | MDN](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles#:~:text=ARIA%20roles%20provide%20semantic%20meaning,yet%20have%20full%20browser%20support))t consistently. We implement roles like `button`, `menuitem`, `slider` (if we had any slider controls) so that any built-in AT recognizes DeepType’s components. Even our custom “focus highlight” that moves could be conveyed via ARIA focus changes (like moving a real invisible focus).
- **No hover dependency, large click targets:** We design for keyboard, not mouse hover. So all interactions must be triggered by focus+activate, never just hover. This is important as many blind users can’t hover. And for motor impaired, hovering might not be possible. We also ensure buttons are large and well spaced (easy to hit with a gaze or a shaky hand if using a switch). This ties into high contrast (next section) – large, distinct buttons.

### High-Contrast Visual Design
While DeepType can be used without looking, we still provide a visual interface that is optimized for low vision and color-blind users:
- **Color and Contrast:** We adhere to at least **WCAG 2.1 AA** contrast ratio (4.5:1 for normal text, 3:1 for large text) and aim for **AAA (7:1)** wherever feasible. The default theme likely will be white text on black background or vice versa, which yields very high contrast. Our palette will be limited to a few colors, used consistently (e.g., one accent color for focus or correct input indicators). We’ll avoid color combinations known to be problematic (like red/green together, since many have red-green color blindness). Any color coding will also have a secondary indicator (like a symbol or text). For example, if we use green text for correct and red for wrong on a visible scoreboard, we’ll also prefix with a “✔” or “✖” symbol so color isn’t the only cue.
- **Font and Size:** All text is in a **clear, sans-serif font** (for readability, e.g., Arial, Verdana, or a specifically accessible font like APHont or Atkinson Hyperlegible). We’ll use a large base font size (at least 18px for body, larger for headings). Users can also customize text size in settings. For dyslexic users, maybe allow a font choice (though our main audience is visually impaired, some low-vision users might also have tracking difficulties, so we consider fonts accordingly).
- **UI Element Design:** Buttons and other controls will have strong outlines and filled shapes to distinguish them. For example, instead of a thin outline checkbox, we might use a big toggleswitch with labels “On/Off”. We’ll ensure focus state of a button is very visually obvious (often default focus indicators are tiny dotted lines – we’ll override that with something bolder).
- **Reduced Clutter:** A sparse design benefits low-vision users who might zoom in. We keep screens uncluttered so that zooming doesn’t hide critical info off-screen. Also, fewer elements means easier high-contrast styling (no need to differentiate many shades). We try to use text and simple icons, avoiding background images or patterns that could reduce contrast or introduce confusion.
- **Dark Mode / Light Mode:** Likely, a dark background with light text is best for many visually impaired (less glare). But some prefer light background. We’ll offer at least these two high-contrast modes out of the box, possibly more (like yellow on black, which some with tunnel vision prefer). The user can choose the scheme that suits their vision. All our color choices will be stored as variables so that switching theme is seamless.
- **Testing:** We will test the interface with common color-blind filters and contrast checkers. Also test on a monochrome setting (imagine someone using a device in high contrast mode where everything is forced black & white – our design should still function). We also consider Windows High Contrast mode (which overrides app colors). Ideally, our app still works if OS forces its palette (which it will if running as a Windows app with high contrast enabled – we should ensure our text doesn’t disappear or something under those conditions).
- **No Reliance on Vision for Key Info:** Even though we make visuals as clear as possible, we still adhere to a rule: anything indicated visually (like “this letter on the on-screen keyboard is highlighted”) is also conveyed via audio. This overlaps with voice-first, but it’s a guideline to ensure equal access. For instance, if a visual user sees a progress bar, a blind user hears a progress percentage spoken. The high-contrast visual is for those who use it, but not mandatory to understand.

### ARIA Roles and Labels for Interface Elements
Using proper **ARIA (Accessible Rich Internet Applications) attributes** is essential to make our custom UI understandable by screen readers and assistive tech:
- **Role Attribution:** Every interactive element gets an appropriate `role`. Buttons will be `<button>` HTML elements or `role="button"` if a custom element. Links use `<a>` or `role="link"`. If we have a custom control (like a toggle or a non-standard widget), we’ll find the closest ARIA role (e.g., `role="switch"` for an on/off toggle, which screen readers announce as a toggle and include state). ARIA roles ensure that assistive tools present and support int ([WAI-ARIA Roles - Accessibility | MDN](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles#:~:text=ARIA%20roles%20provide%20semantic%20meaning,yet%20have%20full%20browser%20support))consistent way.
- **Labels and Descriptions:** We must provide labels for controls that have no visible text. For example, an icon-only button (if we had one) needs `aria-label="Pause lesson"` so screen readers know what it is. Even if there is visible text, sometimes we may want a clearer screen reader label. For instance, a “Next >” button might be clearer as “Next Lesson” for a screen reader, so we’d do `aria-label="Next Lesson"` on it. We’ll also use `aria-describedby` where longer help text is relevant. For example, an input field for a profile name might have a description like “We use this to greet you in the app”, and we link that via `aria-describedby` so the user can hear it if they navigate for more info.
- **Live Regions:** During lessons, there may be dynamically updating text (like a live WPM speed or an error counter). We will utilize ARIA live regions (`aria-live="polite"` or `assertive"` depending on importance) to announce changes. E.g., if we display “Errors: 3”, each time it increments we might announce “Mistake count: 3”. Using a live region allows screen readers to automatically read changes without losing focus context. We’d likely set it to “polite” so it waits until the user is not in the middle of something to announce.
- **Focus Management:** ARIA alone is not enough; we also manage keyboard focus. After certain actions, we might need to programmatically move focus to a logical place. For instance, after closing a modal dialog (like a help popup), return focus to the element that opened it. We’ll follow WAI-ARIA authoring practices for dialogs, menus, etc., which specify where focus should go and using `aria-modal`, `aria-expanded` on toggles, etc. If we make a custom dropdown or pop-up, we ensure to trap focus within it while open and restore on close (these details matter for screen reader and keyboard-only users).
- **Testing with Screen Readers:** We will test with NVDA, JAWS, and VoiceOver to ensure our ARIA labels make sense in the actual announcement. Sometimes what we think is clear might be verbose or awkward when spoken. We might fine-tune labels accordingly. For example, if a screen reader already says “Button”, we don’t need to include the word “button” in our label (to avoid “Start button button”). We’ll use accessible name computation rules to get optimal output.

### Alt Text and Media Descriptions
While DeepType is not image-heavy, any non-text content will have a text equivalent:
- **Logo and Branding:** Our logo on the app (if present) will have `alt="DeepType"` or `aria-label="DeepType logo"` if it’s decorative we might mark it `alt=""` (null alt) to skip it for screen readers. But likely we want the name read.
- **Illustrations:** If there are any illustrations (maybe on a welcome screen or documentation), they will have descriptive alt text. E.g., `alt="A person typing on a keyboard with eyes closed, representing touch typing confidence."` Something that conveys the idea, unless purely decorative in which case null alt.
- **Charts/Graphs:** Not likely in the user interface, but if, say, a teacher dashboard has a progress chart, we’ll provide a summary (e.g., an ARIA live region summary: “Student’s speed increased from 10 to 20 WPM last week”). If we had to include a graph in a report, we’d ensure a textual table or summary is available.
- **Audio Descriptions:** Since our primary output is audio, not visual video, we might not have videos requiring captions. But if there were any instructional videos, we’d need captions and audio descriptions. However, our content is mostly self-voicing interactive, so that covers it.
- **Iconography:** All icons used in buttons will have proper labels. If an icon is purely decorative (like a decorative flourish), we mark it hidden from assistive tech (`aria-hidden="true"` or CSS).
- **ARIA for Non-Text Elements:** If we use a canvas or custom element (imagine a game visualizer or something), we’d use `role="img"` with an `aria-label` describing it, or provide a textual alternative adjacent. But likely, we’ll avoid complex graphics.

### Additional Accessible UX Considerations
- **Tab Order & Logical Navigation:** The tab order (focus order) of elements in the DOM will match the logical order we read them in voice. This prevents confusion for those using keyboard nav. We ensure modals and popups appear logically after triggers in DOM or use appropriate `aria-*` to inform AT.
- **ARIA Alerts for Important Events:** If something critical happens (like connection lost or an error), we use `role="alert"` region to immediately notify the user via screen reader. E.g., if internet goes out and cloud sync fails, an alert might say “Warning: offline, progress will save locally.”
- **No Flashing / Seizure Risks:** We avoid any flashing visuals that could trigger seizures (WCAG guideline to not flash >3 times/sec in high intensity). Our interface is mostly static with subtle transitions, so likely fine.
- **Keyboard Shortcuts:** For advanced users (especially sighted power users or those who prefer keyboard shortcuts to voice), we might implement shortcuts (like Press L to go to Lesson menu, P to pause, etc.). If we do, we’ll ensure to document them and add `aria-keyshortcuts` attribute so screen readers can announce them. This is a nice-to-have that can speed up use without conflicting with normal typing (maybe only active when in menus, not during typing practice to avoid catching normal typing as shortcuts).
- **Form Inputs:** If we have any forms (sign up, etc.), each input has a `<label>` or `aria-label`. Error messages on forms are linked via `aria-describedby` and we use ARIA `role="alert"` on them so they’re announced when appearing.
- **Testing with Diverse Users:** Ultimately, guidelines are validated by user testing. We plan to test the UI with blind users (using screen readers), low-vision users (maybe those with partial sight), and possibly users with cognitive disabilities for simplicity feedback. Their feedback will inform tweaks to our UI wording, timing, etc. For example, an ADHD user might want the option to turn off voice chit-chat and get more direct instructions (so maybe a “concise mode” vs “friendly mode”). We can accommodate that.

By following these guidelines, we ensure DeepType’s UI is **inclusive and user-friendly** for our entire audience. Adhering to ARIA and accessibility standards isn’t just about compliance; it’s about creating a smoother experience. As one accessibility expert mantra says: *“Build it right for the extreme users, and it will work even better for everyone.”* We believe DeepType’s accessible design will not only empower blind users but also result in a generally well-designed product that anyone could use (for instance, a sighted person might appreciate the voice feedback when they switch to another window but still practice typing by listening). 

All these considerations will be documented in our design system, and every developer on the project will be versed in using them. Accessibility is not a separate module but a thread running through every feature – from color choices to code structure with ARIA roles. This guarantees that **when DeepType is launched, it sets a benchmark for accessible educational software**.

## 7. Full Codebase

Below is an outline of the DeepType codebase structure and the key files with their content. The code is heavily commented to explain functionality, following our style guidelines (including ASCII art headers for each file, and console logs for clarity during debugging and learning). This should serve as both the actual code and a learning aid for developers (including those who are visually impaired or have ADHD, as requested, through clear comments and structured sections).

**Repository Structure:**

```plaintext
DeepType/
├── README.md
├── package.json
├── public/
│   └── index.html        # Main HTML file
├── src/
│   ├── app.js            # Main application logic (front-end)
│   ├── styles.css        # Global styles (high contrast, etc.)
│   ├── lessons.js        # Lesson content and curriculum definitions
│   ├── voice.js          # Voice input/output module
│   ├── adaptive.js       # Adaptive learning logic
│   ├── ui.js             # UI rendering and navigation (menus, focus management)
│   ├── storage.js        # Data storage and Supabase integration
│   └── assets/           # (if any audio/image assets)
├── desktop/
│   └── tauri.conf.json   # Config for Tauri desktop app (if applicable)
└── server/
    └── server.js         # Backend server (for API calls to OpenAI, if used)
```

*(Note: Some of these files/modules could be combined in implementation, but are separated here for clarity of roles.)*

---

Now, we’ll present the content of key files with explanatory comments and ASCII art headers.

### File: `public/index.html`

This is the entry HTML page that loads the app. It sets up basic structure and accessibility attributes (lang, meta).

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DeepType – AI Typing Tutor</title>
  <!-- High contrast dark theme by default -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <!-- Main application container -->
  <div id="app" role="application" aria-label="DeepType application">
    <!-- We will dynamically inject content here via app.js -->
  </div>

  <!-- Including the main script -->
  <script src="app.js" type="module"></script>
</body>
</html>
```

**Notes:**  
- The `role="application"` on the #app container tells screen readers that this is a complex web app, potentially adjusting how keyboard events are handled (for some SR, it might switch off virtual cursor mode when focused inside, expecting the app to manage focus).
- We use `type="module"` for app.js, meaning we can use ES6 imports (assuming a bundler or modern browser environment).
- The app content will be rendered dynamically by our JS (to allow easy updates of content as user navigates).
- We set `lang="en"`; if we support multiple languages, we might adjust that attribute dynamically or have separate pages per locale.

### File: `src/app.js`

This is the main application script that ties together modules, handles initial load and global events.

```javascript
/***********************************************
 *  _____                ______               
 * |  __ \               |  ___|              
 * | |  | |_   _  ___ ___| |_ _ __ ___  ______ 
 * | |  | | | | |/ __/ _ \  _| '__/ _ \|______|
 * | |__| | |_| | (_|  __/ | | | | (_) |      
 * |_____/ \__,_|\___\___\_| |_|  \___/       
 *                                           
 * File: app.js
 * Description: Main entry point for DeepType front-end.
 * Initializes the application, handles navigation flow,
 * and ties together UI, voice, lessons, and adaptivity.
 ***********************************************/

import * as UI from './ui.js';
import * as Voice from './voice.js';
import * as Lessons from './lessons.js';
import * as Adaptive from './adaptive.js';
import * as Storage from './storage.js';

// Global state
let currentLesson = null;
let userProfile = null;
let singleSwitchMode = false;

// Initialize application
document.addEventListener('DOMContentLoaded', () => {
  console.log("DeepType app initializing...");
  Storage.init(); // initialize storage (e.g., Supabase or local)
  userProfile = Storage.loadProfile(); // attempt to load user profile if exists

  // Determine if single switch mode should be default (could be from profile or query param)
  singleSwitchMode = userProfile?.preferences?.singleSwitch || false;

  // Setup voice output system
  Voice.initTTS();
  // Attempt to init voice recognition (will check for support)
  Voice.initSTT(onVoiceCommand);

  // Render the main menu
  UI.renderMainMenu(singleSwitchMode);
  Voice.speak("Welcome to DeepType. Press Enter or say 'start' to begin your first lesson.");

  // Setup global keyboard handlers
  setupKeyboardControls();
});

// Handle voice commands globally
function onVoiceCommand(command) {
  console.log("Voice command heard:", command);
  // Normalize command text
  if (!command) return;
  command = command.toLowerCase();
  if (command.includes('start')) {
    UI.startLessonFlow();
  } else if (command.includes('repeat')) {
    UI.repeatCurrentPrompt();
  } else if (command.includes('menu') || command.includes('exit')) {
    UI.renderMainMenu(singleSwitchMode);
  } else if (command.includes('next')) {
    // This could be used to skip or simulate pressing the "next" button in single-switch scanning
    UI.focusNextOption();
  } else if (command.includes('help')) {
    UI.showHelp();
  } else {
    Voice.speak("Sorry, I didn't catch that. Try saying 'repeat' or 'menu'.");
  }
}

// Setup keyboard controls for single-switch and general navigation
function setupKeyboardControls() {
  document.body.addEventListener('keydown', (e) => {
    if (UI.isInputActive()) {
      // If the user is currently typing in a lesson input field, don't intercept normal typing
      return;
    }
    // Keyboard shortcuts and navigation
    if (e.key === ' ' && singleSwitchMode) {
      // Space in single-switch mode: select or progress
      if (!UI.handleSwitchSelect()) {
        UI.focusNextOption();
      }
      e.preventDefault();
    } else if (e.key === 'ArrowRight' || e.key === 'ArrowDown') {
      UI.focusNextOption();
      e.preventDefault();
    } else if (e.key === 'ArrowLeft' || e.key === 'ArrowUp') {
      UI.focusPreviousOption();
      e.preventDefault();
    } else if (e.key === 'Enter') {
      // Enter triggers default action (like activating focused button)
      UI.activateFocusedOption();
      e.preventDefault();
    } else if (e.key === 'Escape') {
      UI.renderMainMenu(singleSwitchMode);
      e.preventDefault();
    } else if (e.key === 'F1') {
      // F1 for help
      UI.showHelp();
      e.preventDefault();
    }
    // (We could add more shortcuts if needed)
  });
}
```

**Highlights & Explanation:**  
- We import modules for UI, Voice, Lessons, Adaptive logic, and Storage (Supabase or local).
- We keep a global `singleSwitchMode` flag. This can be set from user preferences.
- On DOMContentLoaded, we initialize storage (which might setup Supabase or retrieve from localStorage if offline).
- `Voice.initTTS()` might load voices, etc., and `Voice.initSTT(onVoiceCommand)` will set up speech recognition and call `onVoiceCommand` when a phrase is recognized.
- We immediately render main menu via `UI.renderMainMenu` and prompt user with a welcome voice. (We assume UI module manipulates DOM to show some menu).
- We attach a global keydown listener. If an input is active (meaning user is typing in a practice exercise, which we detect via UI state), we let keys through (so the user can type letters normally).
- We define keyboard navigation: spacebar triggers either select or next in single-switch mode. We check `UI.handleSwitchSelect()` – maybe it returns true if it selected an item, false if it just opened a menu, then we call focusNext for scanning.
- Arrow keys allow navigation too (for users who have full keyboard).
- Enter triggers activation of a focused element (like pressing a button).
- Escape brings back main menu (like a way to cancel or exit current context).
- F1 triggers a help overlay.
- `onVoiceCommand` normalizes recognized text and routes commands. We look for keywords like 'start', 'repeat', 'menu', etc. 
  - For 'start', we call `UI.startLessonFlow()` – that presumably picks the appropriate lesson and transitions to lesson interface.
  - 'repeat' triggers UI to repeat current prompt (like if user says repeat during a lesson, it re-voices the instruction).
  - 'menu' or 'exit' goes back to main menu.
  - 'next' might simulate focus next for scanning or skip something.
  - 'help' shows help.
  - If command is unrecognized, we ask again.
- This design allows adding more voice commands easily by extending that if-else (or changing to a mapping).

We included lots of `console.log` statements for debugging (to track commands etc.). Those logs are helpful in development or if a coder wants to see what’s happening (especially for an ADHD coder who might benefit from stepping through logs to maintain focus).

ASCII art at the top identifies the file and provides a quick summary.

### File: `src/ui.js`

This module manages the DOM updates and navigation. It uses ARIA roles and keeps track of focus for keyboard/switch control.

```javascript
/***********************************************
 *  _    _ _ 
 * | |  | (_)
 * | |  | |_ ___  ___ 
 * | |  | | / __|/ __|
 * | |__| | \__ \ (__ 
 *  \____/|_|___/\___|
 *                    
 * File: ui.js
 * Description: Handles User Interface rendering and navigation.
 * Manages menus, focus (for single-switch), and lesson display.
 ***********************************************/

// Import other needed modules (assuming circular deps resolved by careful use)
import { speak } from './voice.js';
import { currentLesson } from './app.js';  // If needed, or we pass state into functions

// Track focused element index for menus
let focusIndex = 0;
let currentMenuOptions = [];

// Helper: remove existing content
function clearAppContainer() {
  const app = document.getElementById('app');
  app.innerHTML = '';
  focusIndex = 0;
  currentMenuOptions = [];
}

// Render main menu
export function renderMainMenu(singleSwitchMode) {
  clearAppContainer();
  const app = document.getElementById('app');

  // Create title
  const title = document.createElement('h1');
  title.innerText = 'DeepType';
  title.className = 'title';
  app.appendChild(title);

  // Menu container with role=menu
  const menu = document.createElement('div');
  menu.setAttribute('role', 'menu');
  menu.id = 'mainMenu';
  app.appendChild(menu);

  // Define menu options
  const options = [
    { text: 'Start Lesson', action: startLessonFlow },
    { text: 'Settings', action: openSettings },
    { text: 'Help', action: showHelp },
    { text: 'Exit', action: exitApp }
  ];
  currentMenuOptions = options;

  options.forEach((opt, index) => {
    const btn = document.createElement('button');
    btn.innerText = opt.text;
    btn.className = 'menuItem';
    btn.setAttribute('role', 'menuitem');
    btn.setAttribute('tabindex', index === 0 ? '0' : '-1'); // Only first is tab-able initially
    // When focused or clicked, call the action
    btn.addEventListener('click', opt.action);
    btn.addEventListener('focus', () => { focusIndex = index; });
    menu.appendChild(btn);
  });

  // Set initial focus
  const firstButton = menu.querySelector('button');
  if (firstButton) firstButton.focus();

  console.log("Main menu rendered. Options:", options.map(o => o.text));
  speak("Main menu. Options: Start Lesson, Settings, Help, Exit.");
}

// Called when user activates "Start Lesson"
export function startLessonFlow() {
  console.log("Starting lesson flow...");
  // Determine which lesson to start: if userProfile exists and has progress, pick next; else Lesson 1.
  const lessonToStart = /* logic to pick lesson, e.g., Lessons.getNextLesson(userProfile.progress) */ Lessons.getLesson(0);
  if (lessonToStart) {
    currentLesson = lessonToStart;
    renderLesson(currentLesson);
  } else {
    speak("No lesson available.");
  }
}

// Render a lesson interface
export function renderLesson(lesson) {
  clearAppContainer();
  const app = document.getElementById('app');
  
  // Create heading for lesson (for low-vision users)
  const lh = document.createElement('h2');
  lh.innerText = lesson.title;
  lh.className = 'lessonTitle';
  app.appendChild(lh);

  // Create a prompt area
  const prompt = document.createElement('div');
  prompt.id = 'prompt';
  prompt.setAttribute('aria-live', 'polite');  // so updates are read
  prompt.innerText = '';  // will be filled
  app.appendChild(prompt);

  // Create an input area if needed (hidden text input to capture typing)
  const hiddenInput = document.createElement('input');
  hiddenInput.type = 'text';
  hiddenInput.id = 'typingInput';
  hiddenInput.setAttribute('aria-label', 'Typing input'); // label for SR
  hiddenInput.style.position = 'absolute';
  hiddenInput.style.opacity = '0';
  app.appendChild(hiddenInput);
  hiddenInput.focus();  // focus so keystrokes go here

  // Listen for keystrokes in the lesson input
  hiddenInput.addEventListener('input', onUserType);

  // Kick off the first prompt of the lesson
  speak(lesson.intro);  // e.g., "Lesson 1. Place your fingers on home row..."
  setTimeout(() => {
    presentNextPrompt(); 
  }, 1000);
}

// Keep track of current step within lesson
let currentStepIndex = 0;
function presentNextPrompt() {
  const lesson = currentLesson;
  if (!lesson) return;
  const promptEl = document.getElementById('prompt');
  const step = lesson.steps[currentStepIndex];
  if (!step) {
    lessonComplete();
    return;
  }
  // Show the prompt text (if any) in large font for those who can see
  promptEl.innerText = step.promptText || '';
  // Speak the prompt (if it's a letter, maybe speak differently vs word)
  const toSpeak = step.audioPrompt || step.promptText;
  speak(toSpeak);
  // Expect the user's input (the onUserType handler will check it)
}

// Handle user typing in lesson
function onUserType(e) {
  const input = e.target;
  const lesson = currentLesson;
  const step = lesson.steps[currentStepIndex];
  const expected = step.expectedInput;  // e.g., 'F' or 'cat'
  const userInput = input.value;
  // For single-character inputs, we check immediately.
  if (step.type === 'char') {
    const char = userInput.slice(-1);  // last typed char
    if (!char) return;
    if (char.toLowerCase() === expected.toLowerCase()) {
      console.log(`Correct key pressed: ${char}`);
      speak("Correct");  // positive feedback
      currentStepIndex++;
      input.value = '';
      setTimeout(presentNextPrompt, 500);
    } else {
      console.log(`Incorrect key. Expected ${expected}, got ${char}`);
      speak(`Oops, that was ${char}. Try again.`);
      input.value = '';
      // (We don't advance stepIndex, user will try again same prompt)
    }
  } else if (step.type === 'word') {
    // If expecting a word, we might wait for space or enter
    if (userInput.endsWith(' ')) {
      const attempt = userInput.trim();
      if (attempt.toLowerCase() === expected.toLowerCase()) {
        console.log(`Correct word typed: ${attempt}`);
        speak("Good job, that was correct.");
        currentStepIndex++;
      } else {
        console.log(`Incorrect word. Expected ${expected}, got ${attempt}`);
        speak(`You typed ${attempt}, but the word was ${expected}. We'll practice it again.`);
        // maybe repeat same step or push it later for practice
        Adaptive.recordMistake(expected);
      }
      input.value = '';
      setTimeout(presentNextPrompt, 500);
    }
  }
  // else: other types (sentences etc.) similar approach
}

// Repeat current prompt (for voice command or button)
export function repeatCurrentPrompt() {
  const lesson = currentLesson;
  if (!lesson) return;
  const step = lesson.steps[currentStepIndex];
  if (step) {
    const toSpeak = step.audioPrompt || step.promptText;
    speak(`I said: ${toSpeak}`);
  }
}

// Mark lesson as complete
function lessonComplete() {
  speak("Lesson complete! Great work.");
  // Ideally record progress
  Storage.saveProgress(currentLesson.id);
  // Then return to menu or next lesson
  currentLesson = null;
  setTimeout(() => {
    renderMainMenu(false);
  }, 2000);
}

// Navigation helpers
export function focusNextOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items.length === 0) return;
  focusIndex = (focusIndex + 1) % items.length;
  items[focusIndex].focus();
  speak(items[focusIndex].innerText);  // announce the newly focused option
}

export function focusPreviousOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items.length === 0) return;
  focusIndex = (focusIndex - 1 + items.length) % items.length;
  items[focusIndex].focus();
  speak(items[focusIndex].innerText);
}

export function activateFocusedOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items[focusIndex]) {
    items[focusIndex].click();
  }
}

// For single switch: 
// handleSwitchSelect returns true if an action was taken (like a menu item activated)
export function handleSwitchSelect() {
  const menu = document.getElementById('mainMenu');
  if (menu) {
    // if menu open, Space acts as Enter on the focused option
    activateFocusedOption();
    return true;
  }
  // If no menu (maybe in lesson), we could treat Space as some other action, but by default:
  return false;
}

// Dummy implementations of other menu actions:
export function openSettings() {
  speak("Settings is not implemented yet.");
  // would render settings UI with options (like toggle singleSwitchMode)
}
export function showHelp() {
  speak("Help: You can say commands like start, repeat, menu. During lessons, type the requested keys. Press Escape to return to menu.");
}
export function exitApp() {
  speak("Exiting. Goodbye!");
  console.log("Exiting DeepType app.");
  // In a web app, we might just reload or clear, in a desktop app, possibly close window.
  // For now:
  clearAppContainer();
  const msg = document.createElement('p');
  msg.innerText = "You have exited the app. Refresh to restart.";
  document.getElementById('app').appendChild(msg);
}
```

**Highlights & Explanation:**  
- `renderMainMenu` creates a menu with `role="menu"` and each button as `role="menuitem"`. We manage `tabindex` such that initially only the first item is focusable (to avoid tabbing into others without our control).
- We store `currentMenuOptions` to map index to action.
- We focus the first button and speak the menu options. We call `speak()` with a summary of options.
- For each button, we attach focus and click events. On focus, we update `focusIndex` (so we know which index is focused for arrow key logic).
- The `startLessonFlow` function gets a lesson (from Lessons module) and calls `renderLesson`.
- `renderLesson` builds the lesson UI: it shows a title, a prompt area (with `aria-live` so screen reader will read any changes politely).
- It also adds a hidden text input to capture typing. This input is positioned off-screen (opacity 0, could also use `left: -999px` etc.) to not distract sighted users, but it's needed to capture key events reliably. We label it for SR as "Typing input" (though the SR user will primarily rely on our voice, but if they use their own SR, it might announce when they type).
- We immediately focus this hidden input so that all keypresses go there without user needing to click anything (they just start typing).
- We attach `onUserType` to its `input` event to handle typing.
- We then start the lesson by speaking an intro and calling `presentNextPrompt()`.
- `presentNextPrompt` sets the prompt text (for low-vision users to see) and speaks it. It uses `lesson.steps` array with each step having `promptText`, `audioPrompt`, `expectedInput`, etc. If no more steps, calls `lessonComplete()`.
- `onUserType` logic: if expecting a character:
  - It grabs the last typed char from input (since we keep input field value).
  - If correct, give feedback and move to next step.
  - If wrong, clear input, speak feedback, and stay on same step (so user can try again). Also maybe record mistake for adaptivity.
- If expecting a word:
  - We wait until user types space (meaning they finished the word). Then check correctness, give appropriate response, and move on or repeat.
  - Note: This simplistic approach uses space as terminator. Alternatively, we might have them press Enter to submit a word. But space is used to separate words if typing a sentence, so this logic might need refinement to handle multi-word inputs. For now, single-word training is fine.
- The code calls `Adaptive.recordMistake(expected)` when a word is mistyped – presumably to log that for later practice. (We haven’t written adaptive.js yet, but it would provide such function.)
- `repeatCurrentPrompt` will re-speak the current prompt with a prefix "I said: ...".
- `lessonComplete` speaks completion, saves progress via Storage, and returns to main menu after a short delay.
- Navigation helpers:
  - `focusNextOption` and `focusPreviousOption` move the focusIndex and focus the corresponding button in the menu. They also call `speak()` to announce the option text (so a blind user knows which option is now focused).
  - `activateFocusedOption` simulates pressing enter on the current focus by triggering the button’s click event.
  - `handleSwitchSelect` ties into our app-level key handler: if menu open, space triggers activation (and returns true meaning it consumed the press). If no menu, maybe in a lesson context, we could have other logic (not implemented here).
- Basic `openSettings`, `showHelp`, `exitApp` are stubs for now. `showHelp` voices some instructions. Ideally, it would present a nice overlay with all commands (with an accessible format).
- We included console logs to track what's being rendered, key correctness, etc., which is useful for debugging and also if a developer runs this in dev tools, they can see these logs for understanding flow.

We ensure important interactive elements have ARIA roles and labels:
- The prompt has `aria-live="polite"` to announce changes.
- The hidden input has a label (though if using their own screen reader, they might not need it since they won't focus it consciously).
- Buttons have roles and text (text is visible so acts as label as well).
- If we had any icon-only or dynamic text, we'd label them.

### File: `src/voice.js`

Manages text-to-speech (TTS) and speech-to-text (STT). We abstract these so we can swap between Web Speech API or other backends. For simplicity, we’ll try Web Speech API and fallback to none if unsupported.

```javascript
/***********************************************
 * __     ___    ____ _____  _    ____ ___ 
 * \ \   / / |  |  _ \_   _|/ \  / ___|_ _|
 *  \ \ / /| |  | |_) || | / _ \| |    | | 
 *   \ V / | |__|  __/ | |/ ___ \ |___ | | 
 *    \_/  |_____|_|    |_/_/   \_\____|___|
 *                                         
 * File: voice.js
 * Description: Voice input (STT) and output (TTS) management.
 * Provides speak() for TTS and sets up speech recognition for voice commands.
 ***********************************************/

// Text-to-Speech (TTS)
let synth;
let voice;
export function initTTS() {
  synth = window.speechSynthesis;
  if (!synth) {
    console.warn("TTS not supported in this browser.");
    return;
  }
  // Optional: choose a specific voice (e.g., a female English voice)
  const voices = synth.getVoices();
  // find an English voice
  voice = voices.find(v => v.lang.startsWith('en') && v.name.includes('Google')) || voices[0];
  console.log("TTS initialized. Using voice:", voice ? voice.name : 'default');
}

export function speak(text) {
  if (!window.speechSynthesis) {
    return; // no TTS available
  }
  if (synth.speaking) {
    synth.cancel(); // stop current speech if any (to avoid overlap)
  }
  const utter = new SpeechSynthesisUtterance(text);
  utter.rate = 0.9; // slightly slower for clarity
  utter.pitch = 1; 
  utter.volume = 1;
  if (voice) utter.voice = voice;
  synth.speak(utter);
}

// Speech-to-Text (STT) for voice commands
let recognition;
export function initSTT(commandCallback) {
  if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
    console.warn("Speech Recognition not supported in this browser.");
    return;
  }
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  recognition = new SpeechRecognition();
  recognition.lang = 'en-US';
  recognition.continuous = true; // continuous listening
  recognition.interimResults = false;
  recognition.maxAlternatives = 1;
  recognition.onresult = (event) => {
    const transcript = event.results[event.results.length - 1][0].transcript.trim();
    console.log("STT result:", transcript);
    commandCallback(transcript);
  };
  recognition.onerror = (event) => {
    console.error("Speech Recognition error:", event.error);
    // If continuous listening stops due to error, attempt restart
    if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
      console.warn("Microphone access denied or STT not allowed.");
      speak("Voice commands are not available.");
    } else {
      // try to restart recognition on network or other errors
      try { recognition.start(); } catch(e) {}
    }
  };
  recognition.onend = () => {
    console.log("STT onend fired, restarting...");
    // automatically restart listening unless intentionally stopped
    try { recognition.start(); } catch(e) {}
  };

  try {
    recognition.start();
    console.log("Speech recognition started.");
    // Provide an audible cue that voice commands are active (if desired)
  } catch (e) {
    console.error("Speech recognition couldn't start:", e);
  }
}
```

**Highlights & Explanation:**  
- `initTTS()` sets up speechSynthesis, picks a voice. `getVoices()` might be empty initially until onvoiceschanged event fires; in real code we might wait or call it after a slight timeout. But for brevity, we take what's available.
- `speak(text)` uses the Web Speech API TTS to speak. It cancels any ongoing speech to avoid overlap (so if multiple calls happen quickly, we speak the latest).
- Rate is set to 0.9 (slightly slower than default 1.0) for clarity. This might be user-adjustable later.
- `initSTT(commandCallback)` uses webkitSpeechRecognition if available. It's experimental but works in Chrome.
- We set continuous listening with no interim results (we only care about final commands).
- On result, we extract the transcript and pass it to the callback provided (which in app.js is `onVoiceCommand`).
- On error: 
  - If not allowed, we notify user (lack of mic permission).
  - If other errors (like network), we attempt to restart `recognition.start()`.
- On end: we auto-restart to keep listening (unless it was purposely stopped). This ensures continuous voice command listening.
- We wrap `recognition.start()` in try-catch as it can throw if called at bad times or user hasn't interacted (Chrome might require user gesture to start audio capture).
- We log starting, and maybe would beep or speak to indicate voice command readiness. (For now, just log).
- We keep `recognition` global so we could implement a stop function if needed (not shown here, but if user pauses voice commands).
- This design ensures the app is always listening for voice commands in the background while the user is practicing (which is what we want for "repeat" etc., but caution: the recognizer might pick up the TTS voice. That’s a known challenge. We might need to tune it by e.g. only listening during silent periods or using a push-to-talk style. This code might erroneously transcribe the tutor’s spoken instructions as commands, which is an issue. One approach: temporarily stop recognition when we speak instructions, or use a VAD (voice activity detection) to pause recognition while TTS is active. For brevity, not handled here, but a real implementation should address it. Alternatively, keep commands distinct enough that instructions wouldn’t trigger them.)

- ASCII art spells "VOICE".

### File: `src/lessons.js`

Contains lesson definitions and possibly logic to fetch next lesson.

```javascript
/***********************************************
 *  _                 _   
 * | |   ___  ___ ___| |_ 
 * | |__/ _ \/ __/ __| __|
 * |____\___/\__\__ \ |_ 
 *               |___/\__|
 * 
 * File: lessons.js
 * Description: Defines lesson content and provides accessors.
 * Each lesson contains a title, intro text, and a sequence of steps.
 ***********************************************/

export const lessons = [
  {
    id: 0,
    title: "Lesson 1: Home Row (F and J)",
    intro: "Lesson 1. Let's start with the home row keys F and J. " +
           "Place your left index on F, right index on J. Now, type the letter F when you hear it.",
    steps: [
      { type: 'char', promptText: "Type F", audioPrompt: "Press the F key.", expectedInput: "F" },
      { type: 'char', promptText: "Type J", audioPrompt: "Now press the J key.", expectedInput: "J" },
      { type: 'char', promptText: "Type F", audioPrompt: "Again, press F.", expectedInput: "F" },
      { type: 'char', promptText: "Type J", audioPrompt: "Again, press J.", expectedInput: "J" },
      // ... more practice
      { type: 'word', promptText: "Type 'fj'", audioPrompt: "Now type F J as a two-letter word.", expectedInput: "fj" }
    ]
  },
  {
    id: 1,
    title: "Lesson 2: Home Row (D and K)",
    intro: "Lesson 2. Next, we'll add D and K.",
    steps: [
      // Similar structure...
    ]
  }
  // Additional lessons...
];

// Function to get a lesson by id or index
export function getLesson(id) {
  return lessons.find(lsn => lsn.id === id) || null;
}

// Placeholder: pick next lesson for a user given their progress
export function getNextLesson(progress) {
  // If progress is an array of completed lesson IDs:
  for (let lsn of lessons) {
    if (!progress || !progress.includes(lsn.id)) {
      return lsn;
    }
  }
  return null;
}
```

**Highlights & Explanation:**  
- We define a simple array of lesson objects. Each has:
  - `id`, `title`, `intro` (what to speak at start), and `steps`.
  - Each step has `type` (could be 'char', 'word', 'sentence', etc.), `promptText` for display, `audioPrompt` for what voice says (if we want a different wording from what’s displayed), and `expectedInput`.
- In lesson1, we practice F and J individually and then as a pair.
- `getLesson(id)` returns the lesson object (to start a specific lesson).
- `getNextLesson(progress)` would normally use stored progress to find the next uncompleted lesson. Here, we just loop through lessons and return the first not in completed progress array.
- For progress tracking, `Storage.saveProgress` would likely push completed ID into some array and save for user.
- More lessons can be added following this structure.
- We might later use adaptivity to alter steps or insert new steps, but base curriculum is here.

### File: `src/adaptive.js`

Handles adaptive logic (for now maybe just collects mistakes, could later adjust lesson order).

```javascript
/***********************************************
 *      _            _   _       
 *     / \   ___ ___| |_(_)_ __  
 *    / _ \ / __/ __| __| | '_ \ 
 *   / ___ \ (__\__ \ |_| | |_) |
 *  /_/   \_\___|___/\__|_| .__/ 
 *                        |_|    
 * File: adaptive.js
 * Description: Adaptive learning utilities.
 * Records mistakes and can modify or suggest extra practice.
 ***********************************************/

// Simple structure to record mistakes frequency
const mistakeCount = {};

// Record a mistake for a particular key or word
export function recordMistake(item) {
  mistakeCount[item] = (mistakeCount[item] || 0) + 1;
  console.log(`Adaptive: recorded mistake for "${item}". Total mistakes: ${mistakeCount[item]}`);
  // (We could decide to dynamically insert a review step for this item)
}

// Suggest an extra practice step if certain threshold reached
export function maybeInjectPractice(currentLesson) {
  // For example, if any key has >3 mistakes, inject a practice step after current one:
  for (let item in mistakeCount) {
    if (mistakeCount[item] >= 3) {
      // Find if not already in current lesson steps soon:
      const practiceStep = {
        type: item.length === 1 ? 'char' : 'word',
        promptText: `Type ${item}`,
        audioPrompt: `Let's practice ${item}`,
        expectedInput: item
      };
      console.log(`Adaptive: injecting extra practice for "${item}"`);
      // Insert practice step after current index (assuming we have access to the index here or handle differently).
      // This is a simplistic approach; a more robust system might queue practice for next lesson.
      return practiceStep;
    }
  }
  return null;
}
```

**Highlights & Explanation:**  
- We use a simple dictionary `mistakeCount` to count errors per item (could be letter or word).
- `recordMistake(item)` increments count and logs it.
- `maybeInjectPractice(currentLesson)` is a hook that could be called at certain times (e.g., after finishing a lesson or between steps) to decide if we should insert an extra step.
  - Here, if any item has 3 or more mistakes, we prepare a practice step focusing on it.
  - This injection logic is simplistic: ideally, we might insert into the lesson flow. But doing that on the fly could complicate `presentNextPrompt`. Perhaps easier: schedule it for next lesson, or just alert the user.
  - But the code indicates returning a practiceStep, which the calling code (if integrated in onUserType maybe) could handle by inserting into lesson.steps or something.
- Right now, nothing calls `maybeInjectPractice` in our UI. We might integrate it at lessonComplete or somewhere if needed.
- The adaptivity now is very minimal. In a more complex version, we might adjust difficulty or skip ahead if user is doing well.

### File: `src/storage.js`

Manages saving/loading from localStorage or Supabase (depending on environment). For simplicity, a local stub is shown:

```javascript
/***********************************************
 *   ____ _______    _    ____ _____ ___ ___  
 *  / ___|_   _\ \  / \  / ___|_   _|_ _/ _ \ 
 * | |     | |  \ \/ _ \| |     | |  | | | | |
 * | |___  | |   | | (_) | |___  | |  | | |_| |
 *  \____| |_|   |_|\___/ \____| |_| |___\___/ 
 *                                            
 * File: storage.js
 * Description: Handles saving/loading user data.
 * If online, uses Supabase; otherwise falls back to localStorage.
 ***********************************************/

// Supabase client if needed (assuming we have included supabase script or using @supabase/supabase-js)
let supabase = null;
const SUPABASE_KEY = '<your-supabase-key>';
const SUPABASE_URL = '<your-supabase-url>';

export function init() {
  if (window.supabase) {
    supabase = window.supabase.createClient(SUPABASE_URL, SUPABASE_KEY);
    console.log("Supabase initialized.");
  } else {
    console.log("Supabase not loaded, using local storage only.");
  }
}

// Load user profile from local storage (or supabase if logged in)
export function loadProfile() {
  try {
    const data = localStorage.getItem('deeptype_profile');
    if (data) {
      const profile = JSON.parse(data);
      console.log("Loaded profile from local storage:", profile);
      return profile;
    }
  } catch (e) {
    console.error("Failed to load profile:", e);
  }
  return null;
}

// Save progress (simple: record lesson completed)
export function saveProgress(lessonId) {
  let profile = loadProfile() || { progress: [] };
  if (!profile.progress.includes(lessonId)) {
    profile.progress.push(lessonId);
  }
  try {
    localStorage.setItem('deeptype_profile', JSON.stringify(profile));
    console.log(`Progress saved. Completed lessons: ${profile.progress}`);
  } catch (e) {
    console.error("Failed to save progress:", e);
  }
  // If using Supabase and logged in, we'd also send update to DB here
}

// Other storage functions like saveSettings, etc., could be added similarly.
```

**Highlights & Explanation:**  
- On `init`, if the Supabase library is available, create a client (with placeholders for URL/KEY which should be configured).
- `loadProfile` tries to get from localStorage and parse. (In a real scenario, if user is logged in with Supabase Auth, we’d fetch from DB).
- `saveProgress` updates a local profile’s progress array and writes it back to localStorage. Also logs it.
- If supabase was connected and user logged, we would call something like `supabase.from('profiles').update({progress: profile.progress}).eq('id', user.id)` etc.
- But offline usage is fine with localStorage.
- We can add other settings similarly (like singleSwitch preference).
- This is simplified for demonstration.

---

With these modules, the codebase provides a full loop:
- App starts -> main menu -> lesson -> interactive typing -> adaptive feedback -> back to menu.
- The code is annotated so that developers can understand what each part does.
- ASCII art headers help a developer locate files quickly (especially in a printed or linear reading scenario, the big text stands out).
- Console logs and comments provide a narrative of the program flow, which can help someone learning or debugging the system.

**Console Logs & Debugging Enhancements:**  
We have used `console.log` generously to mark major events (initialization, rendering, correctness of input, etc.). In a debug mode, these logs could be turned on to follow what's happening step by step:
- E.g., "Correct key pressed: F" tells us input handling logic worked for that case.
- "Main menu rendered. Options: ..." confirms UI built properly.
- "STT result: repeat" shows what the speech recognizer picked up.

For an educational coding perspective, one could run this in a browser console and watch these logs to see the internal state changes. This is beneficial for developers new to this code or even for advanced users curious about the internals.

We can easily extend or adapt this structure:
- If building with a framework (React), the structure might differ (e.g., using components and state instead of directly manipulating DOM), but the logical separation (UI, voice, data) remains similar.
- We kept it framework-agnostic for clarity.

Finally, note that we ignore image embedding or charts as requested. All output here is purely textual or via spoken text (for the app itself). If we had images, we included alt text approach.

**Conclusion:** The above code and documentation present a complete picture of DeepType’s implementation approach. From the executive vision down to code level, we’ve aligned everything with the goal of an accessible, intelligent typing tutor. The structure can be built upon as we add more features (for instance, integrating the conversational AI in the future might add another module, or hooking up Supabase in earnest once keys are set, etc.). 

Each code file starts with an ASCII art banner to make it fun and to break the monotony (which helps ADHD coders by chunking sections clearly). Screen reader users can skip over the ASCII art (since it’s in a comment, it won’t be read aloud) and get straight to the content; the art is more for visual appeal in code editors.

We have thus provided a **comprehensive dossier** that covers vision, design, strategy, and technical implementation for DeepType by Empathy Labs. By following this plan, the DeepType project is well-positioned to succeed and make a meaningful impact in assistive education, demonstrating how thoughtful use of AI and UX design can create inclusive technology for all.
</file>

<file path="src/vite-env.d.ts">
/// <reference types="vite/client" />
</file>

<file path="supabase/functions/gemini-tutor/index.ts">
import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
}
serve(async (req) => {
  // Handle CORS preflight requests
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders })
  }
  try {
    const GEMINI_API_KEY = Deno.env.get('GEMINI_API_KEY')
    if (!GEMINI_API_KEY) {
      throw new Error('GEMINI_API_KEY is not set in environment variables')
    }
    const { text, target, wpm, accuracy } = await req.json()
    console.log('Received request:', { text, target, wpm, accuracy })
    // Call Google Gemini API with proper error logging
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${GEMINI_API_KEY}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: `As a typing tutor, analyze this typing performance and give constructive feedback:
              Target text: "${target}"
              User typed: "${text}"
              Speed: ${wpm} WPM
              Accuracy: ${accuracy}%
              Give specific, encouraging feedback about their performance and tips for improvement.
              Keep the response under 3 sentences.`
          }]
        }]
      })
    })
    if (!response.ok) {
      const errorData = await response.text()
      console.error('Gemini API error:', {
        status: response.status,
        statusText: response.statusText,
        error: errorData
      })
      throw new Error(`Gemini API error: ${response.statusText} - ${errorData}`)
    }
    const data = await response.json()
    console.log('Gemini API response:', data)
    const feedback = data.candidates[0].content.parts[0].text
    return new Response(
      JSON.stringify({ feedback }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    )
  } catch (error) {
    console.error('Error in gemini-tutor function:', error)
    return new Response(
      JSON.stringify({ error: error.message }),
      { 
        status: 500,
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      }
    )
  }
})
</file>

<file path="supabase/migrations/20240218_user_profiles.sql">
-- Create profiles table
CREATE TABLE IF NOT EXISTS profiles (
  id UUID REFERENCES auth.users ON DELETE CASCADE,
  username TEXT UNIQUE,
  full_name TEXT,
  avatar_url TEXT,
  level INTEGER DEFAULT 1,
  total_lessons_completed INTEGER DEFAULT 0,
  total_practice_time INTEGER DEFAULT 0,
  average_wpm INTEGER DEFAULT 0,
  average_accuracy INTEGER DEFAULT 0,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,
  PRIMARY KEY (id)
);
-- Create typing_progress table
CREATE TABLE IF NOT EXISTS typing_progress (
  id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  user_id UUID REFERENCES auth.users ON DELETE CASCADE,
  wpm INTEGER NOT NULL,
  accuracy INTEGER NOT NULL,
  mistakes JSONB,
  level INTEGER NOT NULL,
  lesson_text TEXT,
  completed_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);
-- Create achievements table
CREATE TABLE IF NOT EXISTS achievements (
  id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  name TEXT NOT NULL,
  description TEXT NOT NULL,
  icon TEXT,
  criteria JSONB NOT NULL,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);
-- Create user_achievements table
CREATE TABLE IF NOT EXISTS user_achievements (
  id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  user_id UUID REFERENCES auth.users ON DELETE CASCADE,
  achievement_id UUID REFERENCES achievements ON DELETE CASCADE,
  unlocked_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL,
  UNIQUE(user_id, achievement_id)
);
-- Create typing_analysis table
CREATE TABLE IF NOT EXISTS typing_analysis (
  id UUID DEFAULT uuid_generate_v4() PRIMARY KEY,
  user_id UUID REFERENCES auth.users ON DELETE CASCADE,
  error_patterns JSONB,
  speed_trends JSONB,
  recommended_focus TEXT[],
  created_at TIMESTAMP WITH TIME ZONE DEFAULT timezone('utc'::text, now()) NOT NULL
);
-- Create RLS policies
ALTER TABLE profiles ENABLE ROW LEVEL SECURITY;
ALTER TABLE typing_progress ENABLE ROW LEVEL SECURITY;
ALTER TABLE achievements ENABLE ROW LEVEL SECURITY;
ALTER TABLE user_achievements ENABLE ROW LEVEL SECURITY;
ALTER TABLE typing_analysis ENABLE ROW LEVEL SECURITY;
-- Profiles policies
CREATE POLICY "Public profiles are viewable by everyone"
  ON profiles FOR SELECT
  USING (true);
CREATE POLICY "Users can insert their own profile"
  ON profiles FOR INSERT
  WITH CHECK (auth.uid() = id);
CREATE POLICY "Users can update their own profile"
  ON profiles FOR UPDATE
  USING (auth.uid() = id);
-- Typing progress policies
CREATE POLICY "Users can view their own typing progress"
  ON typing_progress FOR SELECT
  USING (auth.uid() = user_id);
CREATE POLICY "Users can insert their own typing progress"
  ON typing_progress FOR INSERT
  WITH CHECK (auth.uid() = user_id);
-- Achievements policies
CREATE POLICY "Achievements are viewable by everyone"
  ON achievements FOR SELECT
  USING (true);
-- User achievements policies
CREATE POLICY "Users can view their own achievements"
  ON user_achievements FOR SELECT
  USING (auth.uid() = user_id);
CREATE POLICY "Users can unlock achievements"
  ON user_achievements FOR INSERT
  WITH CHECK (auth.uid() = user_id);
-- Typing analysis policies
CREATE POLICY "Users can view their own typing analysis"
  ON typing_analysis FOR SELECT
  USING (auth.uid() = user_id);
CREATE POLICY "Users can insert their own typing analysis"
  ON typing_analysis FOR INSERT
  WITH CHECK (auth.uid() = user_id);
-- Create functions
CREATE OR REPLACE FUNCTION handle_new_user()
RETURNS TRIGGER AS $$
BEGIN
  INSERT INTO public.profiles (id, username, avatar_url)
  VALUES (
    NEW.id,
    NEW.raw_user_meta_data->>'username',
    NEW.raw_user_meta_data->>'avatar_url'
  );
  RETURN NEW;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
-- Create trigger for new user
CREATE TRIGGER on_auth_user_created
  AFTER INSERT ON auth.users
  FOR EACH ROW EXECUTE PROCEDURE handle_new_user();
</file>

<file path="supabase/migrations/20240219_leaderboard_functions.sql">
-- Create a function to get leaderboard data
CREATE OR REPLACE FUNCTION get_leaderboard(
  p_limit integer DEFAULT 100,
  p_time_period text DEFAULT 'all',
  p_sort_by text DEFAULT 'wpm'
)
RETURNS TABLE (
  id uuid,
  username text,
  full_name text,
  avatar_url text,
  average_wpm integer,
  average_accuracy integer,
  total_lessons_completed integer,
  level integer,
  achievements_count bigint,
  rank bigint
)
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_start_date timestamp;
BEGIN
  -- Set time period filter
  CASE p_time_period
    WHEN 'today' THEN
      v_start_date := current_date;
    WHEN 'week' THEN
      v_start_date := date_trunc('week', current_date);
    WHEN 'month' THEN
      v_start_date := date_trunc('month', current_date);
    ELSE
      v_start_date := '1970-01-01'::timestamp;
  END CASE;
  -- Return leaderboard data
  RETURN QUERY
  WITH achievement_counts AS (
    SELECT
      user_id,
      COUNT(*) as achievements_count
    FROM user_achievements
    GROUP BY user_id
  ),
  ranked_users AS (
    SELECT
      p.id,
      p.username,
      p.full_name,
      p.avatar_url,
      p.average_wpm,
      p.average_accuracy,
      p.total_lessons_completed,
      p.level,
      COALESCE(ac.achievements_count, 0) as achievements_count,
      ROW_NUMBER() OVER (
        ORDER BY
          CASE p_sort_by
            WHEN 'wpm' THEN p.average_wpm
            WHEN 'accuracy' THEN p.average_accuracy
            WHEN 'lessons' THEN p.total_lessons_completed
            WHEN 'achievements' THEN COALESCE(ac.achievements_count, 0)
          END DESC
      ) as rank
    FROM profiles p
    LEFT JOIN achievement_counts ac ON p.id = ac.user_id
    WHERE
      CASE p_time_period
        WHEN 'all' THEN true
        ELSE p.last_lesson_date >= v_start_date
      END
  )
  SELECT *
  FROM ranked_users
  LIMIT p_limit;
END;
$$;
-- Grant execute permission to authenticated users
GRANT EXECUTE ON FUNCTION get_leaderboard TO authenticated;
</file>

<file path="supabase/migrations/20240229000000_create_typing_history.sql">
-- First, update the profiles table to include typing history fields
alter table public.profiles 
add column if not exists words_per_minute integer,
add column if not exists accuracy_percentage integer,
add column if not exists last_lesson_date timestamp with time zone;
-- Create the typing history table
create table if not exists public.typing_history (
    id uuid default gen_random_uuid() primary key,
    profile_id uuid references public.profiles(id) on delete cascade not null,
    created_at timestamp with time zone default timezone('utc'::text, now()) not null,
    words_per_minute integer not null,
    accuracy_percentage integer not null,
    lesson_level text not null,
    constraint typing_history_accuracy_percentage_check check ((accuracy_percentage >= 0) AND (accuracy_percentage <= 100))
);
-- Add RLS policies
alter table public.typing_history enable row level security;
create policy "Users can insert their own typing history."
    on public.typing_history for insert
    with check ( auth.uid() = (select id from public.profiles where id = profile_id) );
create policy "Users can view their own typing history."
    on public.typing_history for select
    using ( auth.uid() = (select id from public.profiles where id = profile_id) );
-- Create index for better query performance
create index typing_history_profile_id_idx on public.typing_history(profile_id);
</file>

<file path="supabase/apply-migrations.sh">
#!/bin/bash
# Load environment variables
source .env.local
# Apply schema migrations
PGPASSWORD=$POSTGRES_PASSWORD psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DATABASE -f supabase/migrations/20240218_user_profiles.sql
# Apply function migrations
PGPASSWORD=$POSTGRES_PASSWORD psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DATABASE -f supabase/migrations/20240219_leaderboard_functions.sql
# Verify tables and functions were created
PGPASSWORD=$POSTGRES_PASSWORD psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DATABASE -c "\dt"
PGPASSWORD=$POSTGRES_PASSWORD psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DATABASE -c "\df"
</file>

<file path="supabase/config.toml">
project_id = "olvtlevlbgtalcnhcnvh"

[functions.gemini-tutor]
verify_jwt = false
</file>

<file path=".cursorrules">
<cursor-tools Integration>
# Instructions
Use the following commands to get AI assistance:

**Web Search:**
`cursor-tools web "<your question>"` - Get answers from the web using Perplexity AI (e.g., `cursor-tools web "latest weather in London"`)
when using web for complex queries suggest writing the output to a file somewhere like local-research/<query summary>.md.

**Repository Context:**
`cursor-tools repo "<your question>"` - Get context-aware answers about this repository using Google Gemini (e.g., `cursor-tools repo "explain authentication flow"`)

**Documentation Generation:**
`cursor-tools doc [options]` - Generate comprehensive documentation for this repository (e.g., `cursor-tools doc --output docs.md`)
when using doc for remote repos suggest writing the output to a file somewhere like local-docs/<repo-name>.md.

**GitHub Information:**
`cursor-tools github pr [number]` - Get the last 10 PRs, or a specific PR by number (e.g., `cursor-tools github pr 123`)
`cursor-tools github issue [number]` - Get the last 10 issues, or a specific issue by number (e.g., `cursor-tools github issue 456`)

**Browser Automation (Stateless):**
`cursor-tools browser open <url> [options]` - Open a URL and capture page content, console logs, and network activity (e.g., `cursor-tools browser open "https://example.com" --html`)
`cursor-tools browser act "<instruction>" --url=<url> [options]` - Execute actions on a webpage using natural language instructions (e.g., `cursor-tools browser act "Click Login" --url=https://example.com`)
`cursor-tools browser observe "<instruction>" --url=<url> [options]` - Observe interactive elements on a webpage and suggest possible actions (e.g., `cursor-tools browser observe "interactive elements" --url=https://example.com`)
`cursor-tools browser extract "<instruction>" --url=<url> [options]` - Extract data from a webpage based on natural language instructions (e.g., `cursor-tools browser extract "product names" --url=https://example.com/products`)

**Notes on Browser Commands:**
- All browser commands are stateless: each command starts with a fresh browser instance and closes it when done.
- When using `--connect-to`, special URL values are supported:
  - `current`: Use the existing page without reloading
  - `reload-current`: Use the existing page and refresh it (useful in development)
- Multi step workflows involving state or combining multiple actions are supported in the `act` command using the pipe (|) separator (e.g., `cursor-tools browser act "Click Login | Type 'user@example.com' into email | Click Submit" --url=https://example.com`)
- Video recording is available for all browser commands using the `--video=<directory>` option. This will save a video of the entire browser interaction at 1280x720 resolution. The video file will be saved in the specified directory with a timestamp.
- DO NOT ask browser act to "wait" for anything, the wait command is currently disabled in Stagehand.

**Tool Recommendations:**
- `cursor-tools web` is best for general web information not specific to the repository.
- `cursor-tools repo` is ideal for repository-specific questions, planning, code review and debugging.
- `cursor-tools doc` generates documentation for local or remote repositories.
- `cursor-tools browser` is useful for testing and debugging web apps.

**Running Commands:**
1. **Installed version:** Use `cursor-tools <command>` (if in PATH) or `npm exec cursor-tools "<command>"`, `yarn cursor-tools "<command>"`, `pnpm cursor-tools "<command>"`.
2. **Without installation:** Use `npx -y cursor-tools@latest "<command>"` or `bunx -y cursor-tools@latest "<command>"`.

**General Command Options (Supported by all commands):**
--model=<model name>: Specify an alternative AI model to use
--max-tokens=<number>: Control response length
--save-to=<file path>: Save command output to a file (in *addition* to displaying it)
--help: View all available options (help is not fully implemented yet)

**Documentation Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Generate documentation for a remote GitHub repository

**GitHub Command Options:**
--from-github=<GitHub username>/<repository name>[@<branch>]: Access PRs/issues from a specific GitHub repository

**Browser Command Options (for 'open', 'act', 'observe', 'extract'):**
--console: Capture browser console logs (enabled by default, use --no-console to disable)
--html: Capture page HTML content
--network: Capture network activity (enabled by default, use --no-network to disable)
--screenshot=<file path>: Save a screenshot of the page
--timeout=<milliseconds>: Set navigation timeout (default: 30000ms)
--viewport=<width>x<height>: Set viewport size (e.g., 1280x720). When using --connect-to, viewport is only changed if this option is explicitly provided
--headless: Run browser in headless mode (default: true)
--no-headless: Show browser UI (non-headless mode) for debugging
--connect-to=<port>: Connect to existing Chrome instance
--wait=<duration or selector>: Wait after page load (e.g., '5s', '#element-id', 'selector:.my-class')
--video=<directory>: Save a video recording of the browser interaction to the specified directory (1280x720 resolution). Not available when using --connect-to

**Additional Notes:**
- For detailed information, see `node_modules/cursor-tools/README.md` (if installed locally).
- Configuration is in `cursor-tools.config.json` (or `~/.cursor-tools/config.json`).
- API keys are loaded from `.cursor-tools.env` (or `~/.cursor-tools/.env`).
- Browser commands require separate installation of Playwright: `npm install --save-dev playwright` or `npm install -g playwright`.
- **Remember:** You're part of a team of superhuman expert AIs. Work together to solve complex problems.
<!-- cursor-tools-version: 0.5.0 -->
</cursor-tools Integration>
</file>

<file path=".gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
.vercel
</file>

<file path="components.json">
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "default",
  "rsc": false,
  "tsx": true,
  "tailwind": {
    "config": "tailwind.config.ts",
    "css": "src/index.css",
    "baseColor": "slate",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/lib/utils",
    "ui": "@/components/ui",
    "lib": "@/lib",
    "hooks": "@/hooks"
  }
}
</file>

<file path="eslint.config.js">
import js from "@eslint/js";
import globals from "globals";
import reactHooks from "eslint-plugin-react-hooks";
import reactRefresh from "eslint-plugin-react-refresh";
import tseslint from "typescript-eslint";
export default tseslint.config(
  { ignores: ["dist"] },
  {
    extends: [js.configs.recommended, ...tseslint.configs.recommended],
    files: ["**/*.{ts,tsx}"],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
    plugins: {
      "react-hooks": reactHooks,
      "react-refresh": reactRefresh,
    },
    rules: {
      ...reactHooks.configs.recommended.rules,
      "react-refresh/only-export-components": [
        "warn",
        { allowConstantExport: true },
      ],
      "@typescript-eslint/no-unused-vars": "off",
    },
  }
);
</file>

<file path="index.html">
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="description" content="Your application description" />
    <title>Vite + React + TS</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="package.json">
{
  "name": "vite_react_shadcn_ts",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "lint": "eslint src --ext ts,tsx --report-unused-disable-directives --max-warnings 0",
    "preview": "vite preview",
    "typecheck": "tsc --noEmit",
    "format": "prettier --write \"src/**/*.{ts,tsx,css,md}\""
  },
  "dependencies": {
    "@google/generative-ai": "^0.21.0",
    "@hookform/resolvers": "^3.10.0",
    "@radix-ui/react-accordion": "^1.2.0",
    "@radix-ui/react-alert-dialog": "^1.1.1",
    "@radix-ui/react-aspect-ratio": "^1.1.0",
    "@radix-ui/react-avatar": "^1.1.0",
    "@radix-ui/react-checkbox": "^1.1.1",
    "@radix-ui/react-collapsible": "^1.1.0",
    "@radix-ui/react-context-menu": "^2.2.1",
    "@radix-ui/react-dialog": "^1.1.6",
    "@radix-ui/react-dropdown-menu": "^2.1.1",
    "@radix-ui/react-hover-card": "^1.1.1",
    "@radix-ui/react-label": "^2.1.2",
    "@radix-ui/react-menubar": "^1.1.1",
    "@radix-ui/react-navigation-menu": "^1.2.0",
    "@radix-ui/react-popover": "^1.1.1",
    "@radix-ui/react-progress": "^1.1.0",
    "@radix-ui/react-radio-group": "^1.2.0",
    "@radix-ui/react-scroll-area": "^1.1.0",
    "@radix-ui/react-select": "^2.1.1",
    "@radix-ui/react-separator": "^1.1.0",
    "@radix-ui/react-slider": "^1.2.0",
    "@radix-ui/react-slot": "^1.1.2",
    "@radix-ui/react-switch": "^1.1.0",
    "@radix-ui/react-tabs": "^1.1.0",
    "@radix-ui/react-toast": "^1.2.6",
    "@radix-ui/react-toggle": "^1.1.0",
    "@radix-ui/react-toggle-group": "^1.1.0",
    "@radix-ui/react-tooltip": "^1.1.4",
    "@supabase/supabase-js": "^2.48.1",
    "@tanstack/react-query": "^5.56.2",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "cmdk": "^1.0.0",
    "date-fns": "^3.6.0",
    "embla-carousel-react": "^8.3.0",
    "input-otp": "^1.2.4",
    "lucide-react": "^0.462.0",
    "openai": "^4.85.1",
    "react": "^18.3.1",
    "react-day-picker": "^8.10.1",
    "react-dom": "^18.3.1",
    "react-hook-form": "^7.54.2",
    "react-resizable-panels": "^2.1.3",
    "react-router-dom": "^6.26.2",
    "recharts": "^2.12.7",
    "sonner": "^1.5.0",
    "tailwind-merge": "^2.6.0",
    "vaul": "^0.9.3",
    "vercel": "^41.1.4",
    "zod": "^3.24.2"
  },
  "devDependencies": {
    "@eslint/js": "^9.9.0",
    "@tailwindcss/typography": "^0.5.15",
    "@types/node": "^22.5.5",
    "@types/react": "^18.3.3",
    "@types/react-dom": "^18.3.0",
    "@vitejs/plugin-react-swc": "^3.8.0",
    "autoprefixer": "^10.4.20",
    "cursor-tools": "^0.5.0",
    "eslint": "^9.9.0",
    "eslint-plugin-react-hooks": "^5.1.0-rc.0",
    "eslint-plugin-react-refresh": "^0.4.9",
    "globals": "^15.9.0",
    "lovable-tagger": "^1.1.3",
    "postcss": "^8.5.2",
    "tailwindcss": "^3.4.11",
    "tailwindcss-animate": "^1.0.7",
    "typescript": "^5.5.3",
    "typescript-eslint": "^8.0.1",
    "vite": "^5.4.1",
    "vitest": "^3.0.6"
  }
}
</file>

<file path="postcss.config.cjs">
module.exports = {
  plugins: {
    'tailwindcss/nesting': {},
    tailwindcss: {},
    autoprefixer: {},
  },
};
</file>

<file path="README.md">
PRD DOCUMENT: 

Understood! I will now compile all research, insights, and specifications into a **final, all-in-one dossier** that includes:

- **Executive Summary & Branding** (DeepType by Empathy Labs, under the DeepHustle.ai project)
- **Ultimate Pitch & Pitch Deck** (Both in Markdown format and separate file if possible)
- **Comprehensive PRD** (Product Requirements Document)
- **SWOT Analysis & Gap Analysis**
- **Feature Breakdown & Roadmap** (Token-based deep research breakdown instead of timeframes)
- **Accessibility-first UX/UI Guidelines** (ARIA descriptions, alt-text, voice-first UI details)
- **Full Codebase** (Repository structure, written-out files, ASCII art headers for inclusivity, inline comments for easy comprehension)

I'll ensure it is structured optimally, with **ultra-clear formatting using Markdown syntax** for headers, sections, and technical clarity.

I'll get back with the fully compiled dossier shortly.

# DeepType by Empathy Labs: Comprehensive Project Dossier

## 1. Executive Summary & Branding

**Vision Statement:** *Empower every individual – regardless of vision or ability – to master typing and digital communication through empathetic AI.* DeepType’s mission is to be an **AI-powered typing tutor** that enables blind and visually-impaired users to achieve keyboard proficiency, opening doors to education, employment, and independence. With a focus on **“accessible by design”**, DeepType bridges the digital divide by turning the keyboard into an inclusive gateway for all.

**Naming & Brand Identity:** **DeepType** is the product name, reflecting the use of deep learning (“Deep”) to revolutionize touch typing (“Type”). It exists under the **DeepHustle.ai** project umbrella – an innovation initiative by **Empathy Labs**. Empathy Labs is the organization’s brand, emphasizing user-centric design and emotional intelligence in tech. The naming strategy ensures clarity: Empathy Labs conveys trust and compassion, DeepHustle.ai signals cutting-edge AI innovation, and DeepType itself clearly describes the product’s function. Together, they present a unified brand with a heart (Empathy) and a brain (Deep AI technology).

**Market Positioning:** DeepType is positioned at the intersection of **assistive technology** and **ed-tech**. It targets a significant underserved market: the millions of people with visual impairments who require effective tools to learn keyboard skills. Globally, an estimated **43 million people are blind and 295 million have moderate-to-severe visual impairment** ([Bridging the Digital Disability Divide: Determinants of Internet Use among Visually Impaired Individuals in Thailand](https://www.mdpi.com/2673-7272/4/3/43#:~:text=difficulty%20seeing%20well%20at%20a,that%20the%20prevalence%20of%20visual)). Many of these individuals rely on screen readers (like JAWS or NVDA) to use computers, yet accessible typing training tools are scarce and outdated. DeepType aims to be the **premier digital solution for accessible typing education**, much like a “Duolingo for typing” tailored to blind and low-vision users. By leveraging AI and modern UX practices, it stands out from legacy offerings. The brand promise is empowerment: *DeepType gives users the confidence to navigate the digital world through touch typing.* This resonates strongly in a market where independence and employment are tightly linked to technological skills – especially when **over 70% of blind and visually-impaired adults face unemployment, partly due to limited access to training** ([Employment Barriers for the Blind and Visually Impaired — World Services for the Blind](https://www.wsblind.org/blog/2021/6/16/employment-barriers-for-the-blind-and-visually-impaired#:~:text=variety%20of%20reasons%20from%20lack,impaired%20and%20how%20to%20overcome)). DeepType will be marketed as an **inclusion-driven innovation**: not just another typing app, but a social impact tool that **combines empathy with technology** to transform lives. 

## 2. Ultimate Pitch & Pitch Deck

### High-Impact Introduction (The Elevator Pitch)
Imagine **pressing a key** and unlocking a world of opportunity. DeepType is an AI-powered tutor that speaks, listens, and *understands*, turning the once tedious task of learning to type into an empowering journey for those who need it most. **DeepType by Empathy Labs** is *the first accessibility-first typing coach*, designed for the blind and visually impaired, but delightful for everyone. It’s like having a personal coach who never gets tired, never judges, and is available 24/7 – **“the Swiss Army Knife of typing tutors”** for all abilities.

> **“Because everyone deserves a voice and a keyboard.”**

### Key Differentiators & Competitive Edge
- **🎯 Built for Accessibility from Day One:** Unlike generic typing programs, DeepType is built *ground-up for blind and low-vision users*. Every feature – from audio guidance to high-contrast visuals – follows accessibility best practices (WCAG 2.1, ARIA roles, etc.). This isn’t a retrofit; it’s a revolution. We don’t just meet compliance, we embrace **“accessibility-first”** design as our core ethos.
- **🗣 Voice-First and Hands-On:** DeepType is a **voice-interactive tutor**. It speaks instructions and encouragement in natural language, and listens for commands. Users can navigate the entire learning experience with speech or a single button. This **voice-first interaction** means eyes-free, hassle-free learning – perfect for blind users and also convenient for anyone (think learning while keeping your eyes on another task).
- **🤖 Adaptive AI Coach:** At the heart of DeepType is cutting-edge AI (including large language models) that adapts in real-time to the learner. Make a mistake? DeepType’s AI detects the pattern and offers **personalized feedback and exercises**. Struggling with certain letters? The AI dynamically adjusts the lesson to give extra practice where needed. No static lessons or one-size-fits-all curriculum – **DeepType learns the learner**.
- **🌐 Cross-Platform Convenience:** Available on **web, desktop, and mobile** without separate development silos. Our tech stack enables writing code once and deploying everywhere, ensuring a consistent experience whether the user is on a Windows PC with a screen reader, a Mac, or a mobile device. DeepType even works offline for desktop (via an app) so it’s reliable in classrooms and areas with limited internet.
- **💡 Competitive Edge over Legacy Tools:** Traditional solutions like *Talking Typing Teacher* rely on pre-recorded voices and fixed sc ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))L146】. DeepType, however, uses AI voices and natural language generation to provide **conversational, context-aware guidance**. Unlike screen readers (JAWS/NVDA) that simply echo keys, DeepType teaches with a curriculum, tracks progress, and gamifies the experience. And unlike costly enterprise software, DeepType aims to be affordable or free for end-users (with a sustainable business model backing it – see below).

### Problem & Opportunity
For a visually impaired person, learning to type is not just a skill – it’s a lifeline to the digital world. Yet current options are bleak: decades-old software with robotic feedback, expensive licenses, or relying on general screen readers that **don’t teach**. The result? Many blind individuals struggle with slow typing or never learn, limiting their potential in school and the workplace. **Unmet need:** an engaging, effective, and affordable way to learn keyboarding without sight.

**Opportunity:** DeepType sits at the convergence of two rising tides – the advancement of AI in education, and the push for digital inclusion. Recent breakthroughs show the promise of AI in assistive tech (e.g., *Be My Eyes* integrating OpenAI’s GPT-4 to describe images for blind  ([Introducing Be My AI (formerly Virtual Volunteer) for People who are Blind or Have Low Vision, Powered by OpenAI’s GPT-4](https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer#:~:text=a%20dynamic%20new%20image,a%20wide%20variety%20of%20tasks))1-L4】). Yet, no one has applied such AI prowess to *typing education* for the blind. We have the first-mover advantage to capture this niche and expand it to a broader audience (sighted users also benefit from voice-assisted, hands-free learning – think driving, or dyslexic learners using multi-sensory feedback). 

DeepType can become the **gold standard** for accessible skills training, starting with typing and potentially expanding to other digital literacy skills. By solving a deeply specific problem with excellence, we build trust and brand loyalty in the assistive tech community.

### Business Model & Monetization Strategy
DeepType’s business model balances social impact with sustainability:

- **Freemium Core:** The base product (core typing lessons and accessibility features) is **free for individual users**, ensuring that cost is never a barrier for those who need it most. This echoes the strategy of NVDA, the free screen reader that rapidly gained global adoption – now used by ~65% of screen reader  ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=NVDA%20is%20again%20the%20most,of%20respondents))1-L4】, surpassing its expensive rival JAWS.
- **B2B and Institutional Licensing:** Revenue is generated by offering premium plans to schools, rehabilitation centers, and enterprises. For example:
  - **Education Edition:** Schools for the blind or K-12 special education programs can subscribe to a managed DeepType Classroom package, which includes teacher dashboards, student progress analytics, and custom lesson creation. These institutions often have funding or grants for assistive technology and will pay for a solution that demonstrably improves student outcomes.
  - **Corporate Training & CSR:** Companies aiming to hire and upskill visually-impaired employees (or retrain workers who lost vision) can license DeepType for professional use. Additionally, corporations could sponsor DeepType deployments as part of their Corporate Social Responsibility programs, effectively *sponsoring free licenses for users* in developing regions (a model similar to how some companies sponsor NVDA development via donations).
- **Premium Features for Power Users:** While the basic tutor is free, advanced features could be behind a modest subscription. Examples: an **AI “Tutor Plus”** that users can converse with to get career advice or advanced typing drills, cloud sync of personal progress across devices, or the ability to **generate custom practice content** (e.g., “I want to practice typing this specific book or code snippet” – the AI prepares a lesson). These power features cater to enthusiasts or professionals and can justify a monthly fee.
- **Grants and Partnerships:** Given its mission-driven nature, DeepType will aggressively seek partnerships with nonprofits (e.g., American Foundation for the Blind) and technology grants. These partnerships not only provide funding but also endorsements and user base. For instance, a foundation might fund the development of a new Braille-display integration feature, or a government agency might deploy DeepType in digital literacy initiatives.
- **Open-Source Community Edition:** A portion of DeepType’s code (especially accessibility utilities) could be open-sourced to encourage community contributions and transparency. This fosters goodwill and potentially free improvements, while the core AI tutoring logic and premium services remain proprietary for monetization. It’s similar to how some companies open-source their SDKs but sell hosted services.

**Monetization with Empathy:** At all times, our strategy ensures that **the end-user who most needs DeepType (a blind learner)** is never left out due to cost. Revenue streams target those who *can* pay (schools, orgs, sponsors) to subsidize those who cannot. This aligns with our brand values (Empathy Labs) and creates a positive feedback loop: more users -> more data -> better AI -> more compelling product -> more paying partners.

### Microcopy & Persuasive Messaging
DeepType’s voice and tone are **motivational, friendly, and inclusive**. We use microcopy (short pieces of guiding text or audio) to keep users engaged and encouraged:

- Onboarding greeting: *“Welcome to DeepType – where your fingers learn to sing on the keyboard. Let’s unlock your potential, one key at a time!”*
- When the user makes a mistake: *“Whoops, that didn’t match. No worries – try again, I’m right here with you.”* (No scolding, always supportive.)
- Success message: *“Great job! You nailed that. Ready for the next challenge?”*
- Idle encouragement (if user is inactive): *“I’m still here. Whenever you’re ready, press the space bar and we’ll continue your journey.”*
- Interface labels use empowering language: The “Start” button might say **“Begin Your Journey”**, the help section: **“Need a hand? (Press H)”** spoken as *“You can press H anytime for help – I’ve got tips for you.”*

Persuasive messaging also highlights outcomes: After a progress milestone, DeepType might say, *“You’ve improved your speed by 20%! Imagine writing emails or coding with this speed. You’re on fire!”* This connects the practice to real-life benefits, sustaining motivation.

Throughout our copy, we emphasize **independence, confidence, and fun**. Typing is framed not as a tedious skill, but as *liberation*. *“Your voice in the digital world”* is a recurring theme – since typing enables one to communicate just like sighted peers. Our branding tagline could be: **“DeepType: Touch the keys, touch the world.”** This resonates emotionally and sticks in memory.

### Accessibility-First Product Positioning
DeepType proudly wears the “accessibility-first” badge. In pitches and materials, we make it clear this is **not an afterthought or add-on**. For example, the product website and pitch deck prominently state: **“Designed for accessibility from scratch – not retrofitted.”** We highlight features like:
- **Voice Guided Learning:** All lessons are delivered through clear speech, so a user with zero vision can participate without any setup. *“If you can hear, you can learn with DeepType.”*
- **One-Button Navigation:** The entire UI can be driven with a single key or switch device. This means even users with motor challenges or cognitive overload can navigate step-by-step. (For instance, an on-screen highlight moves through options and the user hits the one button to select – a common approach for switch accessibility.)
- **High Contrast & Large Print:** For low-vision users, DeepType offers bold, large text and high-contrast color themes out of the box. Screenshots in the pitch deck demonstrate a stark black-and-white interface with >AAA contrast, showing our dedication to usable design for low vision.
- **Screen-Reader Friendly:** DeepType works harmoniously with screen readers. However, it often won’t need a screen reader’s assistance because it *is* the screen reader for its own interface. (E.g., the app announces “Menu: Start, Settings, Exit – use arrow keys or say ‘Start’” etc.) This dual approach (integrating with or without external screen reader) means flexibility for user preference.
- **Multimodal Input:** The user can *speak* commands, *type* responses, or even use a *Braille display* in future iterations. DeepType positions itself as *device-agnostic*. If you can press any button or utter a word, you can control it. Such flexibility is rare and a strong selling point in assistive tech.

By positioning accessibility at the forefront, we also capture the interest of allies: educators, parents, occupational therapists, and diversity officers who seek tools that champion inclusive design. DeepType isn’t just a tool; it’s a statement that technology should serve everyone. Our pitch emphasizes this higher purpose, which not only differentiates us but also often sways decision-makers (e.g., a school district choosing between a generic typing software vs. DeepType will see that only DeepType was *built* for their blind students’ needs).

### Pitch Deck Outline (Markdown Slides)

*(Below is a markdown representation of the Pitch Deck for DeepType. Each slide is described with its title and key bullet points.)*

**Slide 1: Title & Vision**  
**DeepType** by Empathy Labs  
*The AI-Powered Typing Tutor for All*  
- *Vision:* Empower every individual to communicate digitally, regardless of vision or ability.  
- *Tagline:* **Touch the keys, touch the world.**  

**Slide 2: The Problem**  
- 285 million people have visual impairments, 43 million blind worl ([Bridging the Digital Disability Divide: Determinants of Internet Use among Visually Impaired Individuals in Thailand](https://www.mdpi.com/2673-7272/4/3/43#:~:text=difficulty%20seeing%20well%20at%20a,that%20the%20prevalence%20of%20visual))1-L4】.  
- Typing = essential skill for education & jobs, yet **no effective way to learn if you can’t see the keyboard.**  
- Legacy solutions are outdated, hard to access, or extremely expensive. (E.g., 70% unemployment among blind partly due to lack of tech s ([Employment Barriers for the Blind and Visually Impaired — World Services for the Blind](https://www.wsblind.org/blog/2021/6/16/employment-barriers-for-the-blind-and-visually-impaired#:~:text=variety%20of%20reasons%20from%20lack,impaired%20and%20how%20to%20overcome))-L77】.)  
- *Opportunity:* Huge gap in assistive education – **time to innovate.**

**Slide 3: The Solution**  
- **DeepType** – an AI tutor that **speaks, listens, and adapts** to the user.  
- Learn to type through interactive audio lessons, real-time feedback, and personalized practice.  
- Use it on any device: phone, tablet, computer – no sight required.  
- Outcome: Blind/low-vision users gain digital independence (sending emails, coding, writing) by mastering typing.

**Slide 4: Why Now? (Market Timing)**  
- **AI & Accessibility Renaissance:** AI is enabling new assistive tech (e.g., GPT-4 Vision in Be My ([Introducing Be My AI (formerly Virtual Volunteer) for People who are Blind or Have Low Vision, Powered by OpenAI’s GPT-4](https://www.bemyeyes.com/blog/introducing-be-my-eyes-virtual-volunteer#:~:text=a%20dynamic%20new%20image,a%20wide%20variety%20of%20tasks))1-L4】, Seeing AI by Microsoft combining visio ([Seeing AI: New Technology Research to Support the Blind and Visually Impaired Community - Microsoft Accessibility Blog](https://blogs.microsoft.com/accessibility/seeing-ai/#:~:text=Seeing%20AI%20will%20use%20computer,identify%20emotions%20on%20people%E2%80%99s%20faces))L137】). It’s *proven* that AI can make tech more inclusive.  
- **Tech Convergence:** Speech recognition, text-to-speech, and language models are all advanced *and affordable* in 2025 – enabling our solution.  
- **Remote Learning Boom:** Post-2020, digital learning tools are mainstream. Accessibility in e-learning is a highlighted need. Institutions seek solutions like DeepType to include all learners.

**Slide 5: Key Features (What makes DeepType special)**  
- 🗣 **Voice-Guided Lessons:** Friendly voice instructions and feedback. *Hands-free learning.*  
- 🤖 **Adaptive Learning AI:** Adjusts difficulty in real-time, generates custom exercises on the fly. *No one gets left behind or bored.*  
- 🎮 **Gamified & Motivating:** Progress badges, fun sound cues, and challenges keep learners engaged. *It’s fun!*  
- 💻 **Cross-Platform:** Use it via web browser, dedicated desktop app, or on mobile. Consistent experience, cloud-synced progress.  
- ♿ **Accessibility at Core:** High-contrast UI, ARIA labels, one-switch mode, works with screen readers – *built to WCAG standards.*  

**Slide 6: Competitive Landscape**  
- **Talking Typing Teacher (Legacy software):** Audio-based lessons with recorded voice. *Cons:* static content, no AI, Windows-only, n ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))137-L146】.  
- **TypeAbility (JAWS add-on):** Teaches typing within JAWS screen reader. *Cons:* requires expensive JAWS, dated curriculum.  
- **NVDA/JAWS Screen Readers:** Not actually teaching tools – they identify keys but don’t provide structured lessons (only a “keyboard help” mode). Also, JAWS costs  ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】 or $1200 perpetual, pricing many out.  
- **Mainstream Typing Apps (e.g., Mavis Beacon):** Visual-centric, unusable for blind users; no voice guidance.  
- **Others (Seeing AI, Be My Eyes):** Solve *different* problems (environment perception, not typing).  
**DeepType’s Edge:** No one offers an **AI-driven, accessibility-first typing tutor.** We combine the strengths of these (voice output, structured curriculum) and add modern AI adaptivity and multi-platform support. We’re in a league of our own – a blue ocean in assistive ed-tech.

**Slide 7: Business Model**  
- **Free for Users:** Core app free for end-users (removing adoption barriers, like NVDA did with screen readers).  
- **B2B/Institutional Sales:** Revenue from schools, rehab centers, libraries – annual licenses including admin dashboards & priority support.  
- **Sponsored Programs:** Partner with nonprofits/corporates to sponsor deployments (CSR funding covers costs for communities in need).  
- **Premium Add-ons:** Optional subscription for advanced personal features (e.g., conversational practice buddy, specialty courses like “coding keyboard shortcuts” or “Excel navigation”).  
- **Scaling Plan:** Start in assistive tech niche -> expand features for general audience (e.g., sighted people using voice tutor while multitasking) -> position as a universal typing tutor (with accessibility as our differentiator and moral backbone).  

**Slide 8: Roadmap & Milestones**  
- **Q1:** MVP launch (Web app beta) – Core lessons A-Z, basic voice feedback. Collect user feedback.  
- **Q2:** Desktop/Mobile apps release (wrapped PWA). Add voice command navigation, cloud sync via Supabase.  
- **Q3:** AI Adaptive Engine v2 – integrate GPT-based error analysis and *live coaching*, plus initial multi-language support (type in English, Spanish, etc.).  
- **Q4:** Institutional Dashboard – analytics for classrooms, content editor for teachers. Begin pilot programs with 3 blind schools.  
- **Year 2:** Scale to 10K+ users, GPT-4 (or Gemini) powered conversational tutor (“Ask DeepType anything”), Braille display integration, and pursue Series A funding for growth.  

**Slide 9: Team & Empathy Labs**  
- **Founders:** [Your Name] – (Background in AI and personal connection to accessibility), [Other Name] – (EdTech veteran, created learning curricula).  
- **Empathy Labs** – Innovation lab focusing on human-centric AI. DeepHustle.ai is our project incubator blending deep learning with human empathy.  
- Advisors include accessibility experts (e.g., a blind tech lead, special-ed teacher) and AI researchers. Backed by [Mentors/Accelerator if any].  
- *Our superpower:* We combine technical expertise with lived experience insights – building *with* the community, not just for them.

**Slide 10: The Ask & Closing**  
- **Seeking:** [If pitching to investors: $X seed funding] OR [If pitching for partnership: pilot opportunities, introductions, etc.].  
- This will fuel development of advanced features (real-time voice AI, more languages) and allow us to distribute DeepType to those who need it globally.  
- **Impact:** A successful DeepType means thousands of people will gain skills, confidence, and jobs that were previously out of reach. It’s not just an investment in a product, it’s an investment in *digital equality*.  
- *Join us* in typing a new chapter of inclusion.  
- **Thank You.**  
*(Contact: your.email@empathylabs.ai | www.deephustle.ai/deeptype)*

*(End of Pitch Deck)*

*Note: Images and graphics (previously planned) are omitted in this text version, but would include screenshots of the app interface, icons representing voice/AI, and an illustrative user persona to humanize the story.* 

## 3. Comprehensive PRD (Product Requirements Document)

### Core Functionalities & User Stories
DeepType’s product requirements focus on delivering a **fully accessible, intelligent typing tutor**. The core functionalities include:

- **Interactive Audio Lessons:** The system provides step-by-step typing lessons through audio prompts. *User Story:* *“As a blind beginner, I want the tutor to tell me which fingers and keys to use so I can learn touch typing without sight.”*  
  - The lesson content ranges from learning home row keys to complex sentences. Each lesson is articulated by a pleasant human-like voice (text-to-speech).
  - The user can control playback: say “repeat” or press a key to hear instructions again. The lesson flows at the user’s pace, waiting for input and giving feedback.
- **Real-time Feedback and Error Correction:** As the user types, DeepType instantly checks input. If the user presses the correct key, they get positive feedback (a chime sound or voice “Good!”). If wrong, the system signals it (buzz sound or “Oops, try again, that was X, not Y”). *User Story:* *“If I mistype, I want immediate correction so I know what to fix.”*  
  - The system identifies which key was pressed in error and can speak its name (“You pressed K, but the target was F”). This reinforces learning of key positions.
  - Errors are logged to adapt difficulty (e.g., if user consistently struggles with a particular hand or letter, the AI will introduce extra practice for it).
- **Voice Commands & Navigation:** DeepType supports a set of voice commands to navigate the app hands-free. *User Story:* *“As a user who can’t see the screen, I want to be able to speak commands like ‘next lesson’ or ‘main menu’ to control the app.”*  
  - Key commands: “Start lesson one”, “Pause”, “Resume”, “Repeat”, “Menu”, “Help”. Alternatively, a single keyboard key (like the spacebar or a special “DeepType Key”) can cycle through options and select them, accommodating one-switch users.
  - The app includes a voice-controlled onboarding/tutorial where it teaches the user how to use these voice or single-key controls (with practice, e.g., “Press the spacebar to select an option now”).
- **User Progress Tracking:** Each user has a profile with their lesson progress, typing speed (WPM), and accuracy stats. *User Story:* *“I want to track my improvement over time and resume where I left off.”*  
  - Achievements or badges are unlocked for milestones (e.g., 5 lessons completed, first 30 WPM speed, etc.) to encourage progress.
  - Data like last lesson completed, current difficulty level, and custom preferences (voice speed, theme) are saved (likely in a cloud database if logged in, or locally if offline).
- **Adaptive Lesson Planning (AI-driven):** The content adapts to the user’s performance. *User Story:* *“If I’m finding something too easy or too hard, I want the tutor to adjust so I’m always appropriately challenged.”*  
  - If the user breezes through lessons with high accuracy, the AI might skip some redundant practice or suggest a more advanced exercise (e.g., move from letters to words sooner).
  - If the user struggles, the AI can inject an extra practice session focusing on troublesome keys, or slow down the pace of new key introductions.
  - This is powered by a rules engine enhanced with machine learning: initially simple (if accuracy < X, repeat lesson), later more complex using pattern recognition (e.g., “user often swaps S and A – maybe do a drill contrasting those letters”).
- **Multimodal Teaching Aids:** While primarily audio, DeepType can optionally display visual aids for those who have some vision:
  - An on-screen keyboard graphic highlighting the key to press (with high contrast colors).
  - Large-print text of the current exercise (e.g., the word or sentence to type) in case low-vision users want to follow along visually.
  - These aids have proper alt-text or ARIA descriptions so even if a blind user accidentally focuses them, the screen reader will say something like “Visual keyboard illustration, current key: F” rather than leaving them in the dark.
- **Content Variety & Gamification:** To keep engagement, the PRD includes mini-games and varied content:
  - Typing games that are audio-based (for example, an audio version of “falling words”: the voice says a random word and the user must type it before another word “falls” – represented by a ticking timer or rising tone; this becomes a score challenge).
  - Fun exercises like typing the lyrics of a song (with the music playing in the background if they want), or typing to control a simple audio game (e.g., “hit the spaceship by typing the letter that corresponds to its coordinate” – this would be described via audio).
  - These are stretch goals, but mentioned in PRD to ensure extensibility of the content engine to support game modes in addition to linear lessons.
- **Accessibility Compliance & Settings:** DeepType will meet or exceed all relevant accessibility guidelines.
  - **Settings include:** TTS voice selection (if multiple voices available), speech rate adjustment, verbosity level (novice mode might speak very verbosely including each character, while advanced mode might assume more and speak less), high contrast toggle, and an option to integrate with existing screen reader (for those who prefer their JAWS voice for consistency – the app could output via screen reader instead of its own TTS).
  - It will also have an **“Audio-only mode”** which ensures that all necessary information is spoken without requiring any visual element (for pure blind usage with no screen).
  - Conversely a “Visual Assist mode” can provide on-screen hints for sighted or low-vision supporters (like a parent or teacher watching can see what’s going on). This dual-output approach ensures both blind users and sighted helpers get what they need.

These core features are driven by our guiding principle: **teach typing in a way that feels natural and supportive to someone who cannot see.** The PRD ensures that any feature that involves output must have an audible form, and any input must be possible via keyboard or voice (not just mouse/touch). 

### Technical Stack Overview
To implement the above, DeepType leverages a modern and flexible technical stack, stitching together AI services, cross-platform frameworks, and accessible UI libraries:

- **Frontend:** We plan to use **Web technologies (HTML, CSS, TypeScript/JavaScript)**, likely within a framework like **React** for modularity. The UI will be a web app at its core for easy cross-platform reach. We might employ frameworks or libraries that aid in building accessible components (e.g., Reach UI or ARIA toolkit for React, which provide pre-built accessible widgets).
- **Lovable.dev & Bolt.new (AI-assisted development):** To accelerate development, we will experiment with AI-powered coding tools. **Lovable.dev** can scaffold a project quickly – generating a React/TypeScript app with our described ([Lovable.dev - AI Web App Builder | Refine](https://refine.dev/blog/lovable-ai/#:~:text=It%20acts%20as%20your%20AI,it%20all%20with%20remarkable%20efficiency))†L71-L79】. It acts as an “AI co-engineer,” setting up the skeleton (routing, basic pages, API integrations with Supabase, etc.) from natural language specs. **Bolt.new**, an AI-powered development en ([Bolt vs. Cursor: Which AI Coding App Is Better?](https://www.thepromptwarrior.com/p/bolt-vs-cursor-which-ai-coding-app-is-better#:~:text=Bolt%20%28bolt.new%29%20is%20a%20web,powered%20development%20environment))†L62-L70】, will be used to prototype interface components rapidly. For example, by prompting “Create a high-contrast landing page with a ‘Start Lesson’ button and our logo,” we can get boilerplate that we then refine. These tools don’t replace coding but speed up initial setup and repetitive tasks, allowing the team to focus on complex logic. *Rationale:* This fits our lean startup approach – we leverage AI to build AI software faster.
- **Backend:** The backend is relatively lightweight – mostly to handle persistent data and AI integration:
  - **Supabase (PostgreSQL database + Auth):** We choose Supabase as our primary backend platform. It provides an **open-source Firebase alternative** with a Postgres database, restful APIs, real-time subscriptions, and user authentication out-o ([Best backends for FlutterFlow: Firebase vs Supabase vs Xano](https://www.lowcode.agency/blog/best-backends-for-flutterflow#:~:text=Best%20backends%20for%20FlutterFlow%3A%20Firebase,time%20databases%2C%20authentication%2C))†L33-L40】. This means we can store user profiles, progress logs, achievements, etc., with minimal backend code. Supabase’s auth will manage sign-ups (with email/password or OAuth if needed) and we can secure data (each user’s data is private, etc).
  - Supabase’s real-time capabilities might be used to sync live progress (for instance, if a teacher is remotely monitoring a student’s lesson, the keystrokes per minute or errors could stream to a dashboard in real-time).
  - We also plan to use Supabase’s storage (for any audio clips or if we allow users to upload custom text content to practice on).
- **AI Services:** A cornerstone is integration of advanced AI:
  - **OpenAI API:** We will utilize OpenAI’s APIs for a couple of purposes. One is the **GPT-4 (or GPT-3.5) model** for language tasks – e.g., generating dynamic practice sentences or engaging trivia about what the user is typing (imagine as they practice, the AI shares a fun fact: “Did you know the word ‘TYPE’ originated from…” to keep things interesting). Another is potential use of OpenAI’s new multimodal/voice features. For instance, OpenAI’s Whisper model for speech-to-text and their text-to-speech for voice output. The **OpenAI Whisper API** can transcribe audio (like user’s spoken commands) with high accuracy. While it doesn’t yet support true streaming real-time transcription in the API, we can chunk audio every few seconds to simulate  ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%20am%20aware%20that%20currently,intentions%20to%20make%20this%20live)) ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%E2%80%99ve%20heard%20of%20people%20sending,to%20chunk%20smartly))†L23-L31】. We’ll design our voice command system around this limitation (short commands which can be captured in <5s chunks, so the delay is negligible).
  - **Google Gemini 2.0 (Flash API):** As we future-proof, we note Google’s upcoming **Gemini 2.0** from DeepMind, which promises **multimodal support and lightning-fast  ([Google | Gemini 2.0 Flash API - Kaggle](https://www.kaggle.com/models/google/gemini-2.0-flash-api#:~:text=Google%20,arrow_drop_up%2027))5†L5-L13】**. Once available, this could power the adaptive coach in DeepType, possibly providing even quicker or more nuanced feedback than GPT. For example, Gemini might be used to analyze a user’s pattern of mistakes in depth or run a real-time conversation mode with the user about their progress. The “Flash API” suggests real-time or streaming capabilities, which could help in creating an interactive agent that feels live. The PRD includes support to integrate Gemini as an alternative or supplement to OpenAI (keeping our architecture model-agnostic so we can plug in the best AI).
  - **Text-to-Speech (TTS):** We have options here: use the Web Speech API in browsers for on-device TTS (which leverages system voices) *and/or* use cloud TTS (e.g., Google Cloud Text-to-Speech or Amazon Polly) for consistent high-quality voices across platforms. For simplicity and offline support on desktop, we will use on-device TTS where available. On mobile, we can use the platform’s native screen reader voice via an API or our own integrated voice. The PRD requirement is that the voice must be clear and preferably natural (neural voices). We’ll research which approach yields the best combination of latency and quality. It might be viable to ship a pre-trained lightweight TTS model for offline (for example, Coqui TTS or similar open-source) for an offline desktop mode, while using cloud for online mode.
- **Cross-platform Frameworks:** To achieve **“web, desktop, mobile-native without re-coding”**, we outline a cross-platform strategy:
  - The core is a Progressive Web App (PWA) built in React/TypeScript. This runs in any modern browser (fulfilling the web requirement).
  - **Desktop:** We will use **Tauri** or Electron to wrap the web app into a desktop application for Windows/Mac/Linux. Tauri is preferred for its lightweight, security, and ability to create native binaries with a web ([tauri-apps/tauri: Build smaller, faster, and more secure desktop and ...](https://github.com/tauri-apps/tauri#:~:text=tauri,end%20framework%20that%20compiles))1†L5-L13】. With Tauri, our web code becomes a desktop app that can access local resources (e.g., the microphone for voice input) and run offline. Tauri allows building installers and an app experience that integrates with the OS (start menu, etc.), all while reusing 99% of our code.
  - **Mobile:** For mobile native, we have a couple of paths. We can package the PWA as an app using something like **Capacitor (from Ionic)** which allows deploying web code as native iOS/Android apps with access to native APIs (for mic, vibration, etc.). Alternatively, React Native could be used with the same business logic, but that’s a separate codebase unless we unify via something like Expo’s web support. Given our resource constraints, we lean towards using the web PWA directly on mobile (modern iOS and Android browsers support many necessary APIs). We ensure the PWA is installable (Add to Home Screen) and works offline after first load (using Service Workers for caching). This way, a user can “install” DeepType from the browser and use it like a native app. In parallel, if needed for App Store presence, we can wrap the PWA in a minimal native shell (Capacitor) and publish it. This still avoids rewriting logic.
  - **Shared Code:** We will maintain a single codebase for logic. For any platform-specific code (like file access on desktop, or different speech API on web vs mobile), we use conditional wrappers or services. E.g., an `AudioInputService` interface with implementations: one uses the browser `webkitSpeechRecognition` (for Chrome), another uses the Cordova/Capacitor plugin for speech on mobile, another perhaps uses an Electron node module or the OS’s Speech API for desktop. The app at runtime picks the appropriate one. This design is specified in the PRD to ensure we prep for multi-environment deployment.
- **Cursor AI & Vercel v0.dev (Dev Tools):** While not part of the product delivered to users, internally we incorporate tools like **Cursor (an AI-assisted code editor)** for improving our development speed and code quality. Cursor can help write repetitive code or tests by conversing with the codebase. **V0.dev** by Vercel is another tool that can generate UI components from descriptions, particularly for React + Tailwi ([Vercel v0.dev: A hands-on review · Reflections - Ann Catherine Jose](https://annjose.com/post/v0-dev-firsthand/#:~:text=Vercel%20v0.dev%3A%20A%20hands,CSS%2C%20and%20shadcn%20UI%20components))3†L5-L13】. We will use v0.dev for designing accessible UI components; for example, describe “a large high-contrast toggle switch with ARIA roles” and refine the output. This ensures even the development process keeps accessibility in mind (the AI is likely to include ARIA attributes if we specify).
- **Realtime Collaboration & API**: If we allow a teacher or remote volunteer to monitor or assist a session, we might use WebSockets or Supabase’s realtime channels. This is a potential feature for v2, but the architecture includes the ability (maybe via a Node.js server using **Socket.io** or Supabase realtime) to broadcast events (like “user completed Lesson 2”) to a connected dashboard. 
- **Testing and CI:** The PRD requires that we integrate automated accessibility testing in our CI pipeline. Tools like **axe-core** (by Deque Systems) can be used to run accessibility audits on our interface to catch issues (like missing labels) early. Also, we plan to include unit tests for critical logic (especially the adaptive algorithm – we can write tests simulating sequences of mistakes and ensuring the lesson adaptation logic does what we expect).

In summary, the technical stack is a blend of **AI services (OpenAI, possibly DeepMind)**, **cloud backend (Supabase)**, and **web-centric cross-platform frameworks (React, Tauri/Capacitor)**. This allows us to meet the broad requirements: intelligent behavior, data persistence, and deployment on multiple platforms with one codebase. By using AI coding aids (Lovable.dev, Bolt.new, Cursor), we also significantly reduce development time, enabling a small team to achieve results comparable to a much larger team – a critical advantage for a startup.

### Cross-Platform Architecture Plan
To ensure DeepType runs seamlessly on web, desktop, and mobile **without duplicating effort**, the architecture is carefully planned:

**Overall Architecture:**  
At its core, DeepType is a single-page application (SPA) that communicates with cloud services as needed. Think of it as a layered design:
- **UI Layer:** React components (or similar) render the interface and manage interactions. This layer cares about things like showing a virtual keyboard, displaying text, capturing keypress or microphone input.
- **Logic Layer:** This is the heart – lesson logic, state management (could use something like Redux or React Context for state), and the adaptive algorithm. It’s abstracted so it doesn’t directly depend on browser APIs – meaning it can run in any JS environment (browser, Node, React Native).
- **Platform Services Layer:** These are small modules that handle platform-specific functions: e.g., `SpeechInputService`, `SpeechOutputService`, `StorageService`. The app, instead of calling `window.speechSynthesis.speak` directly, will call an interface method `speak(text)`. In the web build, that maps to `window.speechSynthesis`; in a mobile build, it could call a native plugin; in desktop, maybe it calls an OS-level TTS or uses the web one as well. Similarly for listening: on web, use `SpeechRecognition` API if available; on desktop, possibly the OS dictation API or route audio to our server’s Whisper. We design these as swappable modules.
- **Backend API Layer:** When the app needs to fetch or save data (login, pulling down a new set of practice sentences, updating progress), it calls our backend via REST or GraphQL (Supabase provides RESTful endpoints and client libraries). There’s also calls to AI APIs: those might be direct from the frontend (for less sensitive requests and if CORS allows) or via our backend proxy (for secure calls that involve API secrets). For instance, generating a custom exercise using GPT might be done by sending a request to our backend function `generateExercise` which then calls OpenAI and returns result, so we don’t expose the API key on the client.

**Web App:** 
- Runs entirely in the browser. On modern browsers, even voice is possible: Chrome’s implementation of the Web Speech API allows real-time-ish speech recognition (though not standard across all browsers yet). The web app will detect capabilities: if using Chrome, can enable full voice commands locally; if not, it might fall back to sending audio to server or require keyboard control for commands.
- The web app is the primary development target (fast iteration with hot-reload, devtools, etc.). It will be responsive (using CSS flexbox or grid) so it can fit small mobile screens up to large desktop screens with reflow. We’ll likely implement a simplified layout for mobile (bigger buttons, maybe hide visuals) versus desktop (which could show more info at once).
- We ensure PWA compliance: a service worker for offline caching of assets and perhaps an offline mode where a set of lessons are available without internet. The app manifest will allow it to be installed on home screen.

**Desktop App:** 
- Using **Tauri**, we package the same web app. Tauri essentially provides a Rust core that loads our HTML/CSS/JS and presents it in a native window (using the system’s webview). It also allows calling native code via an API bridge. We will use that for deeper integration: for example, file system access if we want to log data locally, or to use system-level TTS if needed. But we’ll minimize divergence – ideally the app behaves the same as in a browser.
- Building for desktop will produce .exe for Windows, .app for Mac, etc. All logic still runs in JS in the webview, but with optional assists from Rust. Tauri’s security means we should explicitly allow any API calls from JS to Rust (limiting risk). According to Tauri docs, you can build small (a few MB) apps since it doesn’t bundle a full Chromium like ([tauri-apps/tauri: Build smaller, faster, and more secure desktop and ...](https://github.com/tauri-apps/tauri#:~:text=tauri,end%20framework%20that%20compiles))1†L5-L13】.
- We’ll include auto-update in the desktop version (Tauri has mechanisms or we can roll our own checking our server for updates).
- For desktop, one goal is working fully offline. With everything packaged and using on-device TTS, a user could install and run DeepType on a computer with no internet (good for secure environments or those without connectivity).
- One challenge: speech recognition offline. If the PC has no internet, Chrome’s engine won’t work (it sends to Google servers). We may implement a setting “offline mode” where voice commands are disabled or limited to what we can do locally. There are local speech recognition projects (Vosk, Coqui STT) that could be integrated in the desktop via the Rust side for offline STT. That’s an advanced feature – PRD marks it as a possibility if we target truly offline voice input.

**Mobile App:** 
- If running as pure PWA: On Android, Chrome will allow install and mic usage easily. On iOS, Safari PWA has gotten better and does allow some offline caching and even speech synthesis via Web Speech. Speech recognition on iOS Safari might not be available (as Apple hasn’t enabled the API as of iOS 16/17), so on iPhone the voice commands might be limited unless we use a hack with an external service. For the best experience on mobile, a native wrapper with Capacitor is ideal:
  - **Capacitor** wraps our web code into a WebView inside a minimal native app. We get access to Cordova/Capacitor plugins for things like SpeechRecognition (which under the hood could use Siri’s transcription or just present a native prompt). We’d use the capacitor-community speech recognition plugin, and text-to-speech plugin for consistency.
  - With that, we can publish to App Store/Play Store. The code remains the same, just including capacitor JS bridge scripts and some config.
- Mobile considerations: touches and gestures. A blind user on mobile might use VoiceOver or TalkBack screen reader. We need to ensure compatibility (the app’s elements must be properly labeled so if VoiceOver is running, it can read the “Start Lesson” button, etc.). Alternatively, the user might rely solely on our in-app voice and not the system screen reader. We must handle both gracefully:
  - Possibly detect if a screen reader is active (some OS allow detection) and then adjust (for example, if VoiceOver is on, we might not use our custom gestures to avoid conflict).
  - The single-switch idea on mobile: a Bluetooth switch or just tapping anywhere on screen as the one button. We can implement a full-screen invisible button for “select”, and have an automated scanning focus (which the user hears). This is complex but doable; however, voice commands largely cover it too.
- Performance: Mobile devices have limited processing for AI. But since heavy AI (like GPT) calls are cloud-based, and TTS can be offloaded, the client just needs to handle audio playback/recording and UI. We’ll test on mid-range devices to ensure smooth audio and no lag in typing feedback.

**Security & Privacy:** Given we’re dealing with potentially personal data (user progress, maybe voice recordings), architecture includes:
- Using secure communications (HTTPS for all API calls).
- Storing minimal personal data (maybe just email and performance metrics). All sensitive AI processing (like speech or adaptive suggestions) can be done on the fly and not stored, or if stored, anonymized.
- If we allow cloud sync of user data, we’ll ensure compliance with privacy laws (GDPR etc.), provide data export/delete options.
- The voice data: if we do send voice to our servers (like for Whisper transcription), we’ll do it via secure WebSocket or HTTPS and not retain the raw audio after transcription (unless user opts in to share for improvement).
- These details would be in the PRD under a “Non-functional Requirements” section: performance (e.g., “The app should have <200ms latency for keystroke feedback”), security (“User data encrypted at rest and in transit”), and accessibility (which we treat as functional requirement actually, given the nature).

The cross-platform plan in summary: **One codebase, modular design, deploy everywhere.** This approach minimizes redundant work and ensures feature parity across platforms. Users can start on one device and continue on another seamlessly. For instance, a student practices on a PC at school, then later on their phone at home – DeepType will sync their progress via Supabase so the experience continues. This ubiquity is a strong point in our PRD because many existing tools are limited to one platform (e.g., legacy typing software on Windows only). We’re essentially making DeepType available wherever the user is, ensuring consistency and convenience.

## 4. SWOT Analysis & Gap Analysis

To assess DeepType’s strategic position, we conduct a **SWOT analysis** (Strengths, Weaknesses, Opportunities, Threats) along with a **Gap Analysis** of the current market solutions versus user needs.

### SWOT Analysis

**Strengths:**  
- **Accessibility Expertise:** DeepType is designed with accessibility at its core. This specialized focus is a strong differentiator – few competitors can claim the same level of built-in support for blind users. We have first-mover advantage in this niche.  
- **AI-Powered Adaptivity:** Our use of AI (GPT-4, adaptive algorithms) provides a personalized learning experience that static software cannot match. This not only improves effectiveness but also gives us a tech prestige (“the most advanced tutor out there”).  
- **Cross-Platform Reach:** Being available on web, desktop, and mobile greatly expands our user base. Schools that use Windows PCs, individuals on Macs, and users in developing countries who primarily have Android phones – all can use DeepType. This ubiquity is a strength.  
- **Backed by Empathy Labs Vision:** The strong branding and vision (Empathy + Deep Tech) builds trust. Stakeholders are more likely to support a mission-driven product. It’s not just software, it’s a cause. That can galvanize community support, volunteer contributions, etc.  
- **Community and Cost Advantage:** If we keep the core free, we align with the NVDA approach which saw massive communi ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=NVDA%20is%20again%20the%20most,of%20respondents))35†L1-L4】. This can turn users into evangelists. Also, open-sourcing parts can harness community development (strengthening the product beyond our internal capacity).

**Weaknesses:**  
- **Limited Initial Content/Scope:** As a new product, we might launch with a limited curriculum or features compared to mature competitors. For example, Talking Typing Teacher has a whole suite of lessons, games, and a word processor; we might not have all that on day one. Users might find content not deep enough if we don’t rapidly expand it.  
- **Reliance on AI/Internet:** Some features (like voice AI or advanced adaptivity) rely on internet connectivity and third-party APIs. In scenarios of no internet, our experience might degrade compared to offline software that’s fully self-contained. Also, API costs (OpenAI etc.) could become a burden if usage scales and isn’t monetized proportionally.  
- **User Adoption Hurdle:** Ironically, reaching the target users can be challenging. Many visually impaired learners depend on instructors or institutions to recommend tools. Convincing these gatekeepers (teachers, rehab specialists) to try a new product is a slow process. We also must support users who are not tech-savvy – the onboarding must be foolproof. Any small usability issue for a blind user could turn them away. So our margin for error is slim, especially with such a discerning audience that has been trained to rely on known solutions.  
- **Small Team & Resources:** Initially, we are likely a small team. Implementing and maintaining multi-platform software with heavy AI might stretch our resources. We’ll need to prioritize carefully. Lack of certain domain expertise (e.g., if none of us are blind, we might misjudge some UX aspects) could be a weakness – that’s why involving beta users and experts early is crucial.  
- **Unproven Effectiveness:** We believe our approach is superior, but we will need data to prove learning outcomes. Until we gather success stories or studies, some educators might be skeptical if AI adaptivity truly yields better results than, say, structured repetition. We might also face scrutiny: “Does this actually teach touch typing effectively, or is it too fancy?” Overcoming that initial doubt is a challenge.

**Opportunities:**  
- **Market Void / Blue Ocean:** The niche of *accessible typing tutors* is under-served. The main products are few and dated (TypeAbility, Talking Typer, etc.), leaving a large global user base with needs not fully met. This void is our opportunity to become the de facto solution worldwide, much like how JAWS became synonymous with screen reader in the ’90s, or how **Be My Eyes** became essential on smartphones for visual assistance. We can *define* the category.  
- **Institutional Partnerships:** Schools for the blind, vocational rehab centers, libraries, and disability organizations are actively looking for tools to improve digital literacy. For instance, partnering with national blindness federations or ministries of education could give DeepType an official channel to thousands of users. There are also grants in assistive tech we can tap into (e.g., governments funding tech for inclusive education).  
- **Technology Trends:** Voice assistants and voice UIs are trending. People are increasingly comfortable talking to devices (Alexa, Siri, etc.). DeepType rides this wave as a *voice-first educational app*. We can piggyback on this trend in marketing, and even possibly integrate with those ecosystems (imagine an Alexa skill “typing practice” that is a subset of DeepType’s functionality – a stretch idea, but possible).  
- **Expansion to Other Skills:** Once we establish ourselves in typing, the underlying tech (voice interaction + adaptive learning) could apply to other skills for visually impaired users: using a touchscreen, learning braille through audio drills, coding tutorials (teaching programming with spoken guidance), etc. DeepType could evolve into a suite (“DeepSkills” platform). The opportunity is to leverage our tech platform for multiple products, increasing ROI on our R&D.  
- **Corporate Accessibility Compliance:** Companies are under pressure to be more inclusive. DeepType could be marketed to employers to help upskill blind employees or as a tool for diversity training. It might even be used by sighted employees to practice screen reader mode or keyboard-only usage as empathy training. That’s a tangential use case, but an opportunity for B2B sales beyond the obvious.  
- **Competitive Weaknesses:** Our competitors have known weaknesses we can exploit:
  - *JAWS and TypeAbility:* Expensive and thus not accessible to many (JAWS is $90/year or $1200 ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】; TypeAbility requires JAWS to run, doubling cost). We can undercut by being free/low-cost.
  - *Talking Typing Teacher:* No longer actively supported (as noted, manufacturer offers no tec ([Talking Typing Teacher - Standard](https://www.maxiaids.com/product/talking-typing-teacher-standard#:~:text=,Please%20reference%20instructions))). If it’s abandonware, users will gladly switch to a modern supported product.
  - *General Typing Software:* They ignore blind users entirely – an open goal for us to score. Also, even for sighted users, many find traditional typing tutors boring – our unique voice-interactive approach might attract a subset of mainstream users (like those who prefer auditory learning or have ADHD and enjoy a more interactive style).
- **Global Reach & Localization:** Many developing countries have a growing population of visually impaired individuals with increasing access to tech (cheap Android phones, etc.), but absolutely no local-language typing tutors. We can lead in localization – using our AI to generate lessons in various languages quickly. The opportunity to become the go-to solution in non-English markets is huge (and the competition there is basically zero beyond English). This not only is good mission-wise but also opens avenues for funding from international agencies focusing on literacy and disability.

**Threats:**  
- **Competition from Big Tech:** If our idea proves the market, big players might step in. For instance, Microsoft could enhance their *Seeing AI* app or Windows’ Narrator to include a typing tutor mode. They have resources and existing user base (Seeing AI is free and popul ([Seeing AI - Apps on Google Play](https://play.google.com/store/apps/details?id=com.microsoft.seeingai&hl=en_US#:~:text=Seeing%20AI%20is%20a%20free,blind%20and%20low%20vision%20community))†L11-L18】; JAWS or NVDA could implement a tutorial feature in future versions). If an OS-level feature appears, it’s a serious threat (why install DeepType if Windows teaches typing out-of-the-box?). Our defense is to move quickly and establish brand loyalty and superiority before that happens.  
- **Rapid Tech Changes:** The AI we rely on (OpenAI, etc.) is evolving. There’s a risk of API price increases or policy changes (for example, if OpenAI changes how their educational use licenses work or if a free tier is removed, etc.). Also, new AI models might outshine our approach, requiring continuous integration. If we fail to keep up with the latest (like not adopting Google’s superior model once out), a competitor could and get an edge.  
- **Funding Risks:** As a mission product, if we don’t get the right funding, development could stall. It’s a threat that the project might not sustain purely on revenue early on (because we plan to subsidize users). We mitigate this by pursuing partnerships and proving value to paying customers ASAP.  
- **User Adoption Risks:** The visually impaired community often relies on word-of-mouth and is careful with new tech (due to many products over-promising and under-delivering). If our early version has bugs or inaccessible pieces, word could spread and tarnish our reputation. We could get labeled as “not truly accessible” which is hard to recover from. So quality and community engagement are crucial to avoid the threat of negative perception.  
- **Legal/Compliance:** If we were to collect voice data, there are privacy concerns. Mishandling user data could lead to legal issues or distrust. Also, as an educational tool, we need to be careful with claims – if we operate in the EU, we might need to comply with GDPR, etc. Not doing so could shut us out of key markets. While not an immediate competitor threat, regulatory issues can threaten the project’s reach (for instance, needing to ensure our TTS voices are licensed for our use case, etc.).  
- **Competition from Adjacent Fields:** An indirect threat: a mainstream typing tutor might add an “accessibility mode” cheaply (like just adding voice prompts). Even if it’s not as good as DeepType, if it’s part of a product that’s already deployed widely, it could suck away potential users. For example, if Typing.com (a popular free web tutor) decided to add a screenreader-friendly mode, schools might just use that rather than try something new. We have to stay ahead by truly outperforming any half-measures others might do.

### Gap Analysis (Unmet Needs in Assistive Typing Education)

The gap analysis compares **user needs** (especially those of blind/low-vision learners) against what existing solutions provide, highlighting where those solutions fall short and how DeepType fills the gap:

- **Need:** *Accessible, Non-Visual Guidance.*  
  **Gap in current solutions:** Traditional typing software assumes you can see on-screen instructions or hands. Blind users instead use specialized programs or screen readers, but these often provide minimal guidance (maybe just spoken letters). Tools like Talking Typing Teacher addressed this with recorded  ([Talking Typing Teacher | BoundlessAT.com](https://www.boundlessat.com/Keyboards-Mice/Kids-Keyboards/Talking-Typing-Teacher?srsltid=AfmBOooJa3RWfZmlIyFJK2u1U4CjSpjyTTSi9TPWJ9WIqOAlzfRZIslD#:~:text=Talking%20Typing%20Teacher%20is%20a,Eager%20Eddie%20read%20the%20screen))137-L146】, but it’s not interactive beyond fixed lessons. Modern screen readers have a “keyboard learn mode” (press a key and it announces it), but *no structured lessons or progression*. **DeepType’s Fill:** We provide a full curriculum via audio, not just isolated key feedback. It’s interactive and **context-aware voice guidance**, which currently no mainstream or assistive tool fully offers (existing ones either have voice but no AI interactivity, or interactivity but not voice-first). We also allow voice input for control, which none of the old tutors do – they required keyboard navigation through menus, which is cumbersome.
- **Need:** *Adaptive Learning Pace.*  
  **Gap:** Current teaching programs for the blind are one-size-fits-all. For example, TypeAbility has 99 lessons that everyone goes through the same way. If a student already knows some touch typing, they can’t easily skip ahead without manually picking lessons. Conversely, if they struggle, the software doesn’t custom-tailor more practice; the teacher would have to manually repeat lessons. **DeepType’s Fill:** The adaptive AI engine ensures each learner gets a customized experience – essentially a personal tutor adjusting on the fly. This is a major gap we fill; none of the existing products leverage AI or adaptive algorithms. We cite how modern AI can do this, e.g., *“NVDA and JAWS don’t teach, and older tutors don’t adapt – DeepType is the first to personalize typing education in real-time.”*
- **Need:** *Engagement & Motivation.*  
  **Gap:** Learning to type by touch is tedious, more so when done with bland software. Many visually impaired students lose interest in current tutors because they involve repetitive drills with monotonous feedback. Gamification is minimal (Talking Typer might have some games, but audio games were limited due to tech of its time). Also, immediate encouragement is something a human teacher gives – software often just says “incorrect” in a flat tone. **DeepType’s Fill:** We use gamified elements and lively, *human-like encouragement*. The AI voice can vary phrases, crack a mild joke, or reference earlier progress (“You consistently get F right now – fantastic improvement!”). By tracking progress and awarding badges, we introduce a gaming element. There’s currently a gap in turning typing practice into something fun for blind users; DeepType intends to fill that with audio games and challenges. Even small touches like different sound effects for successes vs. mistakes can improve engagement, and our design documents emphasize a rich audio feedback scheme, whereas older software might just beep or say “wrong” in the same tone.
- **Need:** *Modern Platform Support.*  
  **Gap:** A huge practical gap is that many existing solutions run only on certain platforms (mostly Windows). For instance, Talking Typing Teacher is a Windows program from decades ago (does it run on Windows 10/11 easily? Possibly with compatibility mode). Mac users or mobile users have zero options in that category. In today’s world, a learner might want to practice on their phone or tablet – currently not possible with specialized typing tutors. **DeepType’s Fill:** Cross-platform availability. We meet users where they are. No current competitor offers a mobile app for typing for the blind. DeepType will likely be the first to have that. This is a critical gap we fill: *accessibility of the tutor itself* on different devices.
- **Need:** *Support & Community.*  
  **Gap:** As noted, some older products are no longer supported (no updates, no support lines). If a user hits a bug or compatibility issue, they’re stuck. Also, there’s no community around them (perhaps some mailing lists, but nothing active). **DeepType’s Fill:** As a new, mission-driven product, we plan to build a community forum where users can share experiences, ask questions, and where we (developers) actively respond. This community aspect plus active support (even if via email or chat) addresses the frustration gap. Also, if DeepType is offered partly open-source or free, communities of volunteers (like translators, or those making lesson content) can form, which doesn’t exist currently for closed old software.
- **Need:** *Affordability.*  
  **Gap:** The cost of some solutions (JAWS + TypeAbility combo, or even just JAWS’s own training material) is high. Many individuals in low-income settings cannot afford these. There is a clear gap for a *free or low-cost solution*. NVDA filled that gap in screen readers, but for typing tutors, NVDA only goes so far with its basic help mode. **DeepType’s Fill:** Free core offering – we will ensure a student can learn touch typing without paying. This is filling a direct economic gap. By citing JAWS’ cost and NV ([WebAIM: Screen Reader User Survey #10 Results](https://webaim.org/projects/screenreadersurvey10/#:~:text=Which%20of%20the%20following%20desktop%2Flaptop,Dolphin%20SuperNova%2083%205.4)) ([A New Way to Obtain JAWS and ZoomText | Accessworld | American Foundation for the Blind](https://afb.org/aw/19/12/15137#:~:text=))129-L137】, we strengthen the argument that free, quality tools see widespread adoption in this space.
- **Need:** *Integrations & Extendability.*  
  **Gap:** Current tools are pretty closed. For example, a teacher can’t easily add custom lessons in Talking Typing Teacher beyond what’s built-in (maybe they have some minor customization, but likely limited). And those don’t integrate with learning management systems or other tools. **DeepType’s Fill:** Because it’s modern and API-driven, we could integrate with other systems. For instance, a teacher could download a report of a student’s progress or we could integrate with a braille display (to output the text being typed in braille for deaf-blind users – a future possibility). Being built on web tech, we also can update content continuously, push new lessons, etc., which older software cannot unless you install an update. So we fill the gap of an *evolving platform* versus a static product.

In summary, the gap analysis reveals that **DeepType addresses multiple unmet needs**: a truly accessible interface, personalized learning, engaging experience, multi-device support, and affordability – none of which are collectively present in any one existing product. This analysis reinforces our value proposition: **DeepType isn’t just an incremental improvement, it’s a generational leap in assistive typing education.** Our strategy is to communicate this clearly to users and stakeholders: we’re solving problems that have lingered for years in this domain.

## 5. Feature Breakdown & Roadmap (Token-Based Deep Research Allocation)

To build DeepType efficiently, we break down the development into features and modules, and allocate “deep research” resources (like focused R&D tasks or use of large context AI analysis, measured in notional tokens) to each. We also consider the complexity (for debugging and planning).

Below is the **feature development order** (roughly in priority) along with an estimation of research and debugging complexity:

- **1. Audio Lesson Engine** – *Priority: Highest (MVP)*  
  *Description:* The core system to play audio instructions, accept keystroke input, and give immediate audio feedback. This includes the basics of lesson scripting (sequences of prompts and expected inputs).  
  *Research Allocation:* **🔎🔎 (2 tokens)** – Requires researching best practices in teaching touch typing (education domain research) and tuning TTS for clarity. The basic approach is known (many tutors do this), so minimal deep AI research needed beyond selecting a pleasant voice and designing effective prompt wording. Some research into phonetics may help (ensuring letters are pronounced distinctly, e.g., “F” vs “S”).  
  *Complexity/Debugging:* **Medium.** Handling keyboard events and timing feedback is straightforward, but we must fine-tune for different typing speeds and ensure no input is missed. Debugging involves making sure fast typists don’t break it and slow typists aren’t rushed. Also must debug how it behaves if user presses wrong keys multiple times, etc.

- **2. Voice Command & Control** – *Priority: High (MVP adjunct)*  
  *Description:* Implementing the ability to navigate menus or trigger actions with speech (and alternatively with a single key). For MVP, focus on a few essential commands like “repeat”, “next”, “pause”.  
  *Research Allocation:* **🔎🔎🔎 (3 tokens)** – We need to do a deep dive on **speech recognition integration**. Specifically, research how to use the Web Speech API or Whisper for near-real-time command recognition. This might involve experimenting with different recognition approaches (browser vs server) and measuring latency. There’s also research on keyword spotting (like maybe always listening for the word “DeepType” or a wake word). We might allocate one token for investigating Web Speech API constraints, one for Whisper API chunk ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%20am%20aware%20that%20currently,intentions%20to%20make%20this%20live)) ([Transcribe via Whisper in real-time / live - API - OpenAI Developer Community](https://community.openai.com/t/transcribe-via-whisper-in-real-time-live/354877#:~:text=I%E2%80%99ve%20heard%20of%20people%20sending,to%20chunk%20smartly))1】, and one for designing a command vocabulary that’s easy to recognize (e.g., avoiding hard-to-distinguish words).  
  *Complexity/Debugging:* **High.** Speech input can be unpredictable (accents, noise). Debugging this involves lots of testing with different voices and ensuring false positives/negatives are minimized. Ensuring that the voice recognition doesn’t interfere with the lesson audio (it might pick up the tutor’s voice – we likely have to mute the recognizer when the tutor is talking, etc.). One-button control also needs careful state management (scanning menus etc., which can be tricky to debug timing).

- **3. Curriculum Content & Progression** – *Priority: High*  
  *Description:* Develop the actual lessons from beginner to advanced. Define each lesson’s content (keys introduced, practice words, etc.), and the progression logic (# of exercises before unlocking next lesson, criteria to pass).  
  *Research Allocation:* **🔎 (1 token)** – Mostly educational design rather than technical. We might use one deep research cycle to review existing touch typing curricula (like what order keys are taught in QWERTY, proven methods such as home row first, etc.). Perhaps consult educational research on typing for blind learners (maybe one exists from APH or Perkins). Use AI to generate a bank of practice sentences that are phonetically diverse and relevant. This is not heavy on algorithm research, more on content quality.  
  *Complexity/Debugging:* **Low/Medium.** Content itself doesn’t “bug out” in the software sense, but ensuring it’s effective is key. We will need user testing to adjust difficulty. The main debugging is ensuring the system properly loads and transitions between lessons according to the curriculum definitions (state machine bugs, etc.). That’s manageable.

- **4. Adaptive Learning Algorithm** – *Priority: Medium-High (after basic version works)*  
  *Description:* The AI that adjusts difficulty and practice based on user performance. Initially maybe rule-based (if error rate > 20%, repeat lesson), eventually ML-driven (pattern recognition).  
  *Research Allocation:* **🔎🔎🔎🔎 (4 tokens)** – This is a core differentiator, so we allocate significant deep research. We’d research:
    - Optimal adaptive learning techniques (perhaps look at how Kahn Academy or language learning apps do it).
    - Possibly use a reinforcement learning or Bayesian model to decide when a user is ready to progress. An AI research token might go into prototyping a model that predicts “mastery” of a key.
    - If using GPT to analyze mistakes, allocate a token to prompt engineering: e.g., feeding it the sequence of user inputs and asking it to suggest which keys to focus on.
    - Research also includes user modeling: how to keep a profile of user strengths/weaknesses. We might dedicate one token to reading academic papers on personalized learning for keyboard or similar.  
  *Complexity/Debugging:* **High.** This feature can get complex, as it introduces a lot of condition branches and data handling. Debugging means verifying the adaptation does what we expect – e.g., does it properly detect patterns? We’ll create simulation tests (feed in a fake user who always messes up certain keys, see if it adapts accordingly). Also need to ensure it doesn’t make the experience erratic (too easy or too hard jumps). The complexity is both in design and in testing the machine learning components. We will likely implement a simpler heuristic first (easier to debug), then gradually hand over to AI suggestions.

- **5. Multi-language & Localization Support** – *Priority: Medium*  
  *Description:* Allow the tutor to teach typing in different languages (different keyboard layouts) and localize the interface and voice prompts to other languages.  
  *Research Allocation:* **🔎🔎 (2 tokens)** – We’d do research on how different keyboard layouts (AZERTY, etc.) should be taught – order of keys might differ. Also research on language-specific considerations (for example, some languages have accented characters, we must handle speaking those). One token for investigating TTS and STT capabilities in target languages (ensuring our chosen APIs have good Spanish, French, etc. voices and recognition). Another token for internationalization framework (how to structure our content so it’s translation-friendly). Possibly use AI to assist in translating content or generating language-specific practice text.  
  *Complexity/Debugging:* **Medium.** Internationalization in code can cause bugs (e.g., text not appearing if locale switch fails, or right-to-left languages messing layout). We’ll need to test each language environment thoroughly. Also if we support switching keyboard layout, the input key codes vs expected letters mapping must be handled – not too hard, but details matter (e.g., on a French keyboard hitting the key labeled ‘A’ actually sends a different scancode). We should plan a robust key mapping module. Debugging that requires physically testing or emulating different keyboard layouts.

- **6. Gamified Modules (Audio Games)** – *Priority: Medium (Enhancement)*  
  *Description:* Develop game-like exercises (e.g., a “typing race” where the user must type words quickly to win, or an audio target practice as described).  
  *Research Allocation:* **🔎🔎 (2 tokens)** – Research audio game design for the blind. There’s a niche community and some existing games (like audio shoot ’em ups). We’d spend tokens on learning what makes audio games fun and how to convey game state via sound. Also maybe research using spatial audio cues (e.g., a sound panning left/right to indicate something). If we incorporate scoring, research psychological aspects of reward systems.  
  *Complexity/Debugging:* **Medium.** Game logic can be complex but contained. The main debugging is making sure the games remain accessible (no unintended need for vision) and that timing loops are correct (e.g., in a fast-paced game, ensure performance on different devices doesn’t lag). There might be interesting edge cases like pausing a game, or what if the user speaks a command during a game unintentionally. We’ll sandbox games so they don’t break the main flow. They are optional modules, so they won’t hold up core launch if issues arise (they can be beta features toggled off if needed).

- **7. User Management & Cloud Sync** – *Priority: Medium*  
  *Description:* Enabling user accounts, saving progress to cloud, and syncing across devices. Also includes the teacher dashboard (for multiple users).  
  *Research Allocation:* **🔎 (1 token)** – Most of this is straightforward use of Supabase (which we already know). Minimal research needed beyond reading Supabase docs thoroughly and perhaps looking into data structures for storing metrics. One token might be spent on security/privacy best practices, ensuring we implement auth securely and consider data encryption. If doing a teacher portal, research compliance with student data regulations (FERPA in US schools, etc.).  
  *Complexity/Debugging:* **Medium.** Integrating auth can introduce edge cases (password resets, offline mode for non-logged in, etc.). Data sync bugs might include duplicate records or conflicts if offline edits are allowed. The teacher dashboard is essentially a separate interface filtering through data – complexity depends on how fancy we get (if just viewing stats, it’s simpler; if real-time monitoring or control, more complex). We will likely phase this: initial just personal accounts, later teacher view. Testing multi-user scenarios and data privacy (ensuring user A can’t see user B’s data) is crucial.

- **8. Accessibility QA & Polishing** – *Priority: Continuous*  
  *Description:* This is not a single feature but an ongoing task: conducting thorough accessibility testing (with screen readers, various assistive tech) and addressing any gaps.  
  *Research Allocation:* **🔎 (1 token)** – We allocate “deep research” for keeping up with accessibility best practices. That means reading updated WCAG guidelines, ARIA techniques, or consulting with accessibility experts. Possibly use an AI to audit our UI code for accessibility issues.  
  *Complexity/Debugging:* **Medium-High.** Debugging accessibility is a bit different: it might be fixing an ARIA label or adjusting focus order – not complex algorithmically, but requires meticulous attention. We will treat any accessibility issue as a high priority bug. For example, if during testing we find that a screen reader announces something incorrectly, that needs fixing. It’s an iterative process with each UI component. Also making sure our one-switch navigation doesn’t trap a user or cause an endless loop is something to carefully test (simulate a user only hitting spacebar and ensure they can do everything, albeit slowly).

- **9. Advanced AI Tutor (“Conversational Mode”)** – *Priority: Low (Future enhancement)*  
  *Description:* A mode where the user can ask the AI questions like “What fingers should I use for this?” or even have a conversational lesson summary (“What did I do wrong today?” – and the AI explains). This leverages the LLM to act like a tutor you can talk to.  
  *Research Allocation:* **🔎🔎🔎 (3 tokens)** – This is a bleeding-edge feature, so heavy research. We’d experiment with prompting GPT-4 to act as a tutor given a transcript of the session. One token on figuring out *how to condense session data for the prompt* (maybe we use a summary), one on *natural language understanding of user questions* (maybe some fine-tuning or prompt library like “if user asks something about typing technique, answer from our knowledge base”), and one on *voice interaction specifics* (maintaining context in a voice conversation, possibly using OpenAI’s conversation mode or multi-turn handling). Also research if any similar conversational tutors exist (for other subjects) to learn from their approach.  
  *Complexity/Debugging:* **High.** This combines many systems: ASR (speech to text) to get the question, the LLM to answer, TTS to speak back. There’s potential for error at each stage (misheard question, or nonsensical AI answer). We need to constrain the AI to be accurate and not give harmful advice. Debugging requires a lot of testing with various questions. We may need to implement a fallback if AI fails (like a static FAQ database as backup). This feature would likely be introduced carefully as a beta with disclaimers.

Each feature/module will be tackled in roughly the above order, though some can be parallel (e.g., content development can happen alongside coding the engine, voice command R&D can parallel basic engine coding). 

The term **“token-based deep research”** implies we will use our access to AI (large context models, web research) in measured chunks to answer key unknowns for each feature. For instance, before implementing the voice command, we might spend one “token” (a dedicated session) with GPT-4 browsing literature on best voice UX practices, and another token to prototype the code using an AI coding assistant. By allocating these tokens, we ensure we do not go in blind on complex features – we first gather insights or even let an AI help structure the solution.

**Debugging Complexity Allocation:** We acknowledge some features will consume more debugging time (e.g., speech-related features, adaptive logic). We allocate our development sprints accordingly:
- The initial Audio Lesson Engine and basic UI gets a large chunk of initial debugging allocation since that must be rock-solid (this is our foundation).
- Voice commands and adaptivity we will release in beta phases, expecting to gather bug reports and iterate.
- We’ll maintain a testing matrix (different OS, browsers, with/without screen readers, etc.) and allocate time to run through it for each major release.

Finally, the roadmap in terms of timeline (as partially outlined in the pitch deck’s slide 8) is:
- **Phase 1 (Months 0-3):** Implement features 1, 2, 3 to deliver a functional MVP for English on web/desktop. Conduct deep research for adaptivity but implement basic logic first. Begin user testing with a small group (maybe from a local blind community or online forum).
- **Phase 2 (Months 4-6):** Implement adaptivity (feature 4) and polish based on feedback. Add more lessons to cover full keyboard. Introduce user accounts (part of 7) if needed for testers. Beta release to broader audience, possibly in partnership with an organization to pilot in a class.
- **Phase 3 (Months 6-9):** Add some gamified exercises (5 and partial 6) to increase engagement. Start mobile packaging. Incorporate localization groundwork (so non-English testers can try Spanish or others by end of this phase).
- **Phase 4 (Months 9-12):** Focus on robust cloud sync and teacher dashboard (rest of 7) for institutional use. Expand language support. Hardening accessibility compliance (8) in preparation for an official 1.0 launch. Maybe start work on the conversational AI tutor (9) in R&D, though that might go into next year’s plan.
- **Beyond:** After core product is stable and widely adopted, roll out the advanced AI tutor as a premium add-on and explore other skill modules.

This feature breakdown and roadmap ensures that we tackle critical needs first (so users get value early), while also setting aside time to delve into complex features with adequate research. By assigning “tokens” of deep research to the hairy problems, we leverage AI and existing knowledge to de-risk our development path. Each completed feature moves us closer to the vision of a comprehensive, intelligent typing tutor that leaves no learner behind.

## 6. Accessibility-First UX/UI Guidelines

DeepType’s UX/UI is governed by a simple rule: **if it’s not accessible, it doesn’t go in the product.** Every interface element and interaction is crafted to be usable by our target users. Here we outline key guidelines and principles that our design and development must follow, ensuring an **accessibility-first** experience:

### Voice-First Interaction Principles
Designing for a voice-first interface means assuming the primary mode of user interaction is through spoken dialogue and audio feedback:
- **Everything is Announced:** The user should never have to guess what’s happening. Whenever the app state changes, a concise announcement is made. For example, when a new lesson starts, it might say, “Lesson 5: Typing words with S and D. Press any key to begin.” When a lesson is completed, it announces the result (“Lesson complete! Accuracy: 90%, Speed: 12 WPM. Great job!”).
- **Conversational Tone:** The voice interactions should feel natural. Instead of robotic commands, we use conversational language. This means using first person (“I will now show you...”) or second person (“You can try that again”) to create a rapport. According to usability research, *voice interfaces should b ([3 Reasons Why It's Time to Talk about Voice UI - Frog Design](https://www.frog.co/designmind/3-reasons-why-its-time-to-talk-about-voice-ui#:~:text=3%20Reasons%20Why%20It%27s%20Time,This))to be effective*. We apply this by giving our TTS prompts some personality (while staying professional and clear).
- **Short Prompts & Confirmations:** Users can’t see a long list of options, so voice prompts should be brief and not overload memory. For example, in a menu, don’t read all options at once if there are many; instead, present them one at a time or in small groups. Use auditory icons if helpful (like a subtle tone indicating more options available). After a voice command is given by the user, the system should confirm it understood (e.g., User: “repeat”, System: “Repeating the instruction: [then repeats].”). This confirmation principle prevents confusion in case the speech recognition misheard something.
- **Tolerance and Recovery:** If the system doesn’t catch a voice command or the user says something unexpected, it should handle it gracefully. Maybe provide a gentle reprompt: “Sorry, I didn’t get that. You can say ‘repeat’ or ‘menu’.” This ensures the user never feels stuck. Designing these flows is critical: every voice prompt in our flowchart has an error handling branch.
- **No Unnecessary Voice Input:** We keep required voice input minimal. While we support voice commands, we don’t force the user to speak at any time if they’re not comfortable. There’s always an alternative like pressing a key or clicking. Voice is offered as a convenience and necessity for some, but *not mandatory*. This principle is inclusive (some blind users are non-verbal or simply shy to speak commands, and some have speech impairments).
- **Environmental Awareness:** Recognize that users might be in noisy environments or around others. We allow the user to use headphones and still operate. For privacy or quiet settings, we might include an option for vibrational feedback or subtle sounds instead of spoken feedback (for example, a user who is deaf-blind might rely on vibrations – although that’s a very niche case, advanced but possible with a Braille display’s vibrate feature or phone vibration on key events). Our UI should consider an option “mute voice output” which then outputs to braille display or just uses beeps for correct/wrong (for those who can’t hear). This is part of being voice-first but not voice-only.

### Single-Button UI Mechanics
We design DeepType such that a user with only one switch or one finger available can still navigate the entire application:
- **Focus Highlight and Scan:** At any given screen (or menu), one element is in focus (virtually). For example, on startup, “Start Lesson” might be the focused item. If the user presses the “Select” button (space bar or a hardware switch), that item activates. If the user does nothing for a moment or presses a different special key (or the same button depending on config), the focus will *move to the next item* and announce it. This is a **sequential scan** mechanism. It’s similar to how many switch-accessibility interfaces work, and also how TV interfaces or old Nokia phones worked. This will be an optional mode if voice commands aren’t used.
- **Timing vs Manual Advance:** Some scanning UIs auto-advance focus after X seconds. We likely prefer manual advance (press to move focus) because timing can be stressful and users might miss the window. Manual gives users full control: e.g., press Tab or a special “next” key to cycle focus, and Space or “select” to activate. We ensure that both the physical keyboard and an on-screen single big button can do these (on touchscreen, perhaps tap = select, long press = next, or vice versa — we have to pick an intuitive mapping).
- **Consistent Layout:** The number of interactive elements at any time is kept minimal to aid scanning. For example, the main menu might have 3 options (Start, Settings, Exit). In a lesson, perhaps only one or two (maybe a “stop” button). By limiting choices, we reduce how much the user must scan through. This is a guideline: keep UI screens simple. If a complex input is needed (like entering an email for signup), we handle it with a special flow that is still single-switch friendly (e.g., an onscreen keyboard that scans through letters group by group, although typing an email might be easier if we allow voice dictation or just let a sighted assistant do that step).
- **Visible Focus Indicator:** For low-vision and sighted support observers, we will have a **high-contrast focus ring or highlight** around the currently focused element. Perhaps a thick yellow outline or a bright glow. This is an ARIA best practice for keyboard navigation – ensure focus is not hidden. We also might enlarge the focused item or put a subtle animation (like pulsing) to catch attention. This helps users who can see a bit to follow along, and it’s also good for any keyboard user.
- **Auditory Focus Indicator:** In addition to the voice reading the focused element (“Settings”), we could have a sound cue when moving focus. For instance, a tick sound each time focus advances, and a slightly different tone when wrapping around back to first item. These auditory cues help users understand the interface structure (like “there were 3 items, I heard 3 ticks and now a wrap-around sound, so I know I’m back at top”). This concept comes from existing screen reader behaviors and auditory UI design.
- **ARIA Roles for Widgets:** We will use ARIA roles to inform assistive tech that our custom scanning UI is a list of menu items. For example, in HTML we might mark the menu container with `role="menubar"` and each option as `role="menuitem"`. This way, if a screen reader is running, it knows to treat them as a menu. Even though we have our own voice, this ensures compatibility. According to MDN, ARIA roles provide semantic meaning so screen reade ([WAI-ARIA Roles - Accessibility | MDN](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles#:~:text=ARIA%20roles%20provide%20semantic%20meaning,yet%20have%20full%20browser%20support))t consistently. We implement roles like `button`, `menuitem`, `slider` (if we had any slider controls) so that any built-in AT recognizes DeepType’s components. Even our custom “focus highlight” that moves could be conveyed via ARIA focus changes (like moving a real invisible focus).
- **No hover dependency, large click targets:** We design for keyboard, not mouse hover. So all interactions must be triggered by focus+activate, never just hover. This is important as many blind users can’t hover. And for motor impaired, hovering might not be possible. We also ensure buttons are large and well spaced (easy to hit with a gaze or a shaky hand if using a switch). This ties into high contrast (next section) – large, distinct buttons.

### High-Contrast Visual Design
While DeepType can be used without looking, we still provide a visual interface that is optimized for low vision and color-blind users:
- **Color and Contrast:** We adhere to at least **WCAG 2.1 AA** contrast ratio (4.5:1 for normal text, 3:1 for large text) and aim for **AAA (7:1)** wherever feasible. The default theme likely will be white text on black background or vice versa, which yields very high contrast. Our palette will be limited to a few colors, used consistently (e.g., one accent color for focus or correct input indicators). We’ll avoid color combinations known to be problematic (like red/green together, since many have red-green color blindness). Any color coding will also have a secondary indicator (like a symbol or text). For example, if we use green text for correct and red for wrong on a visible scoreboard, we’ll also prefix with a “✔” or “✖” symbol so color isn’t the only cue.
- **Font and Size:** All text is in a **clear, sans-serif font** (for readability, e.g., Arial, Verdana, or a specifically accessible font like APHont or Atkinson Hyperlegible). We’ll use a large base font size (at least 18px for body, larger for headings). Users can also customize text size in settings. For dyslexic users, maybe allow a font choice (though our main audience is visually impaired, some low-vision users might also have tracking difficulties, so we consider fonts accordingly).
- **UI Element Design:** Buttons and other controls will have strong outlines and filled shapes to distinguish them. For example, instead of a thin outline checkbox, we might use a big toggleswitch with labels “On/Off”. We’ll ensure focus state of a button is very visually obvious (often default focus indicators are tiny dotted lines – we’ll override that with something bolder).
- **Reduced Clutter:** A sparse design benefits low-vision users who might zoom in. We keep screens uncluttered so that zooming doesn’t hide critical info off-screen. Also, fewer elements means easier high-contrast styling (no need to differentiate many shades). We try to use text and simple icons, avoiding background images or patterns that could reduce contrast or introduce confusion.
- **Dark Mode / Light Mode:** Likely, a dark background with light text is best for many visually impaired (less glare). But some prefer light background. We’ll offer at least these two high-contrast modes out of the box, possibly more (like yellow on black, which some with tunnel vision prefer). The user can choose the scheme that suits their vision. All our color choices will be stored as variables so that switching theme is seamless.
- **Testing:** We will test the interface with common color-blind filters and contrast checkers. Also test on a monochrome setting (imagine someone using a device in high contrast mode where everything is forced black & white – our design should still function). We also consider Windows High Contrast mode (which overrides app colors). Ideally, our app still works if OS forces its palette (which it will if running as a Windows app with high contrast enabled – we should ensure our text doesn’t disappear or something under those conditions).
- **No Reliance on Vision for Key Info:** Even though we make visuals as clear as possible, we still adhere to a rule: anything indicated visually (like “this letter on the on-screen keyboard is highlighted”) is also conveyed via audio. This overlaps with voice-first, but it’s a guideline to ensure equal access. For instance, if a visual user sees a progress bar, a blind user hears a progress percentage spoken. The high-contrast visual is for those who use it, but not mandatory to understand.

### ARIA Roles and Labels for Interface Elements
Using proper **ARIA (Accessible Rich Internet Applications) attributes** is essential to make our custom UI understandable by screen readers and assistive tech:
- **Role Attribution:** Every interactive element gets an appropriate `role`. Buttons will be `<button>` HTML elements or `role="button"` if a custom element. Links use `<a>` or `role="link"`. If we have a custom control (like a toggle or a non-standard widget), we’ll find the closest ARIA role (e.g., `role="switch"` for an on/off toggle, which screen readers announce as a toggle and include state). ARIA roles ensure that assistive tools present and support int ([WAI-ARIA Roles - Accessibility | MDN](https://developer.mozilla.org/en-US/docs/Web/Accessibility/ARIA/Roles#:~:text=ARIA%20roles%20provide%20semantic%20meaning,yet%20have%20full%20browser%20support))consistent way.
- **Labels and Descriptions:** We must provide labels for controls that have no visible text. For example, an icon-only button (if we had one) needs `aria-label="Pause lesson"` so screen readers know what it is. Even if there is visible text, sometimes we may want a clearer screen reader label. For instance, a “Next >” button might be clearer as “Next Lesson” for a screen reader, so we’d do `aria-label="Next Lesson"` on it. We’ll also use `aria-describedby` where longer help text is relevant. For example, an input field for a profile name might have a description like “We use this to greet you in the app”, and we link that via `aria-describedby` so the user can hear it if they navigate for more info.
- **Live Regions:** During lessons, there may be dynamically updating text (like a live WPM speed or an error counter). We will utilize ARIA live regions (`aria-live="polite"` or `assertive"` depending on importance) to announce changes. E.g., if we display “Errors: 3”, each time it increments we might announce “Mistake count: 3”. Using a live region allows screen readers to automatically read changes without losing focus context. We’d likely set it to “polite” so it waits until the user is not in the middle of something to announce.
- **Focus Management:** ARIA alone is not enough; we also manage keyboard focus. After certain actions, we might need to programmatically move focus to a logical place. For instance, after closing a modal dialog (like a help popup), return focus to the element that opened it. We’ll follow WAI-ARIA authoring practices for dialogs, menus, etc., which specify where focus should go and using `aria-modal`, `aria-expanded` on toggles, etc. If we make a custom dropdown or pop-up, we ensure to trap focus within it while open and restore on close (these details matter for screen reader and keyboard-only users).
- **Testing with Screen Readers:** We will test with NVDA, JAWS, and VoiceOver to ensure our ARIA labels make sense in the actual announcement. Sometimes what we think is clear might be verbose or awkward when spoken. We might fine-tune labels accordingly. For example, if a screen reader already says “Button”, we don’t need to include the word “button” in our label (to avoid “Start button button”). We’ll use accessible name computation rules to get optimal output.

### Alt Text and Media Descriptions
While DeepType is not image-heavy, any non-text content will have a text equivalent:
- **Logo and Branding:** Our logo on the app (if present) will have `alt="DeepType"` or `aria-label="DeepType logo"` if it’s decorative we might mark it `alt=""` (null alt) to skip it for screen readers. But likely we want the name read.
- **Illustrations:** If there are any illustrations (maybe on a welcome screen or documentation), they will have descriptive alt text. E.g., `alt="A person typing on a keyboard with eyes closed, representing touch typing confidence."` Something that conveys the idea, unless purely decorative in which case null alt.
- **Charts/Graphs:** Not likely in the user interface, but if, say, a teacher dashboard has a progress chart, we’ll provide a summary (e.g., an ARIA live region summary: “Student’s speed increased from 10 to 20 WPM last week”). If we had to include a graph in a report, we’d ensure a textual table or summary is available.
- **Audio Descriptions:** Since our primary output is audio, not visual video, we might not have videos requiring captions. But if there were any instructional videos, we’d need captions and audio descriptions. However, our content is mostly self-voicing interactive, so that covers it.
- **Iconography:** All icons used in buttons will have proper labels. If an icon is purely decorative (like a decorative flourish), we mark it hidden from assistive tech (`aria-hidden="true"` or CSS).
- **ARIA for Non-Text Elements:** If we use a canvas or custom element (imagine a game visualizer or something), we’d use `role="img"` with an `aria-label` describing it, or provide a textual alternative adjacent. But likely, we’ll avoid complex graphics.

### Additional Accessible UX Considerations
- **Tab Order & Logical Navigation:** The tab order (focus order) of elements in the DOM will match the logical order we read them in voice. This prevents confusion for those using keyboard nav. We ensure modals and popups appear logically after triggers in DOM or use appropriate `aria-*` to inform AT.
- **ARIA Alerts for Important Events:** If something critical happens (like connection lost or an error), we use `role="alert"` region to immediately notify the user via screen reader. E.g., if internet goes out and cloud sync fails, an alert might say “Warning: offline, progress will save locally.”
- **No Flashing / Seizure Risks:** We avoid any flashing visuals that could trigger seizures (WCAG guideline to not flash >3 times/sec in high intensity). Our interface is mostly static with subtle transitions, so likely fine.
- **Keyboard Shortcuts:** For advanced users (especially sighted power users or those who prefer keyboard shortcuts to voice), we might implement shortcuts (like Press L to go to Lesson menu, P to pause, etc.). If we do, we’ll ensure to document them and add `aria-keyshortcuts` attribute so screen readers can announce them. This is a nice-to-have that can speed up use without conflicting with normal typing (maybe only active when in menus, not during typing practice to avoid catching normal typing as shortcuts).
- **Form Inputs:** If we have any forms (sign up, etc.), each input has a `<label>` or `aria-label`. Error messages on forms are linked via `aria-describedby` and we use ARIA `role="alert"` on them so they’re announced when appearing.
- **Testing with Diverse Users:** Ultimately, guidelines are validated by user testing. We plan to test the UI with blind users (using screen readers), low-vision users (maybe those with partial sight), and possibly users with cognitive disabilities for simplicity feedback. Their feedback will inform tweaks to our UI wording, timing, etc. For example, an ADHD user might want the option to turn off voice chit-chat and get more direct instructions (so maybe a “concise mode” vs “friendly mode”). We can accommodate that.

By following these guidelines, we ensure DeepType’s UI is **inclusive and user-friendly** for our entire audience. Adhering to ARIA and accessibility standards isn’t just about compliance; it’s about creating a smoother experience. As one accessibility expert mantra says: *“Build it right for the extreme users, and it will work even better for everyone.”* We believe DeepType’s accessible design will not only empower blind users but also result in a generally well-designed product that anyone could use (for instance, a sighted person might appreciate the voice feedback when they switch to another window but still practice typing by listening). 

All these considerations will be documented in our design system, and every developer on the project will be versed in using them. Accessibility is not a separate module but a thread running through every feature – from color choices to code structure with ARIA roles. This guarantees that **when DeepType is launched, it sets a benchmark for accessible educational software**.

## 7. Full Codebase

Below is an outline of the DeepType codebase structure and the key files with their content. The code is heavily commented to explain functionality, following our style guidelines (including ASCII art headers for each file, and console logs for clarity during debugging and learning). This should serve as both the actual code and a learning aid for developers (including those who are visually impaired or have ADHD, as requested, through clear comments and structured sections).

**Repository Structure:**

```plaintext
DeepType/
├── README.md
├── package.json
├── public/
│   └── index.html        # Main HTML file
├── src/
│   ├── app.js            # Main application logic (front-end)
│   ├── styles.css        # Global styles (high contrast, etc.)
│   ├── lessons.js        # Lesson content and curriculum definitions
│   ├── voice.js          # Voice input/output module
│   ├── adaptive.js       # Adaptive learning logic
│   ├── ui.js             # UI rendering and navigation (menus, focus management)
│   ├── storage.js        # Data storage and Supabase integration
│   └── assets/           # (if any audio/image assets)
├── desktop/
│   └── tauri.conf.json   # Config for Tauri desktop app (if applicable)
└── server/
    └── server.js         # Backend server (for API calls to OpenAI, if used)
```

*(Note: Some of these files/modules could be combined in implementation, but are separated here for clarity of roles.)*

---

Now, we’ll present the content of key files with explanatory comments and ASCII art headers.

### File: `public/index.html`

This is the entry HTML page that loads the app. It sets up basic structure and accessibility attributes (lang, meta).

```html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DeepType – AI Typing Tutor</title>
  <!-- High contrast dark theme by default -->
  <link rel="stylesheet" href="styles.css" />
</head>
<body>
  <!-- Main application container -->
  <div id="app" role="application" aria-label="DeepType application">
    <!-- We will dynamically inject content here via app.js -->
  </div>

  <!-- Including the main script -->
  <script src="app.js" type="module"></script>
</body>
</html>
```

**Notes:**  
- The `role="application"` on the #app container tells screen readers that this is a complex web app, potentially adjusting how keyboard events are handled (for some SR, it might switch off virtual cursor mode when focused inside, expecting the app to manage focus).
- We use `type="module"` for app.js, meaning we can use ES6 imports (assuming a bundler or modern browser environment).
- The app content will be rendered dynamically by our JS (to allow easy updates of content as user navigates).
- We set `lang="en"`; if we support multiple languages, we might adjust that attribute dynamically or have separate pages per locale.

### File: `src/app.js`

This is the main application script that ties together modules, handles initial load and global events.

```javascript
/***********************************************
 *  _____                ______               
 * |  __ \               |  ___|              
 * | |  | |_   _  ___ ___| |_ _ __ ___  ______ 
 * | |  | | | | |/ __/ _ \  _| '__/ _ \|______|
 * | |__| | |_| | (_|  __/ | | | | (_) |      
 * |_____/ \__,_|\___\___\_| |_|  \___/       
 *                                           
 * File: app.js
 * Description: Main entry point for DeepType front-end.
 * Initializes the application, handles navigation flow,
 * and ties together UI, voice, lessons, and adaptivity.
 ***********************************************/

import * as UI from './ui.js';
import * as Voice from './voice.js';
import * as Lessons from './lessons.js';
import * as Adaptive from './adaptive.js';
import * as Storage from './storage.js';

// Global state
let currentLesson = null;
let userProfile = null;
let singleSwitchMode = false;

// Initialize application
document.addEventListener('DOMContentLoaded', () => {
  console.log("DeepType app initializing...");
  Storage.init(); // initialize storage (e.g., Supabase or local)
  userProfile = Storage.loadProfile(); // attempt to load user profile if exists

  // Determine if single switch mode should be default (could be from profile or query param)
  singleSwitchMode = userProfile?.preferences?.singleSwitch || false;

  // Setup voice output system
  Voice.initTTS();
  // Attempt to init voice recognition (will check for support)
  Voice.initSTT(onVoiceCommand);

  // Render the main menu
  UI.renderMainMenu(singleSwitchMode);
  Voice.speak("Welcome to DeepType. Press Enter or say 'start' to begin your first lesson.");

  // Setup global keyboard handlers
  setupKeyboardControls();
});

// Handle voice commands globally
function onVoiceCommand(command) {
  console.log("Voice command heard:", command);
  // Normalize command text
  if (!command) return;
  command = command.toLowerCase();
  if (command.includes('start')) {
    UI.startLessonFlow();
  } else if (command.includes('repeat')) {
    UI.repeatCurrentPrompt();
  } else if (command.includes('menu') || command.includes('exit')) {
    UI.renderMainMenu(singleSwitchMode);
  } else if (command.includes('next')) {
    // This could be used to skip or simulate pressing the "next" button in single-switch scanning
    UI.focusNextOption();
  } else if (command.includes('help')) {
    UI.showHelp();
  } else {
    Voice.speak("Sorry, I didn't catch that. Try saying 'repeat' or 'menu'.");
  }
}

// Setup keyboard controls for single-switch and general navigation
function setupKeyboardControls() {
  document.body.addEventListener('keydown', (e) => {
    if (UI.isInputActive()) {
      // If the user is currently typing in a lesson input field, don't intercept normal typing
      return;
    }
    // Keyboard shortcuts and navigation
    if (e.key === ' ' && singleSwitchMode) {
      // Space in single-switch mode: select or progress
      if (!UI.handleSwitchSelect()) {
        UI.focusNextOption();
      }
      e.preventDefault();
    } else if (e.key === 'ArrowRight' || e.key === 'ArrowDown') {
      UI.focusNextOption();
      e.preventDefault();
    } else if (e.key === 'ArrowLeft' || e.key === 'ArrowUp') {
      UI.focusPreviousOption();
      e.preventDefault();
    } else if (e.key === 'Enter') {
      // Enter triggers default action (like activating focused button)
      UI.activateFocusedOption();
      e.preventDefault();
    } else if (e.key === 'Escape') {
      UI.renderMainMenu(singleSwitchMode);
      e.preventDefault();
    } else if (e.key === 'F1') {
      // F1 for help
      UI.showHelp();
      e.preventDefault();
    }
    // (We could add more shortcuts if needed)
  });
}
```

**Highlights & Explanation:**  
- We import modules for UI, Voice, Lessons, Adaptive logic, and Storage (Supabase or local).
- We keep a global `singleSwitchMode` flag. This can be set from user preferences.
- On DOMContentLoaded, we initialize storage (which might setup Supabase or retrieve from localStorage if offline).
- `Voice.initTTS()` might load voices, etc., and `Voice.initSTT(onVoiceCommand)` will set up speech recognition and call `onVoiceCommand` when a phrase is recognized.
- We immediately render main menu via `UI.renderMainMenu` and prompt user with a welcome voice. (We assume UI module manipulates DOM to show some menu).
- We attach a global keydown listener. If an input is active (meaning user is typing in a practice exercise, which we detect via UI state), we let keys through (so the user can type letters normally).
- We define keyboard navigation: spacebar triggers either select or next in single-switch mode. We check `UI.handleSwitchSelect()` – maybe it returns true if it selected an item, false if it just opened a menu, then we call focusNext for scanning.
- Arrow keys allow navigation too (for users who have full keyboard).
- Enter triggers activation of a focused element (like pressing a button).
- Escape brings back main menu (like a way to cancel or exit current context).
- F1 triggers a help overlay.
- `onVoiceCommand` normalizes recognized text and routes commands. We look for keywords like 'start', 'repeat', 'menu', etc. 
  - For 'start', we call `UI.startLessonFlow()` – that presumably picks the appropriate lesson and transitions to lesson interface.
  - 'repeat' triggers UI to repeat current prompt (like if user says repeat during a lesson, it re-voices the instruction).
  - 'menu' or 'exit' goes back to main menu.
  - 'next' might simulate focus next for scanning or skip something.
  - 'help' shows help.
  - If command is unrecognized, we ask again.
- This design allows adding more voice commands easily by extending that if-else (or changing to a mapping).

We included lots of `console.log` statements for debugging (to track commands etc.). Those logs are helpful in development or if a coder wants to see what’s happening (especially for an ADHD coder who might benefit from stepping through logs to maintain focus).

ASCII art at the top identifies the file and provides a quick summary.

### File: `src/ui.js`

This module manages the DOM updates and navigation. It uses ARIA roles and keeps track of focus for keyboard/switch control.

```javascript
/***********************************************
 *  _    _ _ 
 * | |  | (_)
 * | |  | |_ ___  ___ 
 * | |  | | / __|/ __|
 * | |__| | \__ \ (__ 
 *  \____/|_|___/\___|
 *                    
 * File: ui.js
 * Description: Handles User Interface rendering and navigation.
 * Manages menus, focus (for single-switch), and lesson display.
 ***********************************************/

// Import other needed modules (assuming circular deps resolved by careful use)
import { speak } from './voice.js';
import { currentLesson } from './app.js';  // If needed, or we pass state into functions

// Track focused element index for menus
let focusIndex = 0;
let currentMenuOptions = [];

// Helper: remove existing content
function clearAppContainer() {
  const app = document.getElementById('app');
  app.innerHTML = '';
  focusIndex = 0;
  currentMenuOptions = [];
}

// Render main menu
export function renderMainMenu(singleSwitchMode) {
  clearAppContainer();
  const app = document.getElementById('app');

  // Create title
  const title = document.createElement('h1');
  title.innerText = 'DeepType';
  title.className = 'title';
  app.appendChild(title);

  // Menu container with role=menu
  const menu = document.createElement('div');
  menu.setAttribute('role', 'menu');
  menu.id = 'mainMenu';
  app.appendChild(menu);

  // Define menu options
  const options = [
    { text: 'Start Lesson', action: startLessonFlow },
    { text: 'Settings', action: openSettings },
    { text: 'Help', action: showHelp },
    { text: 'Exit', action: exitApp }
  ];
  currentMenuOptions = options;

  options.forEach((opt, index) => {
    const btn = document.createElement('button');
    btn.innerText = opt.text;
    btn.className = 'menuItem';
    btn.setAttribute('role', 'menuitem');
    btn.setAttribute('tabindex', index === 0 ? '0' : '-1'); // Only first is tab-able initially
    // When focused or clicked, call the action
    btn.addEventListener('click', opt.action);
    btn.addEventListener('focus', () => { focusIndex = index; });
    menu.appendChild(btn);
  });

  // Set initial focus
  const firstButton = menu.querySelector('button');
  if (firstButton) firstButton.focus();

  console.log("Main menu rendered. Options:", options.map(o => o.text));
  speak("Main menu. Options: Start Lesson, Settings, Help, Exit.");
}

// Called when user activates "Start Lesson"
export function startLessonFlow() {
  console.log("Starting lesson flow...");
  // Determine which lesson to start: if userProfile exists and has progress, pick next; else Lesson 1.
  const lessonToStart = /* logic to pick lesson, e.g., Lessons.getNextLesson(userProfile.progress) */ Lessons.getLesson(0);
  if (lessonToStart) {
    currentLesson = lessonToStart;
    renderLesson(currentLesson);
  } else {
    speak("No lesson available.");
  }
}

// Render a lesson interface
export function renderLesson(lesson) {
  clearAppContainer();
  const app = document.getElementById('app');
  
  // Create heading for lesson (for low-vision users)
  const lh = document.createElement('h2');
  lh.innerText = lesson.title;
  lh.className = 'lessonTitle';
  app.appendChild(lh);

  // Create a prompt area
  const prompt = document.createElement('div');
  prompt.id = 'prompt';
  prompt.setAttribute('aria-live', 'polite');  // so updates are read
  prompt.innerText = '';  // will be filled
  app.appendChild(prompt);

  // Create an input area if needed (hidden text input to capture typing)
  const hiddenInput = document.createElement('input');
  hiddenInput.type = 'text';
  hiddenInput.id = 'typingInput';
  hiddenInput.setAttribute('aria-label', 'Typing input'); // label for SR
  hiddenInput.style.position = 'absolute';
  hiddenInput.style.opacity = '0';
  app.appendChild(hiddenInput);
  hiddenInput.focus();  // focus so keystrokes go here

  // Listen for keystrokes in the lesson input
  hiddenInput.addEventListener('input', onUserType);

  // Kick off the first prompt of the lesson
  speak(lesson.intro);  // e.g., "Lesson 1. Place your fingers on home row..."
  setTimeout(() => {
    presentNextPrompt(); 
  }, 1000);
}

// Keep track of current step within lesson
let currentStepIndex = 0;
function presentNextPrompt() {
  const lesson = currentLesson;
  if (!lesson) return;
  const promptEl = document.getElementById('prompt');
  const step = lesson.steps[currentStepIndex];
  if (!step) {
    lessonComplete();
    return;
  }
  // Show the prompt text (if any) in large font for those who can see
  promptEl.innerText = step.promptText || '';
  // Speak the prompt (if it's a letter, maybe speak differently vs word)
  const toSpeak = step.audioPrompt || step.promptText;
  speak(toSpeak);
  // Expect the user's input (the onUserType handler will check it)
}

// Handle user typing in lesson
function onUserType(e) {
  const input = e.target;
  const lesson = currentLesson;
  const step = lesson.steps[currentStepIndex];
  const expected = step.expectedInput;  // e.g., 'F' or 'cat'
  const userInput = input.value;
  // For single-character inputs, we check immediately.
  if (step.type === 'char') {
    const char = userInput.slice(-1);  // last typed char
    if (!char) return;
    if (char.toLowerCase() === expected.toLowerCase()) {
      console.log(`Correct key pressed: ${char}`);
      speak("Correct");  // positive feedback
      currentStepIndex++;
      input.value = '';
      setTimeout(presentNextPrompt, 500);
    } else {
      console.log(`Incorrect key. Expected ${expected}, got ${char}`);
      speak(`Oops, that was ${char}. Try again.`);
      input.value = '';
      // (We don't advance stepIndex, user will try again same prompt)
    }
  } else if (step.type === 'word') {
    // If expecting a word, we might wait for space or enter
    if (userInput.endsWith(' ')) {
      const attempt = userInput.trim();
      if (attempt.toLowerCase() === expected.toLowerCase()) {
        console.log(`Correct word typed: ${attempt}`);
        speak("Good job, that was correct.");
        currentStepIndex++;
      } else {
        console.log(`Incorrect word. Expected ${expected}, got ${attempt}`);
        speak(`You typed ${attempt}, but the word was ${expected}. We'll practice it again.`);
        // maybe repeat same step or push it later for practice
        Adaptive.recordMistake(expected);
      }
      input.value = '';
      setTimeout(presentNextPrompt, 500);
    }
  }
  // else: other types (sentences etc.) similar approach
}

// Repeat current prompt (for voice command or button)
export function repeatCurrentPrompt() {
  const lesson = currentLesson;
  if (!lesson) return;
  const step = lesson.steps[currentStepIndex];
  if (step) {
    const toSpeak = step.audioPrompt || step.promptText;
    speak(`I said: ${toSpeak}`);
  }
}

// Mark lesson as complete
function lessonComplete() {
  speak("Lesson complete! Great work.");
  // Ideally record progress
  Storage.saveProgress(currentLesson.id);
  // Then return to menu or next lesson
  currentLesson = null;
  setTimeout(() => {
    renderMainMenu(false);
  }, 2000);
}

// Navigation helpers
export function focusNextOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items.length === 0) return;
  focusIndex = (focusIndex + 1) % items.length;
  items[focusIndex].focus();
  speak(items[focusIndex].innerText);  // announce the newly focused option
}

export function focusPreviousOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items.length === 0) return;
  focusIndex = (focusIndex - 1 + items.length) % items.length;
  items[focusIndex].focus();
  speak(items[focusIndex].innerText);
}

export function activateFocusedOption() {
  const menu = document.getElementById('mainMenu');
  if (!menu) return;
  const items = menu.querySelectorAll('button');
  if (items[focusIndex]) {
    items[focusIndex].click();
  }
}

// For single switch: 
// handleSwitchSelect returns true if an action was taken (like a menu item activated)
export function handleSwitchSelect() {
  const menu = document.getElementById('mainMenu');
  if (menu) {
    // if menu open, Space acts as Enter on the focused option
    activateFocusedOption();
    return true;
  }
  // If no menu (maybe in lesson), we could treat Space as some other action, but by default:
  return false;
}

// Dummy implementations of other menu actions:
export function openSettings() {
  speak("Settings is not implemented yet.");
  // would render settings UI with options (like toggle singleSwitchMode)
}
export function showHelp() {
  speak("Help: You can say commands like start, repeat, menu. During lessons, type the requested keys. Press Escape to return to menu.");
}
export function exitApp() {
  speak("Exiting. Goodbye!");
  console.log("Exiting DeepType app.");
  // In a web app, we might just reload or clear, in a desktop app, possibly close window.
  // For now:
  clearAppContainer();
  const msg = document.createElement('p');
  msg.innerText = "You have exited the app. Refresh to restart.";
  document.getElementById('app').appendChild(msg);
}
```

**Highlights & Explanation:**  
- `renderMainMenu` creates a menu with `role="menu"` and each button as `role="menuitem"`. We manage `tabindex` such that initially only the first item is focusable (to avoid tabbing into others without our control).
- We store `currentMenuOptions` to map index to action.
- We focus the first button and speak the menu options. We call `speak()` with a summary of options.
- For each button, we attach focus and click events. On focus, we update `focusIndex` (so we know which index is focused for arrow key logic).
- The `startLessonFlow` function gets a lesson (from Lessons module) and calls `renderLesson`.
- `renderLesson` builds the lesson UI: it shows a title, a prompt area (with `aria-live` so screen reader will read any changes politely).
- It also adds a hidden text input to capture typing. This input is positioned off-screen (opacity 0, could also use `left: -999px` etc.) to not distract sighted users, but it's needed to capture key events reliably. We label it for SR as "Typing input" (though the SR user will primarily rely on our voice, but if they use their own SR, it might announce when they type).
- We immediately focus this hidden input so that all keypresses go there without user needing to click anything (they just start typing).
- We attach `onUserType` to its `input` event to handle typing.
- We then start the lesson by speaking an intro and calling `presentNextPrompt()`.
- `presentNextPrompt` sets the prompt text (for low-vision users to see) and speaks it. It uses `lesson.steps` array with each step having `promptText`, `audioPrompt`, `expectedInput`, etc. If no more steps, calls `lessonComplete()`.
- `onUserType` logic: if expecting a character:
  - It grabs the last typed char from input (since we keep input field value).
  - If correct, give feedback and move to next step.
  - If wrong, clear input, speak feedback, and stay on same step (so user can try again). Also maybe record mistake for adaptivity.
- If expecting a word:
  - We wait until user types space (meaning they finished the word). Then check correctness, give appropriate response, and move on or repeat.
  - Note: This simplistic approach uses space as terminator. Alternatively, we might have them press Enter to submit a word. But space is used to separate words if typing a sentence, so this logic might need refinement to handle multi-word inputs. For now, single-word training is fine.
- The code calls `Adaptive.recordMistake(expected)` when a word is mistyped – presumably to log that for later practice. (We haven’t written adaptive.js yet, but it would provide such function.)
- `repeatCurrentPrompt` will re-speak the current prompt with a prefix "I said: ...".
- `lessonComplete` speaks completion, saves progress via Storage, and returns to main menu after a short delay.
- Navigation helpers:
  - `focusNextOption` and `focusPreviousOption` move the focusIndex and focus the corresponding button in the menu. They also call `speak()` to announce the option text (so a blind user knows which option is now focused).
  - `activateFocusedOption` simulates pressing enter on the current focus by triggering the button’s click event.
  - `handleSwitchSelect` ties into our app-level key handler: if menu open, space triggers activation (and returns true meaning it consumed the press). If no menu, maybe in a lesson context, we could have other logic (not implemented here).
- Basic `openSettings`, `showHelp`, `exitApp` are stubs for now. `showHelp` voices some instructions. Ideally, it would present a nice overlay with all commands (with an accessible format).
- We included console logs to track what's being rendered, key correctness, etc., which is useful for debugging and also if a developer runs this in dev tools, they can see these logs for understanding flow.

We ensure important interactive elements have ARIA roles and labels:
- The prompt has `aria-live="polite"` to announce changes.
- The hidden input has a label (though if using their own screen reader, they might not need it since they won't focus it consciously).
- Buttons have roles and text (text is visible so acts as label as well).
- If we had any icon-only or dynamic text, we'd label them.

### File: `src/voice.js`

Manages text-to-speech (TTS) and speech-to-text (STT). We abstract these so we can swap between Web Speech API or other backends. For simplicity, we’ll try Web Speech API and fallback to none if unsupported.

```javascript
/***********************************************
 * __     ___    ____ _____  _    ____ ___ 
 * \ \   / / |  |  _ \_   _|/ \  / ___|_ _|
 *  \ \ / /| |  | |_) || | / _ \| |    | | 
 *   \ V / | |__|  __/ | |/ ___ \ |___ | | 
 *    \_/  |_____|_|    |_/_/   \_\____|___|
 *                                         
 * File: voice.js
 * Description: Voice input (STT) and output (TTS) management.
 * Provides speak() for TTS and sets up speech recognition for voice commands.
 ***********************************************/

// Text-to-Speech (TTS)
let synth;
let voice;
export function initTTS() {
  synth = window.speechSynthesis;
  if (!synth) {
    console.warn("TTS not supported in this browser.");
    return;
  }
  // Optional: choose a specific voice (e.g., a female English voice)
  const voices = synth.getVoices();
  // find an English voice
  voice = voices.find(v => v.lang.startsWith('en') && v.name.includes('Google')) || voices[0];
  console.log("TTS initialized. Using voice:", voice ? voice.name : 'default');
}

export function speak(text) {
  if (!window.speechSynthesis) {
    return; // no TTS available
  }
  if (synth.speaking) {
    synth.cancel(); // stop current speech if any (to avoid overlap)
  }
  const utter = new SpeechSynthesisUtterance(text);
  utter.rate = 0.9; // slightly slower for clarity
  utter.pitch = 1; 
  utter.volume = 1;
  if (voice) utter.voice = voice;
  synth.speak(utter);
}

// Speech-to-Text (STT) for voice commands
let recognition;
export function initSTT(commandCallback) {
  if (!('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
    console.warn("Speech Recognition not supported in this browser.");
    return;
  }
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  recognition = new SpeechRecognition();
  recognition.lang = 'en-US';
  recognition.continuous = true; // continuous listening
  recognition.interimResults = false;
  recognition.maxAlternatives = 1;
  recognition.onresult = (event) => {
    const transcript = event.results[event.results.length - 1][0].transcript.trim();
    console.log("STT result:", transcript);
    commandCallback(transcript);
  };
  recognition.onerror = (event) => {
    console.error("Speech Recognition error:", event.error);
    // If continuous listening stops due to error, attempt restart
    if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
      console.warn("Microphone access denied or STT not allowed.");
      speak("Voice commands are not available.");
    } else {
      // try to restart recognition on network or other errors
      try { recognition.start(); } catch(e) {}
    }
  };
  recognition.onend = () => {
    console.log("STT onend fired, restarting...");
    // automatically restart listening unless intentionally stopped
    try { recognition.start(); } catch(e) {}
  };

  try {
    recognition.start();
    console.log("Speech recognition started.");
    // Provide an audible cue that voice commands are active (if desired)
  } catch (e) {
    console.error("Speech recognition couldn't start:", e);
  }
}
```

**Highlights & Explanation:**  
- `initTTS()` sets up speechSynthesis, picks a voice. `getVoices()` might be empty initially until onvoiceschanged event fires; in real code we might wait or call it after a slight timeout. But for brevity, we take what's available.
- `speak(text)` uses the Web Speech API TTS to speak. It cancels any ongoing speech to avoid overlap (so if multiple calls happen quickly, we speak the latest).
- Rate is set to 0.9 (slightly slower than default 1.0) for clarity. This might be user-adjustable later.
- `initSTT(commandCallback)` uses webkitSpeechRecognition if available. It's experimental but works in Chrome.
- We set continuous listening with no interim results (we only care about final commands).
- On result, we extract the transcript and pass it to the callback provided (which in app.js is `onVoiceCommand`).
- On error: 
  - If not allowed, we notify user (lack of mic permission).
  - If other errors (like network), we attempt to restart `recognition.start()`.
- On end: we auto-restart to keep listening (unless it was purposely stopped). This ensures continuous voice command listening.
- We wrap `recognition.start()` in try-catch as it can throw if called at bad times or user hasn't interacted (Chrome might require user gesture to start audio capture).
- We log starting, and maybe would beep or speak to indicate voice command readiness. (For now, just log).
- We keep `recognition` global so we could implement a stop function if needed (not shown here, but if user pauses voice commands).
- This design ensures the app is always listening for voice commands in the background while the user is practicing (which is what we want for "repeat" etc., but caution: the recognizer might pick up the TTS voice. That’s a known challenge. We might need to tune it by e.g. only listening during silent periods or using a push-to-talk style. This code might erroneously transcribe the tutor’s spoken instructions as commands, which is an issue. One approach: temporarily stop recognition when we speak instructions, or use a VAD (voice activity detection) to pause recognition while TTS is active. For brevity, not handled here, but a real implementation should address it. Alternatively, keep commands distinct enough that instructions wouldn’t trigger them.)

- ASCII art spells "VOICE".

### File: `src/lessons.js`

Contains lesson definitions and possibly logic to fetch next lesson.

```javascript
/***********************************************
 *  _                 _   
 * | |   ___  ___ ___| |_ 
 * | |__/ _ \/ __/ __| __|
 * |____\___/\__\__ \ |_ 
 *               |___/\__|
 * 
 * File: lessons.js
 * Description: Defines lesson content and provides accessors.
 * Each lesson contains a title, intro text, and a sequence of steps.
 ***********************************************/

export const lessons = [
  {
    id: 0,
    title: "Lesson 1: Home Row (F and J)",
    intro: "Lesson 1. Let's start with the home row keys F and J. " +
           "Place your left index on F, right index on J. Now, type the letter F when you hear it.",
    steps: [
      { type: 'char', promptText: "Type F", audioPrompt: "Press the F key.", expectedInput: "F" },
      { type: 'char', promptText: "Type J", audioPrompt: "Now press the J key.", expectedInput: "J" },
      { type: 'char', promptText: "Type F", audioPrompt: "Again, press F.", expectedInput: "F" },
      { type: 'char', promptText: "Type J", audioPrompt: "Again, press J.", expectedInput: "J" },
      // ... more practice
      { type: 'word', promptText: "Type 'fj'", audioPrompt: "Now type F J as a two-letter word.", expectedInput: "fj" }
    ]
  },
  {
    id: 1,
    title: "Lesson 2: Home Row (D and K)",
    intro: "Lesson 2. Next, we'll add D and K.",
    steps: [
      // Similar structure...
    ]
  }
  // Additional lessons...
];

// Function to get a lesson by id or index
export function getLesson(id) {
  return lessons.find(lsn => lsn.id === id) || null;
}

// Placeholder: pick next lesson for a user given their progress
export function getNextLesson(progress) {
  // If progress is an array of completed lesson IDs:
  for (let lsn of lessons) {
    if (!progress || !progress.includes(lsn.id)) {
      return lsn;
    }
  }
  return null;
}
```

**Highlights & Explanation:**  
- We define a simple array of lesson objects. Each has:
  - `id`, `title`, `intro` (what to speak at start), and `steps`.
  - Each step has `type` (could be 'char', 'word', 'sentence', etc.), `promptText` for display, `audioPrompt` for what voice says (if we want a different wording from what’s displayed), and `expectedInput`.
- In lesson1, we practice F and J individually and then as a pair.
- `getLesson(id)` returns the lesson object (to start a specific lesson).
- `getNextLesson(progress)` would normally use stored progress to find the next uncompleted lesson. Here, we just loop through lessons and return the first not in completed progress array.
- For progress tracking, `Storage.saveProgress` would likely push completed ID into some array and save for user.
- More lessons can be added following this structure.
- We might later use adaptivity to alter steps or insert new steps, but base curriculum is here.

### File: `src/adaptive.js`

Handles adaptive logic (for now maybe just collects mistakes, could later adjust lesson order).

```javascript
/***********************************************
 *      _            _   _       
 *     / \   ___ ___| |_(_)_ __  
 *    / _ \ / __/ __| __| | '_ \ 
 *   / ___ \ (__\__ \ |_| | |_) |
 *  /_/   \_\___|___/\__|_| .__/ 
 *                        |_|    
 * File: adaptive.js
 * Description: Adaptive learning utilities.
 * Records mistakes and can modify or suggest extra practice.
 ***********************************************/

// Simple structure to record mistakes frequency
const mistakeCount = {};

// Record a mistake for a particular key or word
export function recordMistake(item) {
  mistakeCount[item] = (mistakeCount[item] || 0) + 1;
  console.log(`Adaptive: recorded mistake for "${item}". Total mistakes: ${mistakeCount[item]}`);
  // (We could decide to dynamically insert a review step for this item)
}

// Suggest an extra practice step if certain threshold reached
export function maybeInjectPractice(currentLesson) {
  // For example, if any key has >3 mistakes, inject a practice step after current one:
  for (let item in mistakeCount) {
    if (mistakeCount[item] >= 3) {
      // Find if not already in current lesson steps soon:
      const practiceStep = {
        type: item.length === 1 ? 'char' : 'word',
        promptText: `Type ${item}`,
        audioPrompt: `Let's practice ${item}`,
        expectedInput: item
      };
      console.log(`Adaptive: injecting extra practice for "${item}"`);
      // Insert practice step after current index (assuming we have access to the index here or handle differently).
      // This is a simplistic approach; a more robust system might queue practice for next lesson.
      return practiceStep;
    }
  }
  return null;
}
```

**Highlights & Explanation:**  
- We use a simple dictionary `mistakeCount` to count errors per item (could be letter or word).
- `recordMistake(item)` increments count and logs it.
- `maybeInjectPractice(currentLesson)` is a hook that could be called at certain times (e.g., after finishing a lesson or between steps) to decide if we should insert an extra step.
  - Here, if any item has 3 or more mistakes, we prepare a practice step focusing on it.
  - This injection logic is simplistic: ideally, we might insert into the lesson flow. But doing that on the fly could complicate `presentNextPrompt`. Perhaps easier: schedule it for next lesson, or just alert the user.
  - But the code indicates returning a practiceStep, which the calling code (if integrated in onUserType maybe) could handle by inserting into lesson.steps or something.
- Right now, nothing calls `maybeInjectPractice` in our UI. We might integrate it at lessonComplete or somewhere if needed.
- The adaptivity now is very minimal. In a more complex version, we might adjust difficulty or skip ahead if user is doing well.

### File: `src/storage.js`

Manages saving/loading from localStorage or Supabase (depending on environment). For simplicity, a local stub is shown:

```javascript
/***********************************************
 *   ____ _______    _    ____ _____ ___ ___  
 *  / ___|_   _\ \  / \  / ___|_   _|_ _/ _ \ 
 * | |     | |  \ \/ _ \| |     | |  | | | | |
 * | |___  | |   | | (_) | |___  | |  | | |_| |
 *  \____| |_|   |_|\___/ \____| |_| |___\___/ 
 *                                            
 * File: storage.js
 * Description: Handles saving/loading user data.
 * If online, uses Supabase; otherwise falls back to localStorage.
 ***********************************************/

// Supabase client if needed (assuming we have included supabase script or using @supabase/supabase-js)
let supabase = null;
const SUPABASE_KEY = '<your-supabase-key>';
const SUPABASE_URL = '<your-supabase-url>';

export function init() {
  if (window.supabase) {
    supabase = window.supabase.createClient(SUPABASE_URL, SUPABASE_KEY);
    console.log("Supabase initialized.");
  } else {
    console.log("Supabase not loaded, using local storage only.");
  }
}

// Load user profile from local storage (or supabase if logged in)
export function loadProfile() {
  try {
    const data = localStorage.getItem('deeptype_profile');
    if (data) {
      const profile = JSON.parse(data);
      console.log("Loaded profile from local storage:", profile);
      return profile;
    }
  } catch (e) {
    console.error("Failed to load profile:", e);
  }
  return null;
}

// Save progress (simple: record lesson completed)
export function saveProgress(lessonId) {
  let profile = loadProfile() || { progress: [] };
  if (!profile.progress.includes(lessonId)) {
    profile.progress.push(lessonId);
  }
  try {
    localStorage.setItem('deeptype_profile', JSON.stringify(profile));
    console.log(`Progress saved. Completed lessons: ${profile.progress}`);
  } catch (e) {
    console.error("Failed to save progress:", e);
  }
  // If using Supabase and logged in, we'd also send update to DB here
}

// Other storage functions like saveSettings, etc., could be added similarly.
```

**Highlights & Explanation:**  
- On `init`, if the Supabase library is available, create a client (with placeholders for URL/KEY which should be configured).
- `loadProfile` tries to get from localStorage and parse. (In a real scenario, if user is logged in with Supabase Auth, we’d fetch from DB).
- `saveProgress` updates a local profile’s progress array and writes it back to localStorage. Also logs it.
- If supabase was connected and user logged, we would call something like `supabase.from('profiles').update({progress: profile.progress}).eq('id', user.id)` etc.
- But offline usage is fine with localStorage.
- We can add other settings similarly (like singleSwitch preference).
- This is simplified for demonstration.

---

With these modules, the codebase provides a full loop:
- App starts -> main menu -> lesson -> interactive typing -> adaptive feedback -> back to menu.
- The code is annotated so that developers can understand what each part does.
- ASCII art headers help a developer locate files quickly (especially in a printed or linear reading scenario, the big text stands out).
- Console logs and comments provide a narrative of the program flow, which can help someone learning or debugging the system.

**Console Logs & Debugging Enhancements:**  
We have used `console.log` generously to mark major events (initialization, rendering, correctness of input, etc.). In a debug mode, these logs could be turned on to follow what's happening step by step:
- E.g., "Correct key pressed: F" tells us input handling logic worked for that case.
- "Main menu rendered. Options: ..." confirms UI built properly.
- "STT result: repeat" shows what the speech recognizer picked up.

For an educational coding perspective, one could run this in a browser console and watch these logs to see the internal state changes. This is beneficial for developers new to this code or even for advanced users curious about the internals.

We can easily extend or adapt this structure:
- If building with a framework (React), the structure might differ (e.g., using components and state instead of directly manipulating DOM), but the logical separation (UI, voice, data) remains similar.
- We kept it framework-agnostic for clarity.

Finally, note that we ignore image embedding or charts as requested. All output here is purely textual or via spoken text (for the app itself). If we had images, we included alt text approach.

**Conclusion:** The above code and documentation present a complete picture of DeepType’s implementation approach. From the executive vision down to code level, we’ve aligned everything with the goal of an accessible, intelligent typing tutor. The structure can be built upon as we add more features (for instance, integrating the conversational AI in the future might add another module, or hooking up Supabase in earnest once keys are set, etc.). 

Each code file starts with an ASCII art banner to make it fun and to break the monotony (which helps ADHD coders by chunking sections clearly). Screen reader users can skip over the ASCII art (since it’s in a comment, it won’t be read aloud) and get straight to the content; the art is more for visual appeal in code editors.

We have thus provided a **comprehensive dossier** that covers vision, design, strategy, and technical implementation for DeepType by Empathy Labs. By following this plan, the DeepType project is well-positioned to succeed and make a meaningful impact in assistive education, demonstrating how thoughtful use of AI and UX design can create inclusive technology for all.










# Welcome to your Lovable project

## Project info

**URL**: https://lovable.dev/projects/6c141cc3-da58-4115-8f0a-4af77342f723

## How can I edit this code?

There are several ways of editing your application.

**Use Lovable**

Simply visit the [Lovable Project](https://lovable.dev/projects/6c141cc3-da58-4115-8f0a-4af77342f723) and start prompting.

Changes made via Lovable will be committed automatically to this repo.

**Use your preferred IDE**

If you want to work locally using your own IDE, you can clone this repo and push changes. Pushed changes will also be reflected in Lovable.

The only requirement is having Node.js & npm installed - [install with nvm](https://github.com/nvm-sh/nvm#installing-and-updating)

Follow these steps:

```sh
# Step 1: Clone the repository using the project's Git URL.
git clone <YOUR_GIT_URL>

# Step 2: Navigate to the project directory.
cd <YOUR_PROJECT_NAME>

# Step 3: Install the necessary dependencies.
npm i

# Step 4: Start the development server with auto-reloading and an instant preview.
npm run dev
```

**Edit a file directly in GitHub**

- Navigate to the desired file(s).
- Click the "Edit" button (pencil icon) at the top right of the file view.
- Make your changes and commit the changes.

**Use GitHub Codespaces**

- Navigate to the main page of your repository.
- Click on the "Code" button (green button) near the top right.
- Select the "Codespaces" tab.
- Click on "New codespace" to launch a new Codespace environment.
- Edit files directly within the Codespace and commit and push your changes once you're done.

## What technologies are used for this project?

This project is built with .

- Vite
- TypeScript
- React
- shadcn-ui
- Tailwind CSS

## How can I deploy this project?

Simply open [Lovable](https://lovable.dev/projects/6c141cc3-da58-4115-8f0a-4af77342f723) and click on Share -> Publish.

## I want to use a custom domain - is that possible?

We don't support custom domains (yet). If you want to deploy your project under your own domain then we recommend using Netlify. Visit our docs for more details: [Custom domains](https://docs.lovable.dev/tips-tricks/custom-domain/)
</file>

<file path="tailwind.config.ts">
import type { Config } from "tailwindcss";
import { fontFamily } from "tailwindcss/defaultTheme";
export default {
  darkMode: ["class"],
  content: [
    "./pages/**/*.{ts,tsx}",
    "./components/**/*.{ts,tsx}",
    "./app/**/*.{ts,tsx}",
    "./src/**/*.{ts,tsx}",
  ],
  prefix: "",
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      fontFamily: {
        sans: ["Inter var", "Satoshi", ...fontFamily.sans],
        heading: ["Satoshi", "Inter var", ...fontFamily.sans],
        mono: ["Fira Code", ...fontFamily.mono],
      },
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "accordion-down": {
          from: { height: "0" },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: "0" },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
      },
    },
  },
  plugins: [require("tailwindcss-animate"), require("@tailwindcss/typography")],
} satisfies Config;
</file>

<file path="tsconfig.app.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": ["ES2020", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "isolatedModules": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": false,
    "noUnusedLocals": false,
    "noUnusedParameters": false,
    "noImplicitAny": false,
    "noFallthroughCasesInSwitch": false,

    "baseUrl": ".",
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["src"]
}
</file>

<file path="tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "useDefineForClassFields": true,
    "lib": [
      "ES2020",
      "DOM",
      "DOM.Iterable"
    ],
    "module": "ESNext",
    "skipLibCheck": true,
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "noEmit": true,
    "jsx": "react-jsx",
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noFallthroughCasesInSwitch": true,
    "esModuleInterop": true,
    "baseUrl": ".",
    "paths": {
      "@/*": [
        "src/*"
      ]
    }
  },
  "include": [
    "src"
  ],
  "references": [
    {
      "path": "./tsconfig.node.json"
    }
  ]
}
</file>

<file path="tsconfig.node.json">
{
  "compilerOptions": {
    "composite": true,
    "skipLibCheck": true,
    "module": "ESNext",
    "moduleResolution": "bundler",
    "allowSyntheticDefaultImports": true
  },
  "include": [
    "vite.config.ts"
  ]
}
</file>

<file path="vite.config.ts">
import { defineConfig } from "vite"
import react from "@vitejs/plugin-react-swc"
import { resolve } from "path"
import { componentTagger } from "lovable-tagger"
// https://vitejs.dev/config/
export default defineConfig(({ mode }) => ({
  plugins: [
    react(),
    mode === 'development' && componentTagger(),
  ].filter(Boolean),
  resolve: {
    alias: {
      "@": resolve(__dirname, "./src"),
    },
  },
  server: {
    port: 5173,
    host: true,
    open: true,
  },
  build: {
    sourcemap: true,
    chunkSizeWarningLimit: 1000,
  },
  optimizeDeps: {
    include: ['react', 'react-dom'],
  },
  logLevel: mode === 'development' ? 'info' : 'warn',
}))
</file>

<file path="vitest.config.ts">
/*
 * ████████╗██╗   ██╗████████╗ ██████╗ ██████╗     ██████╗ ██████╗  █████╗ ██╗███╗   ██╗
 * ╚══██╔══╝██║   ██║╚══██╔══╝██╔═══██╗██╔══██╗    ██╔══██╗██╔══██╗██╔══██╗██║████╗  ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██████╔╝    ██████╔╝██████╔╝███████║██║██╔██╗ ██║
 *    ██║   ██║   ██║   ██║   ██║   ██║██╔══██╗    ██╔══██╗██╔══██╗██╔══██║██║██║╚██╗██║
 *    ██║   ╚██████╔╝   ██║   ╚██████╔╝██║  ██║    ██████╔╝██║  ██║██║  ██║██║██║ ╚████║
 *    ╚═╝    ╚═════╝    ╚═╝    ╚═════╝ ╚═╝  ╚═╝    ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝
 *
 * ╔═══════════════════════════════════════════════════════════════════════════════════════════════╗
 * ║ Module: Vitest Configuration                                                                   ║
 * ║ Description: Configuration for Vitest testing framework                                        ║
 * ╚═══════════════════════════════════════════════════════════════════════════════════════════════╝
 */
import { defineConfig } from 'vitest/config'
import react from '@vitejs/plugin-react'
import path from 'path'
export default defineConfig({
  plugins: [react()],
  test: {
    environment: 'jsdom',
    globals: true,
    setupFiles: ['./src/test/setup.ts'],
    coverage: {
      reporter: ['text', 'json', 'html'],
      exclude: [
        'node_modules/',
        'src/test/',
        '**/*.d.ts',
        '**/*.config.*',
        '**/*.test.*'
      ]
    }
  },
  resolve: {
    alias: {
      '@': path.resolve(__dirname, './src')
    }
  }
})
</file>

</files>
